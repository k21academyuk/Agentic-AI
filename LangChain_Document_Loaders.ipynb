{"cells":[{"cell_type":"markdown","metadata":{"id":"mjyiD8I5r7jB"},"source":["# Exploring Document Loaders in LangChain"]},{"cell_type":"markdown","metadata":{"id":"L1KvMtf54l0d"},"source":["## Install OpenAI, HuggingFace and LangChain dependencies"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GSu-YKYjNs0F","outputId":"9abc3220-b062-42ff-ffc1-66df402d504e","executionInfo":{"status":"ok","timestamp":1756900439079,"user_tz":-330,"elapsed":27488,"user":{"displayName":"Prashant Sahu","userId":"17861245123904241513"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n","Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.74)\n","Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.9)\n","Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.16)\n","Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.7)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.43)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.4)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.2)\n","Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (8.5.0)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n","Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.15.0)\n","Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n","Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (3.11.2)\n","Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n","Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.24.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2025.8.3)\n","Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.10.0)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n","Collecting langchain-openai\n","  Downloading langchain_openai-0.3.32-py3-none-any.whl.metadata (2.4 kB)\n","Requirement already satisfied: langchain-core<1.0.0,>=0.3.74 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.3.74)\n","Requirement already satisfied: openai<2.0.0,>=1.99.9 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (1.101.0)\n","Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.11.0)\n","Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.74->langchain-openai) (0.4.16)\n","Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.74->langchain-openai) (8.5.0)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.74->langchain-openai) (1.33)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.74->langchain-openai) (6.0.2)\n","Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.74->langchain-openai) (4.15.0)\n","Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.74->langchain-openai) (25.0)\n","Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.74->langchain-openai) (2.11.7)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (4.10.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (0.28.1)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (0.10.0)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (1.3.1)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (4.67.1)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2.32.4)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.99.9->langchain-openai) (3.10)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.99.9->langchain-openai) (2025.8.3)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.99.9->langchain-openai) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.99.9->langchain-openai) (0.16.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.74->langchain-openai) (3.0.0)\n","Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.74->langchain-openai) (3.11.2)\n","Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.74->langchain-openai) (1.0.0)\n","Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.74->langchain-openai) (0.24.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.74->langchain-openai) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.74->langchain-openai) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.74->langchain-openai) (0.4.1)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.4.3)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.5.0)\n","Downloading langchain_openai-0.3.32-py3-none-any.whl (74 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: langchain-openai\n","Successfully installed langchain-openai-0.3.32\n","Collecting langchain-community\n","  Downloading langchain_community-0.3.29-py3-none-any.whl.metadata (2.9 kB)\n","Collecting langchain-core<2.0.0,>=0.3.75 (from langchain-community)\n","  Downloading langchain_core-0.3.75-py3-none-any.whl.metadata (5.7 kB)\n","Requirement already satisfied: langchain<2.0.0,>=0.3.27 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.3.27)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.43)\n","Collecting requests<3,>=2.32.5 (from langchain-community)\n","  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (6.0.2)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.12.15)\n","Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (8.5.0)\n","Collecting dataclasses-json<0.7,>=0.6.7 (from langchain-community)\n","  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n","Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.10.1)\n","Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.16)\n","Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.1)\n","Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.4)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n","Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.6.7->langchain-community)\n","  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n","Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.6.7->langchain-community)\n","  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n","Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain<2.0.0,>=0.3.27->langchain-community) (0.3.9)\n","Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain<2.0.0,>=0.3.27->langchain-community) (2.11.7)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.75->langchain-community) (1.33)\n","Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.75->langchain-community) (4.15.0)\n","Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.75->langchain-community) (25.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community) (0.28.1)\n","Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community) (3.11.2)\n","Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\n","Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community) (0.24.0)\n","Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.1.1)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (0.4.1)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain-community) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain-community) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain-community) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain-community) (2025.8.3)\n","Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.4)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (4.10.0)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (0.16.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<2.0.0,>=0.3.75->langchain-community) (3.0.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.27->langchain-community) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.27->langchain-community) (2.33.2)\n","Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.6.7->langchain-community)\n","  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.3.1)\n","Downloading langchain_community-0.3.29-py3-none-any.whl (2.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n","Downloading langchain_core-0.3.75-py3-none-any.whl (443 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m444.0/444.0 kB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n","Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n","Installing collected packages: requests, mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain-core, langchain-community\n","  Attempting uninstall: requests\n","    Found existing installation: requests 2.32.4\n","    Uninstalling requests-2.32.4:\n","      Successfully uninstalled requests-2.32.4\n","  Attempting uninstall: langchain-core\n","    Found existing installation: langchain-core 0.3.74\n","    Uninstalling langchain-core-0.3.74:\n","      Successfully uninstalled langchain-core-0.3.74\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed dataclasses-json-0.6.7 langchain-community-0.3.29 langchain-core-0.3.75 marshmallow-3.26.1 mypy-extensions-1.1.0 requests-2.32.5 typing-inspect-0.9.0\n"]}],"source":["!pip install langchain #==0.3.11\n","!pip install langchain-openai #==0.2.12\n","!pip install langchain-community #==0.3.11"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CB6lHzbz5a10","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"ada5287f-8f85-4174-deae-97bb5736273b","executionInfo":{"status":"ok","timestamp":1756900471460,"user_tz":-330,"elapsed":30132,"user":{"displayName":"Prashant Sahu","userId":"17861245123904241513"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting unstructured[all-docs]\n","  Downloading unstructured-0.18.14-py3-none-any.whl.metadata (24 kB)\n","Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from unstructured[all-docs]) (3.4.3)\n","Collecting filetype (from unstructured[all-docs])\n","  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n","Collecting python-magic (from unstructured[all-docs])\n","  Downloading python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from unstructured[all-docs]) (5.4.0)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from unstructured[all-docs]) (3.9.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from unstructured[all-docs]) (2.32.5)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from unstructured[all-docs]) (4.13.5)\n","Collecting emoji (from unstructured[all-docs])\n","  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n","Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.12/dist-packages (from unstructured[all-docs]) (0.6.7)\n","Collecting python-iso639 (from unstructured[all-docs])\n","  Downloading python_iso639-2025.2.18-py3-none-any.whl.metadata (14 kB)\n","Collecting langdetect (from unstructured[all-docs])\n","  Downloading langdetect-1.0.9.tar.gz (981 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from unstructured[all-docs]) (2.0.2)\n","Collecting rapidfuzz (from unstructured[all-docs])\n","  Downloading rapidfuzz-3.14.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (12 kB)\n","Collecting backoff (from unstructured[all-docs])\n","  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from unstructured[all-docs]) (4.15.0)\n","Collecting unstructured-client (from unstructured[all-docs])\n","  Downloading unstructured_client-0.42.3-py3-none-any.whl.metadata (23 kB)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from unstructured[all-docs]) (1.17.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from unstructured[all-docs]) (4.67.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from unstructured[all-docs]) (5.9.5)\n","Collecting python-oxmsg (from unstructured[all-docs])\n","  Downloading python_oxmsg-0.0.2-py3-none-any.whl.metadata (5.0 kB)\n","Requirement already satisfied: html5lib in /usr/local/lib/python3.12/dist-packages (from unstructured[all-docs]) (1.1)\n","Collecting msoffcrypto-tool (from unstructured[all-docs])\n","  Downloading msoffcrypto_tool-5.4.2-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: xlrd in /usr/local/lib/python3.12/dist-packages (from unstructured[all-docs]) (2.0.2)\n","Collecting onnx>=1.17.0 (from unstructured[all-docs])\n","  Downloading onnx-1.19.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (7.0 kB)\n","Collecting pdf2image (from unstructured[all-docs])\n","  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n","Collecting pi-heif (from unstructured[all-docs])\n","  Downloading pi_heif-1.1.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.5 kB)\n","Collecting python-pptx>=1.0.1 (from unstructured[all-docs])\n","  Downloading python_pptx-1.0.2-py3-none-any.whl.metadata (2.5 kB)\n","Collecting google-cloud-vision (from unstructured[all-docs])\n","  Downloading google_cloud_vision-3.10.2-py3-none-any.whl.metadata (9.6 kB)\n","Collecting onnxruntime>=1.19.0 (from unstructured[all-docs])\n","  Downloading onnxruntime-1.22.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\n","Collecting python-docx>=1.1.2 (from unstructured[all-docs])\n","  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n","Collecting unstructured.pytesseract>=0.3.12 (from unstructured[all-docs])\n","  Downloading unstructured.pytesseract-0.3.15-py3-none-any.whl.metadata (11 kB)\n","Collecting pdfminer.six (from unstructured[all-docs])\n","  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from unstructured[all-docs]) (3.5)\n","Collecting pypdf (from unstructured[all-docs])\n","  Downloading pypdf-6.0.0-py3-none-any.whl.metadata (7.1 kB)\n","Collecting pypandoc (from unstructured[all-docs])\n","  Downloading pypandoc-1.15-py3-none-any.whl.metadata (16 kB)\n","Requirement already satisfied: openpyxl in /usr/local/lib/python3.12/dist-packages (from unstructured[all-docs]) (3.1.5)\n","Collecting effdet (from unstructured[all-docs])\n","  Downloading effdet-0.4.1-py3-none-any.whl.metadata (33 kB)\n","Collecting unstructured-inference>=1.0.5 (from unstructured[all-docs])\n","  Downloading unstructured_inference-1.0.5-py3-none-any.whl.metadata (5.3 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from unstructured[all-docs]) (2.2.2)\n","Collecting pikepdf (from unstructured[all-docs])\n","  Downloading pikepdf-9.10.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n","Requirement already satisfied: markdown in /usr/local/lib/python3.12/dist-packages (from unstructured[all-docs]) (3.8.2)\n","Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.12/dist-packages (from onnx>=1.17.0->unstructured[all-docs]) (5.29.5)\n","Requirement already satisfied: ml_dtypes in /usr/local/lib/python3.12/dist-packages (from onnx>=1.17.0->unstructured[all-docs]) (0.5.3)\n","Collecting coloredlogs (from onnxruntime>=1.19.0->unstructured[all-docs])\n","  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.19.0->unstructured[all-docs]) (25.2.10)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.19.0->unstructured[all-docs]) (25.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.19.0->unstructured[all-docs]) (1.13.3)\n","Requirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.12/dist-packages (from python-pptx>=1.0.1->unstructured[all-docs]) (11.3.0)\n","Collecting XlsxWriter>=0.5.7 (from python-pptx>=1.0.1->unstructured[all-docs])\n","  Downloading xlsxwriter-3.2.5-py3-none-any.whl.metadata (2.7 kB)\n","Requirement already satisfied: python-multipart in /usr/local/lib/python3.12/dist-packages (from unstructured-inference>=1.0.5->unstructured[all-docs]) (0.0.20)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.12/dist-packages (from unstructured-inference>=1.0.5->unstructured[all-docs]) (0.34.4)\n","Requirement already satisfied: opencv-python!=4.7.0.68 in /usr/local/lib/python3.12/dist-packages (from unstructured-inference>=1.0.5->unstructured[all-docs]) (4.12.0.88)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from unstructured-inference>=1.0.5->unstructured[all-docs]) (3.10.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from unstructured-inference>=1.0.5->unstructured[all-docs]) (2.8.0+cu126)\n","Requirement already satisfied: timm in /usr/local/lib/python3.12/dist-packages (from unstructured-inference>=1.0.5->unstructured[all-docs]) (1.0.19)\n","Requirement already satisfied: transformers>=4.25.1 in /usr/local/lib/python3.12/dist-packages (from unstructured-inference>=1.0.5->unstructured[all-docs]) (4.55.4)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (from unstructured-inference>=1.0.5->unstructured[all-docs]) (1.10.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from unstructured-inference>=1.0.5->unstructured[all-docs]) (1.16.1)\n","Collecting pypdfium2 (from unstructured-inference>=1.0.5->unstructured[all-docs])\n","  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->unstructured[all-docs]) (2.7)\n","Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json->unstructured[all-docs]) (3.26.1)\n","Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json->unstructured[all-docs]) (0.9.0)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from effdet->unstructured[all-docs]) (0.23.0+cu126)\n","Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from effdet->unstructured[all-docs]) (2.0.10)\n","Requirement already satisfied: omegaconf>=2.0 in /usr/local/lib/python3.12/dist-packages (from effdet->unstructured[all-docs]) (2.3.0)\n","Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision->unstructured[all-docs]) (2.25.1)\n","Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-cloud-vision->unstructured[all-docs]) (2.38.0)\n","Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-cloud-vision->unstructured[all-docs]) (1.26.1)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.12/dist-packages (from html5lib->unstructured[all-docs]) (1.17.0)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from html5lib->unstructured[all-docs]) (0.5.1)\n","Requirement already satisfied: cryptography>=39.0 in /usr/local/lib/python3.12/dist-packages (from msoffcrypto-tool->unstructured[all-docs]) (43.0.3)\n","Collecting olefile>=0.46 (from msoffcrypto-tool->unstructured[all-docs])\n","  Downloading olefile-0.47-py2.py3-none-any.whl.metadata (9.7 kB)\n","Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->unstructured[all-docs]) (8.2.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk->unstructured[all-docs]) (1.5.1)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk->unstructured[all-docs]) (2024.11.6)\n","Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl->unstructured[all-docs]) (2.0.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->unstructured[all-docs]) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->unstructured[all-docs]) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->unstructured[all-docs]) (2025.2)\n","Collecting Deprecated (from pikepdf->unstructured[all-docs])\n","  Downloading Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->unstructured[all-docs]) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->unstructured[all-docs]) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->unstructured[all-docs]) (2025.8.3)\n","Requirement already satisfied: aiofiles>=24.1.0 in /usr/local/lib/python3.12/dist-packages (from unstructured-client->unstructured[all-docs]) (24.1.0)\n","Requirement already satisfied: httpcore>=1.0.9 in /usr/local/lib/python3.12/dist-packages (from unstructured-client->unstructured[all-docs]) (1.0.9)\n","Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from unstructured-client->unstructured[all-docs]) (0.28.1)\n","Requirement already satisfied: pydantic>=2.11.2 in /usr/local/lib/python3.12/dist-packages (from unstructured-client->unstructured[all-docs]) (2.11.7)\n","Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from unstructured-client->unstructured[all-docs]) (1.0.0)\n","Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=39.0->msoffcrypto-tool->unstructured[all-docs]) (1.17.1)\n","Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision->unstructured[all-docs]) (1.70.0)\n","Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision->unstructured[all-docs]) (1.74.0)\n","Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision->unstructured[all-docs]) (1.71.2)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision->unstructured[all-docs]) (5.5.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision->unstructured[all-docs]) (0.4.2)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision->unstructured[all-docs]) (4.9.1)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore>=1.0.9->unstructured-client->unstructured[all-docs]) (0.16.0)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured[all-docs]) (4.10.0)\n","Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from omegaconf>=2.0->effdet->unstructured[all-docs]) (4.9.3)\n","Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.12/dist-packages (from omegaconf>=2.0->effdet->unstructured[all-docs]) (6.0.2)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.11.2->unstructured-client->unstructured[all-docs]) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.11.2->unstructured-client->unstructured[all-docs]) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.11.2->unstructured-client->unstructured[all-docs]) (0.4.1)\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from timm->unstructured-inference>=1.0.5->unstructured[all-docs]) (0.6.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference>=1.0.5->unstructured[all-docs]) (3.19.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference>=1.0.5->unstructured[all-docs]) (75.2.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference>=1.0.5->unstructured[all-docs]) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference>=1.0.5->unstructured[all-docs]) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference>=1.0.5->unstructured[all-docs]) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference>=1.0.5->unstructured[all-docs]) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference>=1.0.5->unstructured[all-docs]) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference>=1.0.5->unstructured[all-docs]) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference>=1.0.5->unstructured[all-docs]) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference>=1.0.5->unstructured[all-docs]) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference>=1.0.5->unstructured[all-docs]) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference>=1.0.5->unstructured[all-docs]) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference>=1.0.5->unstructured[all-docs]) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference>=1.0.5->unstructured[all-docs]) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference>=1.0.5->unstructured[all-docs]) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference>=1.0.5->unstructured[all-docs]) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference>=1.0.5->unstructured[all-docs]) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference>=1.0.5->unstructured[all-docs]) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference>=1.0.5->unstructured[all-docs]) (3.4.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.19.0->unstructured[all-docs]) (1.3.0)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.25.1->unstructured-inference>=1.0.5->unstructured[all-docs]) (0.21.4)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->unstructured-inference>=1.0.5->unstructured[all-docs]) (1.1.8)\n","Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured[all-docs]) (1.1.0)\n","Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.19.0->unstructured[all-docs])\n","  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->unstructured-inference>=1.0.5->unstructured[all-docs]) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->unstructured-inference>=1.0.5->unstructured[all-docs]) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->unstructured-inference>=1.0.5->unstructured[all-docs]) (4.59.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->unstructured-inference>=1.0.5->unstructured[all-docs]) (1.4.9)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->unstructured-inference>=1.0.5->unstructured[all-docs]) (3.2.3)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=39.0->msoffcrypto-tool->unstructured[all-docs]) (2.22)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision->unstructured[all-docs]) (0.6.1)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.27.0->unstructured-client->unstructured[all-docs]) (1.3.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->unstructured-inference>=1.0.5->unstructured[all-docs]) (3.0.2)\n","Downloading onnx-1.19.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (18.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m106.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading onnxruntime-1.22.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m96.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_pptx-1.0.2-py3-none-any.whl (472 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.8/472.8 kB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading unstructured_inference-1.0.5-py3-none-any.whl (48 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading unstructured.pytesseract-0.3.15-py3-none-any.whl (14 kB)\n","Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n","Downloading effdet-0.4.1-py3-none-any.whl (112 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.5/112.5 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading emoji-2.14.1-py3-none-any.whl (590 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n","Downloading google_cloud_vision-3.10.2-py3-none-any.whl (527 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.9/527.9 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading msoffcrypto_tool-5.4.2-py3-none-any.whl (48 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.7/48.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n","Downloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m114.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pi_heif-1.1.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pikepdf-9.10.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (2.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m103.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pypandoc-1.15-py3-none-any.whl (21 kB)\n","Downloading pypdf-6.0.0-py3-none-any.whl (310 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.5/310.5 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_iso639-2025.2.18-py3-none-any.whl (167 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.6/167.6 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n","Downloading python_oxmsg-0.0.2-py3-none-any.whl (31 kB)\n","Downloading rapidfuzz-3.14.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (3.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m101.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading unstructured-0.18.14-py3-none-any.whl (1.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m83.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading unstructured_client-0.42.3-py3-none-any.whl (207 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.8/207.8 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading olefile-0.47-py2.py3-none-any.whl (114 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xlsxwriter-3.2.5-py3-none-any.whl (172 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.3/172.3 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n","Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m105.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: langdetect\n","  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=6487248c7c05c7bfe873431e27a8524c56849eda1456eda1dff1fa2e2c7ab427\n","  Stored in directory: /root/.cache/pip/wheels/c1/67/88/e844b5b022812e15a52e4eaa38a1e709e99f06f6639d7e3ba7\n","Successfully built langdetect\n","Installing collected packages: filetype, XlsxWriter, unstructured.pytesseract, rapidfuzz, python-magic, python-iso639, python-docx, pypdfium2, pypdf, pypandoc, pi-heif, pdf2image, olefile, langdetect, humanfriendly, emoji, Deprecated, backoff, python-pptx, python-oxmsg, pikepdf, onnx, coloredlogs, unstructured-client, pdfminer.six, onnxruntime, msoffcrypto-tool, unstructured, google-cloud-vision, unstructured-inference, effdet\n","Successfully installed Deprecated-1.2.18 XlsxWriter-3.2.5 backoff-2.2.1 coloredlogs-15.0.1 effdet-0.4.1 emoji-2.14.1 filetype-1.2.0 google-cloud-vision-3.10.2 humanfriendly-10.0 langdetect-1.0.9 msoffcrypto-tool-5.4.2 olefile-0.47 onnx-1.19.0 onnxruntime-1.22.1 pdf2image-1.17.0 pdfminer.six-20250506 pi-heif-1.1.0 pikepdf-9.10.2 pypandoc-1.15 pypdf-6.0.0 pypdfium2-4.30.0 python-docx-1.2.0 python-iso639-2025.2.18 python-magic-0.4.27 python-oxmsg-0.0.2 python-pptx-1.0.2 rapidfuzz-3.14.0 unstructured-0.18.14 unstructured-client-0.42.3 unstructured-inference-1.0.5 unstructured.pytesseract-0.3.15\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["google"]},"id":"46e06f68fb634fc88c5c17fe5b21fd60"}},"metadata":{}}],"source":["# takes 2 - 5 mins to install on Colab\n","!pip install \"unstructured[all-docs]\" #==0.14.0\""]},{"cell_type":"markdown","metadata":{"id":"ZTFImul36TRH"},"source":["After installing `unstructured`above remember to restart your session when it shows you the following popup, if it doesn't go to `Runtime`and `Restart Session`\n","\n","![](https://i.imgur.com/UOBaotk.png)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NhEW-tOywUgt","colab":{"base_uri":"https://localhost:8080/"},"outputId":"46595ff8-df80-482c-8b7d-305600eef6c2","executionInfo":{"status":"ok","timestamp":1756900517801,"user_tz":-330,"elapsed":6561,"user":{"displayName":"Prashant Sahu","userId":"17861245123904241513"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","tesseract-ocr is already the newest version (4.1.1-2.1build1).\n","0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","The following NEW packages will be installed:\n","  poppler-utils\n","0 upgraded, 1 newly installed, 0 to remove and 35 not upgraded.\n","Need to get 186 kB of archives.\n","After this operation, 697 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.10 [186 kB]\n","Fetched 186 kB in 0s (1,914 kB/s)\n","debconf: unable to initialize frontend: Dialog\n","debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n","debconf: falling back to frontend: Readline\n","debconf: unable to initialize frontend: Readline\n","debconf: (This frontend requires a controlling tty.)\n","debconf: falling back to frontend: Teletype\n","dpkg-preconfigure: unable to re-open stdin: \n","Selecting previously unselected package poppler-utils.\n","(Reading database ... 126371 files and directories currently installed.)\n","Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.10_amd64.deb ...\n","Unpacking poppler-utils (22.02.0-2ubuntu0.10) ...\n","Setting up poppler-utils (22.02.0-2ubuntu0.10) ...\n","Processing triggers for man-db (2.10.2-1) ...\n"]}],"source":["# install OCR dependencies for unstructured\n","!sudo apt-get install tesseract-ocr\n","!sudo apt-get install poppler-utils"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MWNjOhSbRaOw","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0a4155f8-095e-419d-e3fb-e1917ad0a9d2","executionInfo":{"status":"ok","timestamp":1756900542067,"user_tz":-330,"elapsed":18790,"user":{"displayName":"Prashant Sahu","userId":"17861245123904241513"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting jq==1.7.0\n","  Downloading jq-1.7.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n","Downloading jq-1.7.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (664 kB)\n","\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/664.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m655.4/664.7 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.7/664.7 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: jq\n","Successfully installed jq-1.7.0\n","Collecting pypdf==4.2.0\n","  Downloading pypdf-4.2.0-py3-none-any.whl.metadata (7.4 kB)\n","Downloading pypdf-4.2.0-py3-none-any.whl (290 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pypdf\n","  Attempting uninstall: pypdf\n","    Found existing installation: pypdf 6.0.0\n","    Uninstalling pypdf-6.0.0:\n","      Successfully uninstalled pypdf-6.0.0\n","Successfully installed pypdf-4.2.0\n","Collecting pymupdf==1.24.4\n","  Downloading PyMuPDF-1.24.4-cp312-none-manylinux2014_x86_64.whl.metadata (3.4 kB)\n","Collecting PyMuPDFb==1.24.3 (from pymupdf==1.24.4)\n","  Downloading PyMuPDFb-1.24.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.4 kB)\n","Downloading PyMuPDF-1.24.4-cp312-none-manylinux2014_x86_64.whl (3.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading PyMuPDFb-1.24.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (15.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.8/15.8 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: PyMuPDFb, pymupdf\n","Successfully installed PyMuPDFb-1.24.3 pymupdf-1.24.4\n"]}],"source":["!pip install jq==1.7.0\n","!pip install pypdf==4.2.0\n","!pip install pymupdf==1.24.4"]},{"cell_type":"markdown","metadata":{"id":"aqX0BkkWZ_e0"},"source":["## Document Loaders\n","\n","Document loaders are used to import data from various sources into LangChain as `Document` objects. A `Document` typically includes a piece of text along with its associated metadata.\n","\n","### Examples of Document Loaders:\n","\n","- **Text File Loader:** Loads data from a simple `.txt` file.\n","- **Web Page Loader:** Retrieves the text content from any web page.\n","- **YouTube Video Transcript Loader:** Loads transcripts from YouTube videos.\n","\n","### Functionality:\n","\n","- **Load Method:** Each document loader has a `load` method that enables the loading of data as documents from a pre-configured source.\n","- **Lazy Load Option:** Some loaders also support a \"lazy load\" feature, which allows data to be loaded into memory gradually as needed.\n","\n","For more detailed information, visit [LangChain's document loader documentation](https://python.langchain.com/docs/modules/data_connection/document_loaders/).\n"]},{"cell_type":"markdown","metadata":{"id":"CEx_nNkHLqZY"},"source":["### Text Loader\n","\n","The simplest loader reads in a file as text and places it all into one document.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Al7y4r93LKA4","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7f98a1e8-4f3a-4d5c-8a0e-139f502b6bf2","executionInfo":{"status":"ok","timestamp":1756900557036,"user_tz":-330,"elapsed":327,"user":{"displayName":"Prashant Sahu","userId":"17861245123904241513"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100  5067  100  5067    0     0  24317      0 --:--:-- --:--:-- --:--:-- 24244\n"]}],"source":["!curl -o README.md https://raw.githubusercontent.com/langchain-ai/langchain/master/README.md"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3ehpI19eLKEo"},"outputs":[],"source":["from langchain_community.document_loaders import TextLoader\n","\n","loader = TextLoader(\"./README.md\")\n","doc = loader.load()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SxR60glzvzQN","colab":{"base_uri":"https://localhost:8080/"},"outputId":"51fc2928-9f79-45e7-c203-722bafc5be89","executionInfo":{"status":"ok","timestamp":1756900560912,"user_tz":-330,"elapsed":25,"user":{"displayName":"Prashant Sahu","userId":"17861245123904241513"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["1"]},"metadata":{},"execution_count":5}],"source":["len(doc)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uu-7i1PcdHpE","colab":{"base_uri":"https://localhost:8080/","height":186},"outputId":"0303070d-60b5-4cab-8073-a2237694f5ec","executionInfo":{"status":"ok","timestamp":1756900563369,"user_tz":-330,"elapsed":5,"user":{"displayName":"Prashant Sahu","userId":"17861245123904241513"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["langchain_core.documents.base.Document"],"text/html":["<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n","      pre.function-repr-contents {\n","        overflow-x: auto;\n","        padding: 8px 12px;\n","        max-height: 500px;\n","      }\n","\n","      pre.function-repr-contents.function-repr-contents-collapsed {\n","        cursor: pointer;\n","        max-height: 100px;\n","      }\n","    </style>\n","    <pre style=\"white-space: initial; background:\n","         var(--colab-secondary-surface-color); padding: 8px 12px;\n","         border-bottom: 1px solid var(--colab-border-color);\"><b>langchain_core.documents.base.Document</b><br/>def __init__(page_content: str, **kwargs: Any) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.12/dist-packages/langchain_core/documents/base.py</a>Class for storing a piece of text and associated metadata.\n","\n","Example:\n","\n","    .. code-block:: python\n","\n","        from langchain_core.documents import Document\n","\n","        document = Document(\n","            page_content=&quot;Hello, world!&quot;,\n","            metadata={&quot;source&quot;: &quot;https://example.com&quot;}\n","        )</pre>\n","      <script>\n","      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n","        for (const element of document.querySelectorAll('.filepath')) {\n","          element.style.display = 'block'\n","          element.onclick = (event) => {\n","            event.preventDefault();\n","            event.stopPropagation();\n","            google.colab.files.view(element.textContent, 256);\n","          };\n","        }\n","      }\n","      for (const element of document.querySelectorAll('.function-repr-contents')) {\n","        element.onclick = (event) => {\n","          event.preventDefault();\n","          event.stopPropagation();\n","          element.classList.toggle('function-repr-contents-collapsed');\n","        };\n","      }\n","      </script>\n","      </div>"]},"metadata":{},"execution_count":6}],"source":["type(doc[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FFbJx7Q9LKG4","scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"outputId":"9fe7316e-f901-450e-be8b-1cae0317b267","executionInfo":{"status":"ok","timestamp":1756900565171,"user_tz":-330,"elapsed":34,"user":{"displayName":"Prashant Sahu","userId":"17861245123904241513"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["<picture>\n","  <source media=\"(prefers-color-scheme: light)\" srcset=\"docs/static/img/logo-dark.svg\">\n","  \n"]}],"source":["print(doc[0].page_content[:100])"]},{"cell_type":"markdown","metadata":{"id":"oTf3za4x7CtK"},"source":["### Markdown Loader\n","\n","Markdown is a lightweight markup language for creating formatted text using a plain-text editor.\n","\n","This showcases how to load Markdown documents into a langchain document format that we can use in our pipelines and chains."]},{"cell_type":"markdown","metadata":{"id":"sEmzUQK9_640"},"source":["Load the whole document"]},{"cell_type":"markdown","source":["Download nltk packages if needed"],"metadata":{"id":"YJt1cdnFOeP0"}},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt_tab')\n","nltk.download('averaged_perceptron_tagger_eng')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CD-IBE4POUwu","outputId":"f33f5bfb-815e-497a-c830-884790baa790","executionInfo":{"status":"ok","timestamp":1756900571501,"user_tz":-330,"elapsed":2634,"user":{"displayName":"Prashant Sahu","userId":"17861245123904241513"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D1uZEc-f8TyV"},"outputs":[],"source":["from langchain_community.document_loaders import UnstructuredMarkdownLoader\n","\n","loader = UnstructuredMarkdownLoader(\"./README.md\", mode='single')\n","docs = loader.load()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DPkNbhXv7Z3B","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b138d46e-dce0-4f8a-a421-1ec7eea9b3f2","executionInfo":{"status":"ok","timestamp":1756900583642,"user_tz":-330,"elapsed":37,"user":{"displayName":"Prashant Sahu","userId":"17861245123904241513"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["1"]},"metadata":{},"execution_count":10}],"source":["len(docs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m547b2uc8vy9","colab":{"base_uri":"https://localhost:8080/","height":186},"outputId":"e2f8fd5b-2931-4f92-dfa1-dbd3bcfceffc","executionInfo":{"status":"ok","timestamp":1756900586749,"user_tz":-330,"elapsed":16,"user":{"displayName":"Prashant Sahu","userId":"17861245123904241513"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["langchain_core.documents.base.Document"],"text/html":["<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n","      pre.function-repr-contents {\n","        overflow-x: auto;\n","        padding: 8px 12px;\n","        max-height: 500px;\n","      }\n","\n","      pre.function-repr-contents.function-repr-contents-collapsed {\n","        cursor: pointer;\n","        max-height: 100px;\n","      }\n","    </style>\n","    <pre style=\"white-space: initial; background:\n","         var(--colab-secondary-surface-color); padding: 8px 12px;\n","         border-bottom: 1px solid var(--colab-border-color);\"><b>langchain_core.documents.base.Document</b><br/>def __init__(page_content: str, **kwargs: Any) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.12/dist-packages/langchain_core/documents/base.py</a>Class for storing a piece of text and associated metadata.\n","\n","Example:\n","\n","    .. code-block:: python\n","\n","        from langchain_core.documents import Document\n","\n","        document = Document(\n","            page_content=&quot;Hello, world!&quot;,\n","            metadata={&quot;source&quot;: &quot;https://example.com&quot;}\n","        )</pre>\n","      <script>\n","      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n","        for (const element of document.querySelectorAll('.filepath')) {\n","          element.style.display = 'block'\n","          element.onclick = (event) => {\n","            event.preventDefault();\n","            event.stopPropagation();\n","            google.colab.files.view(element.textContent, 256);\n","          };\n","        }\n","      }\n","      for (const element of document.querySelectorAll('.function-repr-contents')) {\n","        element.onclick = (event) => {\n","          event.preventDefault();\n","          event.stopPropagation();\n","          element.classList.toggle('function-repr-contents-collapsed');\n","        };\n","      }\n","      </script>\n","      </div>"]},"metadata":{},"execution_count":11}],"source":["type(docs[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_gia_n-T8Ytn","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e6deb4ff-7da3-4a81-d294-d491f3d14742","executionInfo":{"status":"ok","timestamp":1756900588695,"user_tz":-330,"elapsed":19,"user":{"displayName":"Prashant Sahu","userId":"17861245123904241513"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Release Notes\n","\n","PyPI - License\n","\n","PyPI - Downloads\n","\n","GitHub star chart\n","\n","Open in Dev Containers\n","\n","Open in \n"]}],"source":["print(docs[0].page_content[:100])"]},{"cell_type":"markdown","metadata":{"id":"AVyZosgZ_-U8"},"source":["Load document and separate based on elements"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SOPDX8w85MeG"},"outputs":[],"source":["from langchain_community.document_loaders import UnstructuredMarkdownLoader\n","\n","loader = UnstructuredMarkdownLoader(\"./README.md\", mode=\"elements\")\n","docs = loader.load()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dr4WVaEg-qTr","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b6e3673c-7c74-445c-bc6f-b039dbb9a2d5","executionInfo":{"status":"ok","timestamp":1756900592129,"user_tz":-330,"elapsed":220,"user":{"displayName":"Prashant Sahu","userId":"17861245123904241513"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["30"]},"metadata":{},"execution_count":14}],"source":["len(docs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HpfQwXmD-rji","colab":{"base_uri":"https://localhost:8080/"},"outputId":"fac7e3f3-08a3-4678-f65c-98e2f3024fe0","executionInfo":{"status":"ok","timestamp":1756900592611,"user_tz":-330,"elapsed":99,"user":{"displayName":"Prashant Sahu","userId":"17861245123904241513"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Document(metadata={'source': './README.md', 'image_url': 'https://img.shields.io/github/release/langchain-ai/langchain?style=flat-square', 'link_texts': ['Release Notes'], 'link_urls': ['https://github.com/langchain-ai/langchain/releases'], 'languages': ['eng'], 'file_directory': '.', 'filename': 'README.md', 'filetype': 'text/markdown', 'last_modified': '2025-09-03T11:55:57', 'category': 'Image', 'element_id': 'fb048c0fef65aadf1f7c01bc20304f16'}, page_content='Release Notes'),\n"," Document(metadata={'source': './README.md', 'image_url': 'https://img.shields.io/pypi/l/langchain-core?style=flat-square', 'link_texts': ['PyPI - License'], 'link_urls': ['https://opensource.org/licenses/MIT'], 'languages': ['eng'], 'file_directory': '.', 'filename': 'README.md', 'filetype': 'text/markdown', 'last_modified': '2025-09-03T11:55:57', 'category': 'Image', 'element_id': '594c7e3d43abeaa71d7a8d9ef076568e'}, page_content='PyPI - License'),\n"," Document(metadata={'source': './README.md', 'image_url': 'https://img.shields.io/pepy/dt/langchain', 'link_texts': ['PyPI - Downloads'], 'link_urls': ['https://pypistats.org/packages/langchain-core'], 'languages': ['eng'], 'file_directory': '.', 'filename': 'README.md', 'filetype': 'text/markdown', 'last_modified': '2025-09-03T11:55:57', 'category': 'Image', 'element_id': 'cf260c7e27a3b045234e8ff3e6532528'}, page_content='PyPI - Downloads'),\n"," Document(metadata={'source': './README.md', 'image_url': 'https://img.shields.io/github/stars/langchain-ai/langchain?style=flat-square', 'link_texts': ['GitHub star chart'], 'link_urls': ['https://star-history.com/#langchain-ai/langchain'], 'languages': ['eng'], 'file_directory': '.', 'filename': 'README.md', 'filetype': 'text/markdown', 'last_modified': '2025-09-03T11:55:57', 'category': 'Image', 'element_id': 'bb35d98b97ed041035c17500c0533089'}, page_content='GitHub star chart'),\n"," Document(metadata={'source': './README.md', 'image_url': 'https://img.shields.io/static/v1?label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode&style=flat-square', 'link_texts': ['Open in Dev Containers'], 'link_urls': ['https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/langchain-ai/langchain'], 'languages': ['eng'], 'file_directory': '.', 'filename': 'README.md', 'filetype': 'text/markdown', 'last_modified': '2025-09-03T11:55:57', 'category': 'Image', 'element_id': '5f61b0b7cd936639af8756eff2284a88'}, page_content='Open in Dev Containers'),\n"," Document(metadata={'source': './README.md', 'image_url': 'https://github.com/codespaces/badge.svg', 'link_texts': ['Open in Github Codespace'], 'link_urls': ['https://codespaces.new/langchain-ai/langchain'], 'languages': ['eng'], 'file_directory': '.', 'filename': 'README.md', 'filetype': 'text/markdown', 'last_modified': '2025-09-03T11:55:57', 'category': 'Image', 'element_id': '350e231cd4f3b2f13eb973392215b020'}, page_content='Open in Github Codespace'),\n"," Document(metadata={'source': './README.md', 'image_url': 'https://img.shields.io/endpoint?url=https://codspeed.io/badge.json', 'link_texts': ['CodSpeed Badge'], 'link_urls': ['https://codspeed.io/langchain-ai/langchain'], 'languages': ['eng'], 'file_directory': '.', 'filename': 'README.md', 'filetype': 'text/markdown', 'last_modified': '2025-09-03T11:55:57', 'category': 'Image', 'element_id': 'a8df5f1b3e647ceabed1b2038f207a77'}, page_content='CodSpeed Badge'),\n"," Document(metadata={'source': './README.md', 'image_url': 'https://img.shields.io/twitter/url/https/twitter.com/langchainai.svg?style=social&label=Follow%20%40LangChainAI', 'link_texts': ['Twitter'], 'link_urls': ['https://twitter.com/langchainai'], 'languages': ['eng'], 'file_directory': '.', 'filename': 'README.md', 'filetype': 'text/markdown', 'last_modified': '2025-09-03T11:55:57', 'category': 'Image', 'element_id': '46516067537f28ff118750e246d63556'}, page_content='Twitter'),\n"," Document(metadata={'source': './README.md', 'link_texts': ['LangChain.js'], 'link_urls': ['https://github.com/langchain-ai/langchainjs'], 'languages': ['eng'], 'file_directory': '.', 'filename': 'README.md', 'filetype': 'text/markdown', 'last_modified': '2025-09-03T11:55:57', 'category': 'NarrativeText', 'element_id': '387bd43c4223c43d3489278360aa0b18'}, page_content='[!NOTE] Looking for the JS/TS library? Check out LangChain.js.'),\n"," Document(metadata={'source': './README.md', 'languages': ['eng'], 'file_directory': '.', 'filename': 'README.md', 'filetype': 'text/markdown', 'last_modified': '2025-09-03T11:55:57', 'category': 'NarrativeText', 'element_id': '3a81a5d90cdf42b3067dc356ee017946'}, page_content='LangChain is a framework for building LLM-powered applications. It helps you chain together interoperable components and third-party integrations to simplify AI application development — all while future-proofing decisions as the underlying technology evolves.')]"]},"metadata":{},"execution_count":15}],"source":["docs[:10]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EUcCM72S-vHb","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b75f5b5b-8396-480a-e9f8-c84bdcc217ba","executionInfo":{"status":"ok","timestamp":1756900594152,"user_tz":-330,"elapsed":49,"user":{"displayName":"Prashant Sahu","userId":"17861245123904241513"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Counter({'Image': 8,\n","         'NarrativeText': 6,\n","         'UncategorizedText': 2,\n","         'Title': 3,\n","         'ListItem': 11})"]},"metadata":{},"execution_count":16}],"source":["from collections import Counter\n","Counter([doc.metadata['category'] for doc in docs])"]},{"cell_type":"markdown","metadata":{"id":"7h8Hb5sKABXD"},"source":["Comparing Unstructured.io loaders vs LangChain wrapper API"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RZqKGM8q8RpS"},"outputs":[],"source":["from unstructured.partition.md import partition_md\n","\n","docs = partition_md(filename=\"./README.md\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eQES4WJY80IM","colab":{"base_uri":"https://localhost:8080/"},"outputId":"26511e80-02b7-488a-91b7-56e5b2ed89fd","executionInfo":{"status":"ok","timestamp":1756900597539,"user_tz":-330,"elapsed":47,"user":{"displayName":"Prashant Sahu","userId":"17861245123904241513"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["30"]},"metadata":{},"execution_count":18}],"source":["len(docs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"88F0QIA2-_96","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1e585179-2c23-4d9c-8fb6-7e017b6bfc87","executionInfo":{"status":"ok","timestamp":1756900598839,"user_tz":-330,"elapsed":137,"user":{"displayName":"Prashant Sahu","userId":"17861245123904241513"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[<unstructured.documents.elements.Image at 0x7a8a3c2d33b0>,\n"," <unstructured.documents.elements.Image at 0x7a8a37681a60>,\n"," <unstructured.documents.elements.Image at 0x7a8a37683dd0>,\n"," <unstructured.documents.elements.Image at 0x7a8a37683e60>,\n"," <unstructured.documents.elements.Image at 0x7a8a37683680>,\n"," <unstructured.documents.elements.Image at 0x7a8a37680350>,\n"," <unstructured.documents.elements.Image at 0x7a8a37681580>,\n"," <unstructured.documents.elements.Image at 0x7a8a37680410>,\n"," <unstructured.documents.elements.NarrativeText at 0x7a8a3c2d8410>,\n"," <unstructured.documents.elements.NarrativeText at 0x7a8a3c2d8740>]"]},"metadata":{},"execution_count":19}],"source":["docs[:10]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oVGTYLJf7fgC","colab":{"base_uri":"https://localhost:8080/"},"outputId":"aef095eb-4cc6-4efe-a5de-682e7944a22d","executionInfo":{"status":"ok","timestamp":1756900599092,"user_tz":-330,"elapsed":51,"user":{"displayName":"Prashant Sahu","userId":"17861245123904241513"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'type': 'Image',\n"," 'element_id': 'fb048c0fef65aadf1f7c01bc20304f16',\n"," 'text': 'Release Notes',\n"," 'metadata': {'image_url': 'https://img.shields.io/github/release/langchain-ai/langchain?style=flat-square',\n","  'link_texts': ['Release Notes'],\n","  'link_urls': ['https://github.com/langchain-ai/langchain/releases'],\n","  'languages': ['eng'],\n","  'file_directory': '.',\n","  'filename': 'README.md',\n","  'filetype': 'text/markdown',\n","  'last_modified': '2025-09-03T11:55:57'}}"]},"metadata":{},"execution_count":20}],"source":["docs[0].to_dict()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FDMICcgV_GiF","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2e18f5b5-36c4-4e5b-e4a2-4646c284b0a3","executionInfo":{"status":"ok","timestamp":1756900600322,"user_tz":-330,"elapsed":34,"user":{"displayName":"Prashant Sahu","userId":"17861245123904241513"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'type': 'Image',\n"," 'element_id': '594c7e3d43abeaa71d7a8d9ef076568e',\n"," 'text': 'PyPI - License',\n"," 'metadata': {'image_url': 'https://img.shields.io/pypi/l/langchain-core?style=flat-square',\n","  'link_texts': ['PyPI - License'],\n","  'link_urls': ['https://opensource.org/licenses/MIT'],\n","  'languages': ['eng'],\n","  'file_directory': '.',\n","  'filename': 'README.md',\n","  'filetype': 'text/markdown',\n","  'last_modified': '2025-09-03T11:55:57'}}"]},"metadata":{},"execution_count":21}],"source":["docs[1].to_dict()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g87ZcUNG_Ka_","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ee80ae2a-4572-464b-b48a-736206c5e0c9","executionInfo":{"status":"ok","timestamp":1756900601387,"user_tz":-330,"elapsed":68,"user":{"displayName":"Prashant Sahu","userId":"17861245123904241513"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Document(metadata={'image_url': 'https://img.shields.io/github/release/langchain-ai/langchain?style=flat-square', 'link_texts': ['Release Notes'], 'link_urls': ['https://github.com/langchain-ai/langchain/releases'], 'languages': ['eng'], 'file_directory': '.', 'filename': 'README.md', 'filetype': 'text/markdown', 'last_modified': '2025-09-03T11:55:57'}, page_content='Release Notes'),\n"," Document(metadata={'image_url': 'https://img.shields.io/pypi/l/langchain-core?style=flat-square', 'link_texts': ['PyPI - License'], 'link_urls': ['https://opensource.org/licenses/MIT'], 'languages': ['eng'], 'file_directory': '.', 'filename': 'README.md', 'filetype': 'text/markdown', 'last_modified': '2025-09-03T11:55:57'}, page_content='PyPI - License'),\n"," Document(metadata={'image_url': 'https://img.shields.io/pepy/dt/langchain', 'link_texts': ['PyPI - Downloads'], 'link_urls': ['https://pypistats.org/packages/langchain-core'], 'languages': ['eng'], 'file_directory': '.', 'filename': 'README.md', 'filetype': 'text/markdown', 'last_modified': '2025-09-03T11:55:57'}, page_content='PyPI - Downloads'),\n"," Document(metadata={'image_url': 'https://img.shields.io/github/stars/langchain-ai/langchain?style=flat-square', 'link_texts': ['GitHub star chart'], 'link_urls': ['https://star-history.com/#langchain-ai/langchain'], 'languages': ['eng'], 'file_directory': '.', 'filename': 'README.md', 'filetype': 'text/markdown', 'last_modified': '2025-09-03T11:55:57'}, page_content='GitHub star chart'),\n"," Document(metadata={'image_url': 'https://img.shields.io/static/v1?label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode&style=flat-square', 'link_texts': ['Open in Dev Containers'], 'link_urls': ['https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/langchain-ai/langchain'], 'languages': ['eng'], 'file_directory': '.', 'filename': 'README.md', 'filetype': 'text/markdown', 'last_modified': '2025-09-03T11:55:57'}, page_content='Open in Dev Containers'),\n"," Document(metadata={'image_url': 'https://github.com/codespaces/badge.svg', 'link_texts': ['Open in Github Codespace'], 'link_urls': ['https://codespaces.new/langchain-ai/langchain'], 'languages': ['eng'], 'file_directory': '.', 'filename': 'README.md', 'filetype': 'text/markdown', 'last_modified': '2025-09-03T11:55:57'}, page_content='Open in Github Codespace'),\n"," Document(metadata={'image_url': 'https://img.shields.io/endpoint?url=https://codspeed.io/badge.json', 'link_texts': ['CodSpeed Badge'], 'link_urls': ['https://codspeed.io/langchain-ai/langchain'], 'languages': ['eng'], 'file_directory': '.', 'filename': 'README.md', 'filetype': 'text/markdown', 'last_modified': '2025-09-03T11:55:57'}, page_content='CodSpeed Badge'),\n"," Document(metadata={'image_url': 'https://img.shields.io/twitter/url/https/twitter.com/langchainai.svg?style=social&label=Follow%20%40LangChainAI', 'link_texts': ['Twitter'], 'link_urls': ['https://twitter.com/langchainai'], 'languages': ['eng'], 'file_directory': '.', 'filename': 'README.md', 'filetype': 'text/markdown', 'last_modified': '2025-09-03T11:55:57'}, page_content='Twitter'),\n"," Document(metadata={'link_texts': ['LangChain.js'], 'link_urls': ['https://github.com/langchain-ai/langchainjs'], 'languages': ['eng'], 'file_directory': '.', 'filename': 'README.md', 'filetype': 'text/markdown', 'last_modified': '2025-09-03T11:55:57'}, page_content='[!NOTE] Looking for the JS/TS library? Check out LangChain.js.'),\n"," Document(metadata={'languages': ['eng'], 'file_directory': '.', 'filename': 'README.md', 'filetype': 'text/markdown', 'last_modified': '2025-09-03T11:55:57'}, page_content='LangChain is a framework for building LLM-powered applications. It helps you chain together interoperable components and third-party integrations to simplify AI application development — all while future-proofing decisions as the underlying technology evolves.')]"]},"metadata":{},"execution_count":22}],"source":["from langchain_core.documents import Document\n","\n","lc_docs = [Document(page_content=doc.text,\n","                    metadata=doc.metadata.to_dict())\n","              for doc in docs]\n","lc_docs[:10]"]},{"cell_type":"markdown","metadata":{"id":"ZSSBO_Y3fB8P"},"source":["### CSV Loader\n","\n","A comma-separated values (CSV) file is a delimited text file that uses a comma to separate values. Each line of the file is a data record. Each record consists of one or more fields, separated by commas.\n","\n","LangChain implements a CSV Loader that will load CSV files into a sequence of `Document` objects. Each row of the CSV file is converted to one document."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QuoCrfQUODLt"},"outputs":[],"source":["import pandas as pd\n","\n","# Create a DataFrame with some dummy real estate data\n","data = {\n","    'Property_ID': [101, 102, 103, 104, 105],\n","    'Address': ['123 Elm St', '456 Oak St', '789 Pine St', '321 Maple St', '654 Cedar St'],\n","    'City': ['Springfield', 'Rivertown', 'Laketown', 'Hillside', 'Sunnyvale'],\n","    'State': ['CA', 'TX', 'FL', 'NY', 'CO'],\n","    'Zip_Code': [98765, 87654, 76543, 65432, 54321],\n","    'Bedrooms': [3, 2, 4, 3, 5],\n","    'Bathrooms': [2, 1, 3, 2, 4],\n","    'Listing_Price': [500000, 350000, 600000, 475000, 750000]\n","}\n","\n","df = pd.DataFrame(data)\n","\n","# Save the DataFrame to a CSV file\n","df.to_csv('data.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hG-_cJFULKI3"},"outputs":[],"source":["from langchain_community.document_loaders.csv_loader import CSVLoader\n","\n","loader = CSVLoader(file_path=\"./data.csv\")\n","docs = loader.load()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dgrc_HhzLKMF","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9fe21dbd-c736-43ca-b745-f148bc9a6e06","executionInfo":{"status":"ok","timestamp":1756900606257,"user_tz":-330,"elapsed":63,"user":{"displayName":"Prashant Sahu","userId":"17861245123904241513"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Document(metadata={'source': './data.csv', 'row': 0}, page_content='Property_ID: 101\\nAddress: 123 Elm St\\nCity: Springfield\\nState: CA\\nZip_Code: 98765\\nBedrooms: 3\\nBathrooms: 2\\nListing_Price: 500000'),\n"," Document(metadata={'source': './data.csv', 'row': 1}, page_content='Property_ID: 102\\nAddress: 456 Oak St\\nCity: Rivertown\\nState: TX\\nZip_Code: 87654\\nBedrooms: 2\\nBathrooms: 1\\nListing_Price: 350000'),\n"," Document(metadata={'source': './data.csv', 'row': 2}, page_content='Property_ID: 103\\nAddress: 789 Pine St\\nCity: Laketown\\nState: FL\\nZip_Code: 76543\\nBedrooms: 4\\nBathrooms: 3\\nListing_Price: 600000'),\n"," Document(metadata={'source': './data.csv', 'row': 3}, page_content='Property_ID: 104\\nAddress: 321 Maple St\\nCity: Hillside\\nState: NY\\nZip_Code: 65432\\nBedrooms: 3\\nBathrooms: 2\\nListing_Price: 475000'),\n"," Document(metadata={'source': './data.csv', 'row': 4}, page_content='Property_ID: 105\\nAddress: 654 Cedar St\\nCity: Sunnyvale\\nState: CO\\nZip_Code: 54321\\nBedrooms: 5\\nBathrooms: 4\\nListing_Price: 750000')]"]},"metadata":{},"execution_count":25}],"source":["docs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R8vVzwmseaoH","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f3f96f70-e733-48fc-d340-b80912cddefc","executionInfo":{"status":"ok","timestamp":1756900607196,"user_tz":-330,"elapsed":5,"user":{"displayName":"Prashant Sahu","userId":"17861245123904241513"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Document(metadata={'source': './data.csv', 'row': 0}, page_content='Property_ID: 101\\nAddress: 123 Elm St\\nCity: Springfield\\nState: CA\\nZip_Code: 98765\\nBedrooms: 3\\nBathrooms: 2\\nListing_Price: 500000')"]},"metadata":{},"execution_count":26}],"source":["docs[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LdENGJXQOXA-","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0f728ad4-4115-4546-f0f4-7c5d1ae4e96d","executionInfo":{"status":"ok","timestamp":1756900608522,"user_tz":-330,"elapsed":103,"user":{"displayName":"Prashant Sahu","userId":"17861245123904241513"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Property_ID: 101\n","Address: 123 Elm St\n","City: Springfield\n","State: CA\n","Zip_Code: 98765\n","Bedrooms: 3\n","Bathrooms: 2\n","Listing_Price: 500000\n"]}],"source":["print(docs[0].page_content)"]},{"cell_type":"markdown","metadata":{"id":"4HVFIEiiB1Wu"},"source":["`CSVLoader` will accept a `csv_args` kwarg that supports customization of arguments passed to Python's csv.`DictReader`. See the [`csv` module](https://docs.python.org/3/library/csv.html) documentation for more information of what `csv` args are supported."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XxLiCAl2CHE2"},"outputs":[],"source":["loader = CSVLoader(file_path=\"./data.csv\",\n","                   csv_args={\n","                      \"delimiter\": \",\",\n","                      \"quotechar\": '\"',\n","                      \"fieldnames\": [\"Property ID\", \"Address\", \"City\", \"State\",\n","                                     \"Zip Code\", \"Bedrooms\", \"Bathrooms\", \"Price\"],\n","                   },\n","                  )\n","docs = loader.load()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8WuekvnIDShM","colab":{"base_uri":"https://localhost:8080/"},"outputId":"fb8cdda9-9354-48e0-a046-54bdc352b3cb","executionInfo":{"status":"ok","timestamp":1756900611040,"user_tz":-330,"elapsed":18,"user":{"displayName":"Prashant Sahu","userId":"17861245123904241513"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Document(metadata={'source': './data.csv', 'row': 0}, page_content='Property ID: Property_ID\\nAddress: Address\\nCity: City\\nState: State\\nZip Code: Zip_Code\\nBedrooms: Bedrooms\\nBathrooms: Bathrooms\\nPrice: Listing_Price'),\n"," Document(metadata={'source': './data.csv', 'row': 1}, page_content='Property ID: 101\\nAddress: 123 Elm St\\nCity: Springfield\\nState: CA\\nZip Code: 98765\\nBedrooms: 3\\nBathrooms: 2\\nPrice: 500000'),\n"," Document(metadata={'source': './data.csv', 'row': 2}, page_content='Property ID: 102\\nAddress: 456 Oak St\\nCity: Rivertown\\nState: TX\\nZip Code: 87654\\nBedrooms: 2\\nBathrooms: 1\\nPrice: 350000'),\n"," Document(metadata={'source': './data.csv', 'row': 3}, page_content='Property ID: 103\\nAddress: 789 Pine St\\nCity: Laketown\\nState: FL\\nZip Code: 76543\\nBedrooms: 4\\nBathrooms: 3\\nPrice: 600000'),\n"," Document(metadata={'source': './data.csv', 'row': 4}, page_content='Property ID: 104\\nAddress: 321 Maple St\\nCity: Hillside\\nState: NY\\nZip Code: 65432\\nBedrooms: 3\\nBathrooms: 2\\nPrice: 475000'),\n"," Document(metadata={'source': './data.csv', 'row': 5}, page_content='Property ID: 105\\nAddress: 654 Cedar St\\nCity: Sunnyvale\\nState: CO\\nZip Code: 54321\\nBedrooms: 5\\nBathrooms: 4\\nPrice: 750000')]"]},"metadata":{},"execution_count":29}],"source":["docs"]},{"cell_type":"markdown","metadata":{"id":"VXseBM98BgkG"},"source":["Unstructured.io loads the entire CSV as a single table"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"89SAdDs7A4eR"},"outputs":[],"source":["from langchain_community.document_loaders import UnstructuredCSVLoader\n","\n","loader = UnstructuredCSVLoader(\"./data.csv\")\n","docs = loader.load()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uvic6kW8BEn3","colab":{"base_uri":"https://localhost:8080/"},"outputId":"30e7b1f7-7adf-4c07-8ba4-41c553b1c38f","executionInfo":{"status":"ok","timestamp":1756900613861,"user_tz":-330,"elapsed":14,"user":{"displayName":"Prashant Sahu","userId":"17861245123904241513"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["1"]},"metadata":{},"execution_count":31}],"source":["len(docs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"54gFzsugBnad","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1e6c1127-e956-43c6-a6df-843a5ea00801","executionInfo":{"status":"ok","timestamp":1756900615037,"user_tz":-330,"elapsed":26,"user":{"displayName":"Prashant Sahu","userId":"17861245123904241513"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Document(metadata={'source': './data.csv'}, page_content='Property_ID Address City State Zip_Code Bedrooms Bathrooms Listing_Price 101 123 Elm St Springfield CA 98765 3 2 500000 102 456 Oak St Rivertown TX 87654 2 1 350000 103 789 Pine St Laketown FL 76543 4 3 600000 104 321 Maple St Hillside NY 65432 3 2 475000 105 654 Cedar St Sunnyvale CO 54321 5 4 750000')"]},"metadata":{},"execution_count":32}],"source":["docs[0]"]},{"cell_type":"markdown","metadata":{"id":"DrHu4ts4fHgH"},"source":["### JSON Loader\n","\n","[JSON (JavaScript Object Notation)](https://en.wikipedia.org/wiki/JSON) is an open standard file format and data interchange format that uses human-readable text to store and transmit data objects consisting of attribute–value pairs and arrays (or other serializable values).\n","\n","[JSON Lines](https://jsonlines.org/) is a file format where each line is a valid JSON value.\n","\n","LangChain implements a [JSONLoader](https://api.python.langchain.com/en/latest/document_loaders/langchain_community.document_loaders.json_loader.JSONLoader.html) to convert JSON and JSONL data into LangChain `Document` objects. It uses a specified [`jq` schema](https://en.wikipedia.org/wiki/Jq_(programming_language)) to parse the JSON files, allowing for the extraction of specific fields into the content and metadata of the LangChain Document.\n","\n","It uses the `jq` python package. Check out [this manual](https://jqlang.github.io/jq/manual/) for a detailed documentation of the `jq` syntax."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ALFBPgsnOd0L"},"outputs":[],"source":["import json\n","\n","# Sample data dictionary similar to the one you provided but with modified contents\n","data = {\n","    'image': {'creation_timestamp': 1675549016, 'uri': 'image_of_the_meeting.jpg'},\n","    'is_still_participant': True,\n","    'joinable_mode': {'link': '', 'mode': 1},\n","    'magic_words': [],\n","    'messages': [\n","        {'content': 'See you soon!',\n","         'sender_name': 'User B',\n","         'timestamp_ms': 1675597571851},\n","        {'content': 'Thanks for the update! See you then.',\n","         'sender_name': 'User A',\n","         'timestamp_ms': 1675597435669},\n","        {'content': 'Actually, the green one is sold out.',\n","         'sender_name': 'User B',\n","         'timestamp_ms': 1675596277579},\n","        {'content': 'I was hoping to purchase the green one!',\n","         'sender_name': 'User A',\n","         'timestamp_ms': 1675595140251},\n","        {'content': 'I’m really interested in the green one, not the red!',\n","         'sender_name': 'User A',\n","         'timestamp_ms': 1675595109305},\n","        {'content': 'Here’s the $150 for it.',\n","         'sender_name': 'User B',\n","         'timestamp_ms': 1675595068468},\n","        {'photos': [{'creation_timestamp': 1675595059,\n","                     'uri': 'image_of_the_item.jpg'}],\n","         'sender_name': 'User B',\n","         'timestamp_ms': 1675595060730},\n","        {'content': 'It typically sells for at least $200 online',\n","         'sender_name': 'User B',\n","         'timestamp_ms': 1675595045152},\n","        {'content': 'How much are you asking?',\n","         'sender_name': 'User A',\n","         'timestamp_ms': 1675594799696},\n","        {'content': 'Good morning! $50 is far too low.',\n","         'sender_name': 'User B',\n","         'timestamp_ms': 1675577876645},\n","        {'content': 'Hello! I’m interested in the item you posted. I can offer $50. Let me know if that works for you. Thanks!',\n","         'sender_name': 'User A',\n","         'timestamp_ms': 1675549022673}\n","    ],\n","    'participants': [{'name': 'User A'}, {'name': 'User B'}],\n","    'thread_path': 'inbox/User A and User B chat',\n","    'title': 'User A and User B chat'\n","}\n","\n","# Save the modified data to a JSON file\n","with open('chat_data.json', 'w') as file:\n","    json.dump(data, file, indent=4)\n"]},{"cell_type":"markdown","metadata":{"id":"bZ9YwY4xG7KD"},"source":["To load the full data as a single document"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aD1kjnQRQ0h5"},"outputs":[],"source":["from langchain_community.document_loaders import JSONLoader\n","\n","loader = JSONLoader(\n","    file_path='./chat_data.json',\n","    jq_schema='.',\n","    text_content=False)\n","\n","data = loader.load()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EQIAvS8rGpNx","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c904dc59-ac98-4319-93e6-8c1bb477bba0","executionInfo":{"status":"ok","timestamp":1756900629012,"user_tz":-330,"elapsed":231,"user":{"displayName":"Prashant Sahu","userId":"17861245123904241513"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["1"]},"metadata":{},"execution_count":35}],"source":["len(data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dPmHC3aYRgFA","colab":{"base_uri":"https://localhost:8080/"},"outputId":"447cfc1c-8992-4b44-81f5-bbe35f1d236d","executionInfo":{"status":"ok","timestamp":1756900630062,"user_tz":-330,"elapsed":49,"user":{"displayName":"Prashant Sahu","userId":"17861245123904241513"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Document(metadata={'source': '/content/chat_data.json', 'seq_num': 1}, page_content='{\"image\": {\"creation_timestamp\": 1675549016, \"uri\": \"image_of_the_meeting.jpg\"}, \"is_still_participant\": true, \"joinable_mode\": {\"link\": \"\", \"mode\": 1}, \"magic_words\": [], \"messages\": [{\"content\": \"See you soon!\", \"sender_name\": \"User B\", \"timestamp_ms\": 1675597571851}, {\"content\": \"Thanks for the update! See you then.\", \"sender_name\": \"User A\", \"timestamp_ms\": 1675597435669}, {\"content\": \"Actually, the green one is sold out.\", \"sender_name\": \"User B\", \"timestamp_ms\": 1675596277579}, {\"content\": \"I was hoping to purchase the green one!\", \"sender_name\": \"User A\", \"timestamp_ms\": 1675595140251}, {\"content\": \"I\\\\u2019m really interested in the green one, not the red!\", \"sender_name\": \"User A\", \"timestamp_ms\": 1675595109305}, {\"content\": \"Here\\\\u2019s the $150 for it.\", \"sender_name\": \"User B\", \"timestamp_ms\": 1675595068468}, {\"photos\": [{\"creation_timestamp\": 1675595059, \"uri\": \"image_of_the_item.jpg\"}], \"sender_name\": \"User B\", \"timestamp_ms\": 1675595060730}, {\"content\": \"It typically sells for at least $200 online\", \"sender_name\": \"User B\", \"timestamp_ms\": 1675595045152}, {\"content\": \"How much are you asking?\", \"sender_name\": \"User A\", \"timestamp_ms\": 1675594799696}, {\"content\": \"Good morning! $50 is far too low.\", \"sender_name\": \"User B\", \"timestamp_ms\": 1675577876645}, {\"content\": \"Hello! I\\\\u2019m interested in the item you posted. I can offer $50. Let me know if that works for you. Thanks!\", \"sender_name\": \"User A\", \"timestamp_ms\": 1675549022673}], \"participants\": [{\"name\": \"User A\"}, {\"name\": \"User B\"}], \"thread_path\": \"inbox/User A and User B chat\", \"title\": \"User A and User B chat\"}')]"]},"metadata":{},"execution_count":36}],"source":["data"]},{"cell_type":"markdown","metadata":{"id":"TY48CWHhG_KF"},"source":["Suppose we are interested in extracting the values under the `messages` key of the JSON data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VC5cWidHSFuR","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b5adefe7-0ea3-4c3f-d51d-39abce05f999","executionInfo":{"status":"ok","timestamp":1756900631846,"user_tz":-330,"elapsed":166,"user":{"displayName":"Prashant Sahu","userId":"17861245123904241513"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Document(metadata={'source': '/content/chat_data.json', 'seq_num': 1}, page_content='{\"content\": \"See you soon!\", \"sender_name\": \"User B\", \"timestamp_ms\": 1675597571851}'),\n"," Document(metadata={'source': '/content/chat_data.json', 'seq_num': 2}, page_content='{\"content\": \"Thanks for the update! See you then.\", \"sender_name\": \"User A\", \"timestamp_ms\": 1675597435669}'),\n"," Document(metadata={'source': '/content/chat_data.json', 'seq_num': 3}, page_content='{\"content\": \"Actually, the green one is sold out.\", \"sender_name\": \"User B\", \"timestamp_ms\": 1675596277579}'),\n"," Document(metadata={'source': '/content/chat_data.json', 'seq_num': 4}, page_content='{\"content\": \"I was hoping to purchase the green one!\", \"sender_name\": \"User A\", \"timestamp_ms\": 1675595140251}'),\n"," Document(metadata={'source': '/content/chat_data.json', 'seq_num': 5}, page_content='{\"content\": \"I\\\\u2019m really interested in the green one, not the red!\", \"sender_name\": \"User A\", \"timestamp_ms\": 1675595109305}'),\n"," Document(metadata={'source': '/content/chat_data.json', 'seq_num': 6}, page_content='{\"content\": \"Here\\\\u2019s the $150 for it.\", \"sender_name\": \"User B\", \"timestamp_ms\": 1675595068468}'),\n"," Document(metadata={'source': '/content/chat_data.json', 'seq_num': 7}, page_content='{\"photos\": [{\"creation_timestamp\": 1675595059, \"uri\": \"image_of_the_item.jpg\"}], \"sender_name\": \"User B\", \"timestamp_ms\": 1675595060730}'),\n"," Document(metadata={'source': '/content/chat_data.json', 'seq_num': 8}, page_content='{\"content\": \"It typically sells for at least $200 online\", \"sender_name\": \"User B\", \"timestamp_ms\": 1675595045152}'),\n"," Document(metadata={'source': '/content/chat_data.json', 'seq_num': 9}, page_content='{\"content\": \"How much are you asking?\", \"sender_name\": \"User A\", \"timestamp_ms\": 1675594799696}'),\n"," Document(metadata={'source': '/content/chat_data.json', 'seq_num': 10}, page_content='{\"content\": \"Good morning! $50 is far too low.\", \"sender_name\": \"User B\", \"timestamp_ms\": 1675577876645}'),\n"," Document(metadata={'source': '/content/chat_data.json', 'seq_num': 11}, page_content='{\"content\": \"Hello! I\\\\u2019m interested in the item you posted. I can offer $50. Let me know if that works for you. Thanks!\", \"sender_name\": \"User A\", \"timestamp_ms\": 1675549022673}')]"]},"metadata":{},"execution_count":37}],"source":["loader = JSONLoader(\n","    file_path='./chat_data.json',\n","    jq_schema='.messages[]',\n","    text_content=False)\n","\n","data = loader.load()\n","data"]},{"cell_type":"markdown","metadata":{"id":"HK59yCK3H-C-"},"source":["Suppose we are interested in extracting the values under the `content` field within the `messages` key of the JSON data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n7Bq2FHlSVb7","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0c52b9c8-aed4-41ef-c59b-1c532df6066e","executionInfo":{"status":"ok","timestamp":1756900634877,"user_tz":-330,"elapsed":43,"user":{"displayName":"Prashant Sahu","userId":"17861245123904241513"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Document(metadata={'source': '/content/chat_data.json', 'seq_num': 1}, page_content='See you soon!'),\n"," Document(metadata={'source': '/content/chat_data.json', 'seq_num': 2}, page_content='Thanks for the update! See you then.'),\n"," Document(metadata={'source': '/content/chat_data.json', 'seq_num': 3}, page_content='Actually, the green one is sold out.'),\n"," Document(metadata={'source': '/content/chat_data.json', 'seq_num': 4}, page_content='I was hoping to purchase the green one!'),\n"," Document(metadata={'source': '/content/chat_data.json', 'seq_num': 5}, page_content='I’m really interested in the green one, not the red!'),\n"," Document(metadata={'source': '/content/chat_data.json', 'seq_num': 6}, page_content='Here’s the $150 for it.'),\n"," Document(metadata={'source': '/content/chat_data.json', 'seq_num': 7}, page_content=''),\n"," Document(metadata={'source': '/content/chat_data.json', 'seq_num': 8}, page_content='It typically sells for at least $200 online'),\n"," Document(metadata={'source': '/content/chat_data.json', 'seq_num': 9}, page_content='How much are you asking?'),\n"," Document(metadata={'source': '/content/chat_data.json', 'seq_num': 10}, page_content='Good morning! $50 is far too low.'),\n"," Document(metadata={'source': '/content/chat_data.json', 'seq_num': 11}, page_content='Hello! I’m interested in the item you posted. I can offer $50. Let me know if that works for you. Thanks!')]"]},"metadata":{},"execution_count":38}],"source":["loader = JSONLoader(\n","    file_path='./chat_data.json',\n","    jq_schema='.messages[].content',\n","    text_content=False)\n","\n","data = loader.load()\n","data"]},{"cell_type":"markdown","metadata":{"id":"tZB2fxI9fKxC"},"source":["### PDF Loaders\n","\n","[Portable Document Format (PDF)](https://en.wikipedia.org/wiki/PDF), standardized as ISO 32000, is a file format developed by Adobe in 1992 to present documents, including text formatting and images, in a manner independent of application software, hardware, and operating systems.\n","\n","LangChain integrates with a host of PDF parsers. Some are simple and relatively low-level; others will support OCR and image-processing, or perform advanced document layout analysis. The right choice will depend on your use-case and through experimentation.\n","\n","Here we will see how to load PDF documents into the LangChain `Document` format\n","\n","We download a research paper to experiment with"]},{"cell_type":"markdown","metadata":{"id":"p2NWiC51KDbm"},"source":["If the following command fails you can download the paper manually by going to http://arxiv.org/pdf/2103.15348.pdf, save it as `layoutparser_paper.pdf`and upload it on the left in Colab from the upload files option"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t_zMe1cES7Tb","colab":{"base_uri":"https://localhost:8080/"},"outputId":"6d3ef678-4221-4c78-80ba-9a669f48ca4d","executionInfo":{"status":"ok","timestamp":1756900640586,"user_tz":-330,"elapsed":1435,"user":{"displayName":"Prashant Sahu","userId":"17861245123904241513"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-09-03 11:57:19--  http://arxiv.org/pdf/2103.15348.pdf\n","Resolving arxiv.org (arxiv.org)... 151.101.195.42, 151.101.131.42, 151.101.67.42, ...\n","Connecting to arxiv.org (arxiv.org)|151.101.195.42|:80... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: https://arxiv.org/pdf/2103.15348.pdf [following]\n","--2025-09-03 11:57:19--  https://arxiv.org/pdf/2103.15348.pdf\n","Connecting to arxiv.org (arxiv.org)|151.101.195.42|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: /pdf/2103.15348 [following]\n","--2025-09-03 11:57:19--  https://arxiv.org/pdf/2103.15348\n","Reusing existing connection to arxiv.org:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 4686220 (4.5M) [application/pdf]\n","Saving to: ‘layoutparser_paper.pdf’\n","\n","layoutparser_paper. 100%[===================>]   4.47M  8.46MB/s    in 0.5s    \n","\n","2025-09-03 11:57:20 (8.46 MB/s) - ‘layoutparser_paper.pdf’ saved [4686220/4686220]\n","\n"]}],"source":["!wget -O 'layoutparser_paper.pdf' 'http://arxiv.org/pdf/2103.15348.pdf'"]},{"cell_type":"markdown","metadata":{"id":"Qs1ZBxbxfNXq"},"source":["#### PyPDFLoader\n","\n","Here we load a PDF using `pypdf` into list of documents, where each document contains the page content and metadata with page number. Typically each PDF page becomes one document"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a0be_BLqTI0H","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d64f6eb8-b287-4d7c-8163-316bfae292e4","executionInfo":{"status":"ok","timestamp":1756900773484,"user_tz":-330,"elapsed":975,"user":{"displayName":"Prashant Sahu","userId":"17861245123904241513"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/pypdf/_crypt_providers/_cryptography.py:32: CryptographyDeprecationWarning: ARC4 has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.ARC4 and will be removed from this module in 48.0.0.\n","  from cryptography.hazmat.primitives.ciphers.algorithms import AES, ARC4\n"]}],"source":["from langchain_community.document_loaders import PyPDFLoader\n","\n","loader = PyPDFLoader(\"./layoutparser_paper.pdf\")\n","pages = loader.load()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gJZo2Hk9hMgb","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ac42ca1d-1070-45e0-e0d7-4192a429f10f","executionInfo":{"status":"ok","timestamp":1756900774906,"user_tz":-330,"elapsed":133,"user":{"displayName":"Prashant Sahu","userId":"17861245123904241513"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["16"]},"metadata":{},"execution_count":41}],"source":["len(pages)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XrqsL5lhToUY","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b8184971-1925-484f-ba36-c55450bece99","executionInfo":{"status":"ok","timestamp":1756900775211,"user_tz":-330,"elapsed":96,"user":{"displayName":"Prashant Sahu","userId":"17861245123904241513"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-22T01:27:10+00:00', 'author': '', 'keywords': '', 'moddate': '2021-06-22T01:27:10+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': './layoutparser_paper.pdf', 'total_pages': 16, 'page': 0, 'page_label': '1'}, page_content='LayoutParser : A Uniﬁed Toolkit for Deep\\nLearning Based Document Image Analysis\\nZejiang Shen1( \\x00), Ruochen Zhang2, Melissa Dell3, Benjamin Charles Germain\\nLee4, Jacob Carlson3, and Weining Li5\\n1Allen Institute for AI\\nshannons@allenai.org\\n2Brown University\\nruochen zhang@brown.edu\\n3Harvard University\\n{melissadell,jacob carlson }@fas.harvard.edu\\n4University of Washington\\nbcgl@cs.washington.edu\\n5University of Waterloo\\nw422li@uwaterloo.ca\\nAbstract. Recent advances in document image analysis (DIA) have been\\nprimarily driven by the application of neural networks. Ideally, research\\noutcomes could be easily deployed in production and extended for further\\ninvestigation. However, various factors like loosely organized codebases\\nand sophisticated model conﬁgurations complicate the easy reuse of im-\\nportant innovations by a wide audience. Though there have been on-going\\neﬀorts to improve reusability and simplify deep learning (DL) model\\ndevelopment in disciplines like natural language processing and computer\\nvision, none of them are optimized for challenges in the domain of DIA.\\nThis represents a major gap in the existing toolkit, as DIA is central to\\nacademic research across a wide range of disciplines in the social sciences\\nand humanities. This paper introduces LayoutParser , an open-source\\nlibrary for streamlining the usage of DL in DIA research and applica-\\ntions. The core LayoutParser library comes with a set of simple and\\nintuitive interfaces for applying and customizing DL models for layout de-\\ntection, character recognition, and many other document processing tasks.\\nTo promote extensibility, LayoutParser also incorporates a community\\nplatform for sharing both pre-trained models and full document digiti-\\nzation pipelines. We demonstrate that LayoutParser is helpful for both\\nlightweight and large-scale digitization pipelines in real-word use cases.\\nThe library is publicly available at https://layout-parser.github.io .\\nKeywords: Document Image Analysis ·Deep Learning ·Layout Analysis\\n·Character Recognition ·Open Source library ·Toolkit.\\n1 Introduction\\nDeep Learning(DL)-based approaches are the state-of-the-art for a wide range of\\ndocument image analysis (DIA) tasks including document image classiﬁcation [ 11,arXiv:2103.15348v2  [cs.CV]  21 Jun 2021')"]},"metadata":{},"execution_count":42}],"source":["pages[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8xU5qyqJT7TB","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8b272c3a-1259-4cd0-de02-783e9f8c5dcb","executionInfo":{"status":"ok","timestamp":1756900776294,"user_tz":-330,"elapsed":39,"user":{"displayName":"Prashant Sahu","userId":"17861245123904241513"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["LayoutParser : A Uniﬁed Toolkit for Deep\n","Learning Based Document Image Analysis\n","Zejiang Shen1( \u0000), Ruochen Zhang2, Melissa Dell3, Benjamin Charles Germain\n","Lee4, Jacob Carlson3, and Weining Li5\n","1Allen Institute for AI\n","shannons@allenai.org\n","2Brown University\n","ruochen zhang@brown.edu\n","3Harvard University\n","{melissadell,jacob carlson }@fas.harvard.edu\n","4University of Washington\n","bcgl@cs.washington.edu\n","5University of Waterloo\n","w422li@uwaterloo.ca\n","Abstract. Recent advances in document image analysis (DIA) have been\n","primarily driven by the application of neural networks. Ideally, research\n","outcomes could be easily deployed in production and extended for further\n","investigation. However, various factors like loosely organized codebases\n","and sophisticated model conﬁgurations complicate the easy reuse of im-\n","portant innovations by a wide audience. Though there have been on-going\n","eﬀorts to improve reusability and simplify deep learning (DL) model\n","development in disciplines like natural language processing and computer\n","vision, none of them are optimized for challenges in the domain of DIA.\n","This represents a major gap in the existing toolkit, as DIA is central to\n","academic research across a wide range of disciplines in the social sciences\n","and humanities. This paper introduces LayoutParser , an open-source\n","library for streamlining the usage of DL in DIA research and applica-\n","tions. The core LayoutParser library comes with a set of simple and\n","intuitive interfaces for applying and customizing DL models for layout de-\n","tection, character recognition, and many other document processing tasks.\n","To promote extensibility, LayoutParser also incorporates a community\n","platform for sharing both pre-trained models and full document digiti-\n","zation pipelines. We demonstrate that LayoutParser is helpful for both\n","lightweight and large-scale digitization pipelines in real-word use cases.\n","The library is publicly available at https://layout-parser.github.io .\n","Keywords: Document Image Analysis ·Deep Learning ·Layout Analysis\n","·Character Recognition ·Open Source library ·Toolkit.\n","1 Introduction\n","Deep Learning(DL)-based approaches are the state-of-the-art for a wide range of\n","document image analysis (DIA) tasks including document image classiﬁcation [ 11,arXiv:2103.15348v2  [cs.CV]  21 Jun 2021\n"]}],"source":["print(pages[0].page_content)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KcDMgTfBTrZL","colab":{"base_uri":"https://localhost:8080/"},"outputId":"56d4de5e-ebfb-4b18-b5c1-54b6bc809281","executionInfo":{"status":"ok","timestamp":1756900777533,"user_tz":-330,"elapsed":59,"user":{"displayName":"Prashant Sahu","userId":"17861245123904241513"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["LayoutParser : A Uniﬁed Toolkit for DL-Based DIA 5\n","Table 1: Current layout detection models in the LayoutParser model zoo\n","Dataset Base Model1Large Model Notes\n","PubLayNet [38] F / M M Layouts of modern scientiﬁc documents\n","PRImA [3] M - Layouts of scanned modern magazines and scientiﬁc reports\n","Newspaper [17] F - Layouts of scanned US newspapers from the 20th century\n","TableBank [18] F F Table region on modern scientiﬁc and business document\n","HJDataset [31] F / M - Layouts of history Japanese documents\n","1For each dataset, we train several models of diﬀerent sizes for diﬀerent needs (the trade-oﬀ between accuracy\n","vs. computational cost). For “base model” and “large model”, we refer to using the ResNet 50 or ResNet 101\n","backbones [ 13], respectively. One can train models of diﬀerent architectures, like Faster R-CNN [ 28] (F) and Mask\n","R-CNN [ 12] (M). For example, an F in the Large Model column indicates it has a Faster R-CNN model trained\n","using the ResNet 101 backbone. The platform is maintained and a number of additions will be made to the model\n","zoo in coming months.\n","layout data structures , which are optimized for eﬃciency and versatility. 3) When\n","necessary, users can employ existing or customized OCR models via the uniﬁed\n","API provided in the OCR module . 4)LayoutParser comes with a set of utility\n","functions for the visualization and storage of the layout data. 5) LayoutParser\n","is also highly customizable, via its integration with functions for layout data\n","annotation and model training . We now provide detailed descriptions for each\n","component.\n","3.1 Layout Detection Models\n","InLayoutParser , a layout model takes a document image as an input and\n","generates a list of rectangular boxes for the target content regions. Diﬀerent\n","from traditional methods, it relies on deep convolutional neural networks rather\n","than manually curated rules to identify content regions. It is formulated as an\n","object detection problem and state-of-the-art models like Faster R-CNN [ 28] and\n","Mask R-CNN [ 12] are used. This yields prediction results of high accuracy and\n","makes it possible to build a concise, generalized interface for layout detection.\n","LayoutParser , built upon Detectron2 [ 35], provides a minimal API that can\n","perform layout detection with only four lines of code in Python:\n","1import layoutparser as lp\n","2image = cv2. imread (\" image_file \") # load images\n","3model = lp. Detectron2LayoutModel (\n","4 \"lp :// PubLayNet / faster_rcnn_R_50_FPN_3x / config \")\n","5layout = model . detect ( image )\n","LayoutParser provides a wealth of pre-trained model weights using various\n","datasets covering diﬀerent languages, time periods, and document types. Due to\n","domain shift [ 7], the prediction performance can notably drop when models are ap-\n","plied to target samples that are signiﬁcantly diﬀerent from the training dataset. As\n","document structures and layouts vary greatly in diﬀerent domains, it is important\n","to select models trained on a dataset similar to the test samples. A semantic syntax\n","is used for initializing the model weights in LayoutParser , using both the dataset\n","name and model name lp://<dataset-name>/<model-architecture-name> .\n"]}],"source":["print(pages[4].page_content)"]},{"cell_type":"markdown","metadata":{"id":"x_T0_7KtfUJj"},"source":["#### PyMuPDFLoader\n","\n","This is the fastest of the PDF parsing options, and contains detailed metadata about the PDF and its pages, as well as returns one document per page. It uses the `pymupdf` library internally."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l3KTMV_3XmfL"},"outputs":[],"source":["from langchain_community.document_loaders import PyMuPDFLoader\n","\n","loader = PyMuPDFLoader(\"./layoutparser_paper.pdf\")\n","pages = loader.load()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JXUAxQnJhuHO","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f4d93c71-cf4a-4c0e-e71d-69c860036802","executionInfo":{"status":"ok","timestamp":1756900780230,"user_tz":-330,"elapsed":104,"user":{"displayName":"Prashant Sahu","userId":"17861245123904241513"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["16"]},"metadata":{},"execution_count":46}],"source":["len(pages)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CZM5poERdRpL","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7e42125a-c683-4a51-c306-07aa14a54fce","executionInfo":{"status":"ok","timestamp":1756900781941,"user_tz":-330,"elapsed":65,"user":{"displayName":"Prashant Sahu","userId":"17861245123904241513"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-22T01:27:10+00:00', 'source': './layoutparser_paper.pdf', 'file_path': './layoutparser_paper.pdf', 'total_pages': 16, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-22T01:27:10+00:00', 'trapped': '', 'modDate': 'D:20210622012710Z', 'creationDate': 'D:20210622012710Z', 'page': 0}, page_content='LayoutParser: A Uniﬁed Toolkit for Deep\\nLearning Based Document Image Analysis\\nZejiang Shen1 (\\x00), Ruochen Zhang2, Melissa Dell3, Benjamin Charles Germain\\nLee4, Jacob Carlson3, and Weining Li5\\n1 Allen Institute for AI\\nshannons@allenai.org\\n2 Brown University\\nruochen zhang@brown.edu\\n3 Harvard University\\n{melissadell,jacob carlson}@fas.harvard.edu\\n4 University of Washington\\nbcgl@cs.washington.edu\\n5 University of Waterloo\\nw422li@uwaterloo.ca\\nAbstract. Recent advances in document image analysis (DIA) have been\\nprimarily driven by the application of neural networks. Ideally, research\\noutcomes could be easily deployed in production and extended for further\\ninvestigation. However, various factors like loosely organized codebases\\nand sophisticated model conﬁgurations complicate the easy reuse of im-\\nportant innovations by a wide audience. Though there have been on-going\\neﬀorts to improve reusability and simplify deep learning (DL) model\\ndevelopment in disciplines like natural language processing and computer\\nvision, none of them are optimized for challenges in the domain of DIA.\\nThis represents a major gap in the existing toolkit, as DIA is central to\\nacademic research across a wide range of disciplines in the social sciences\\nand humanities. This paper introduces LayoutParser, an open-source\\nlibrary for streamlining the usage of DL in DIA research and applica-\\ntions. The core LayoutParser library comes with a set of simple and\\nintuitive interfaces for applying and customizing DL models for layout de-\\ntection, character recognition, and many other document processing tasks.\\nTo promote extensibility, LayoutParser also incorporates a community\\nplatform for sharing both pre-trained models and full document digiti-\\nzation pipelines. We demonstrate that LayoutParser is helpful for both\\nlightweight and large-scale digitization pipelines in real-word use cases.\\nThe library is publicly available at https://layout-parser.github.io.\\nKeywords: Document Image Analysis · Deep Learning · Layout Analysis\\n· Character Recognition · Open Source library · Toolkit.\\n1\\nIntroduction\\nDeep Learning(DL)-based approaches are the state-of-the-art for a wide range of\\ndocument image analysis (DIA) tasks including document image classiﬁcation [11,\\narXiv:2103.15348v2  [cs.CV]  21 Jun 2021')"]},"metadata":{},"execution_count":47}],"source":["pages[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uhr0Y1C90TH0","colab":{"base_uri":"https://localhost:8080/"},"outputId":"6818bf75-1696-4d90-b8ca-2c372caa660b","executionInfo":{"status":"ok","timestamp":1756900783209,"user_tz":-330,"elapsed":30,"user":{"displayName":"Prashant Sahu","userId":"17861245123904241513"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'producer': 'pdfTeX-1.40.21',\n"," 'creator': 'LaTeX with hyperref',\n"," 'creationdate': '2021-06-22T01:27:10+00:00',\n"," 'source': './layoutparser_paper.pdf',\n"," 'file_path': './layoutparser_paper.pdf',\n"," 'total_pages': 16,\n"," 'format': 'PDF 1.5',\n"," 'title': '',\n"," 'author': '',\n"," 'subject': '',\n"," 'keywords': '',\n"," 'moddate': '2021-06-22T01:27:10+00:00',\n"," 'trapped': '',\n"," 'modDate': 'D:20210622012710Z',\n"," 'creationDate': 'D:20210622012710Z',\n"," 'page': 0}"]},"metadata":{},"execution_count":48}],"source":["pages[0].metadata"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BdaA9hkHXmhs","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b3606cad-4f06-4e09-bd1a-6e38cac32e43","executionInfo":{"status":"ok","timestamp":1756900784531,"user_tz":-330,"elapsed":36,"user":{"displayName":"Prashant Sahu","userId":"17861245123904241513"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["LayoutParser: A Uniﬁed Toolkit for Deep\n","Learning Based Document Image Analysis\n","Zejiang Shen1 (\u0000), Ruochen Zhang2, Melissa Dell3, Benjamin Charles Germain\n","Lee4, Jacob Carlson3, and Weining Li5\n","1 Allen Institute for AI\n","shannons@allenai.org\n","2 Brown University\n","ruochen zhang@brown.edu\n","3 Harvard University\n","{melissadell,jacob carlson}@fas.harvard.edu\n","4 University of Washington\n","bcgl@cs.washington.edu\n","5 University of Waterloo\n","w422li@uwaterloo.ca\n","Abstract. Recent advances in document image analysis (DIA) have been\n","primarily driven by the application of neural networks. Ideally, research\n","outcomes could be easily deployed in production and extended for further\n","investigation. However, various factors like loosely organized codebases\n","and sophisticated model conﬁgurations complicate the easy reuse of im-\n","portant innovations by a wide audience. Though there have been on-going\n","eﬀorts to improve reusability and simplify deep learning (DL) model\n","development in disciplines like natural language processing and computer\n","vision, none of them are optimized for challenges in the domain of DIA.\n","This represents a major gap in the existing toolkit, as DIA is central to\n","academic research across a wide range of disciplines in the social sciences\n","and humanities. This paper introduces LayoutParser, an open-source\n","library for streamlining the usage of DL in DIA research and applica-\n","tions. The core LayoutParser library comes with a set of simple and\n","intuitive interfaces for applying and customizing DL models for layout de-\n","tection, character recognition, and many other document processing tasks.\n","To promote extensibility, LayoutParser also incorporates a community\n","platform for sharing both pre-trained models and full document digiti-\n","zation pipelines. We demonstrate that LayoutParser is helpful for both\n","lightweight and large-scale digitization pipelines in real-word use cases.\n","The library is publicly available at https://layout-parser.github.io.\n","Keywords: Document Image Analysis · Deep Learning · Layout Analysis\n","· Character Recognition · Open Source library · Toolkit.\n","1\n","Introduction\n","Deep Learning(DL)-based approaches are the state-of-the-art for a wide range of\n","document image analysis (DIA) tasks including document image classiﬁcation [11,\n","arXiv:2103.15348v2  [cs.CV]  21 Jun 2021\n"]}],"source":["print(pages[0].page_content)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eDxTzEe0Le6I","colab":{"base_uri":"https://localhost:8080/"},"outputId":"77b03382-b85c-4293-c38a-488854645e9c","executionInfo":{"status":"ok","timestamp":1756900785710,"user_tz":-330,"elapsed":118,"user":{"displayName":"Prashant Sahu","userId":"17861245123904241513"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["LayoutParser: A Uniﬁed Toolkit for DL-Based DIA\n","5\n","Table 1: Current layout detection models in the LayoutParser model zoo\n","Dataset\n","Base Model1 Large Model\n","Notes\n","PubLayNet [38]\n","F / M\n","M\n","Layouts of modern scientiﬁc documents\n","PRImA [3]\n","M\n","-\n","Layouts of scanned modern magazines and scientiﬁc reports\n","Newspaper [17]\n","F\n","-\n","Layouts of scanned US newspapers from the 20th century\n","TableBank [18]\n","F\n","F\n","Table region on modern scientiﬁc and business document\n","HJDataset [31]\n","F / M\n","-\n","Layouts of history Japanese documents\n","1 For each dataset, we train several models of diﬀerent sizes for diﬀerent needs (the trade-oﬀbetween accuracy\n","vs. computational cost). For “base model” and “large model”, we refer to using the ResNet 50 or ResNet 101\n","backbones [13], respectively. One can train models of diﬀerent architectures, like Faster R-CNN [28] (F) and Mask\n","R-CNN [12] (M). For example, an F in the Large Model column indicates it has a Faster R-CNN model trained\n","using the ResNet 101 backbone. The platform is maintained and a number of additions will be made to the model\n","zoo in coming months.\n","layout data structures, which are optimized for eﬃciency and versatility. 3) When\n","necessary, users can employ existing or customized OCR models via the uniﬁed\n","API provided in the OCR module. 4) LayoutParser comes with a set of utility\n","functions for the visualization and storage of the layout data. 5) LayoutParser\n","is also highly customizable, via its integration with functions for layout data\n","annotation and model training. We now provide detailed descriptions for each\n","component.\n","3.1\n","Layout Detection Models\n","In LayoutParser, a layout model takes a document image as an input and\n","generates a list of rectangular boxes for the target content regions. Diﬀerent\n","from traditional methods, it relies on deep convolutional neural networks rather\n","than manually curated rules to identify content regions. It is formulated as an\n","object detection problem and state-of-the-art models like Faster R-CNN [28] and\n","Mask R-CNN [12] are used. This yields prediction results of high accuracy and\n","makes it possible to build a concise, generalized interface for layout detection.\n","LayoutParser, built upon Detectron2 [35], provides a minimal API that can\n","perform layout detection with only four lines of code in Python:\n","1 import\n","layoutparser as lp\n","2 image = cv2.imread(\"image_file\") # load\n","images\n","3 model = lp. Detectron2LayoutModel (\n","4\n","\"lp:// PubLayNet/ faster_rcnn_R_50_FPN_3x /config\")\n","5 layout = model.detect(image)\n","LayoutParser provides a wealth of pre-trained model weights using various\n","datasets covering diﬀerent languages, time periods, and document types. Due to\n","domain shift [7], the prediction performance can notably drop when models are ap-\n","plied to target samples that are signiﬁcantly diﬀerent from the training dataset. As\n","document structures and layouts vary greatly in diﬀerent domains, it is important\n","to select models trained on a dataset similar to the test samples. A semantic syntax\n","is used for initializing the model weights in LayoutParser, using both the dataset\n","name and model name lp://<dataset-name>/<model-architecture-name>.\n"]}],"source":["print(pages[4].page_content)"]},{"cell_type":"markdown","metadata":{"id":"3Zvyk9ACL8fx"},"source":["#### UnstructuredPDFLoader\n","\n","[Unstructured.io](https://unstructured-io.github.io/unstructured/) supports a common interface for working with unstructured or semi-structured file formats, such as Markdown or PDF. LangChain's [`UnstructuredPDFLoader`](https://api.python.langchain.com/en/latest/document_loaders/langchain_community.document_loaders.pdf.UnstructuredPDFLoader.html) integrates with Unstructured to parse PDF documents into LangChain [`Document`](https://api.python.langchain.com/en/latest/documents/langchain_core.documents.base.Document.html) objects."]},{"cell_type":"markdown","metadata":{"id":"uv4PBKyfR2od"},"source":["Load PDF as a single document - no complex parsing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z8qz1HKWMRTf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1756900796643,"user_tz":-330,"elapsed":8829,"user":{"displayName":"Prashant Sahu","userId":"17861245123904241513"}},"outputId":"d9237ff2-8e72-42b3-87a7-8b8c84eac1bf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Warning: No languages specified, defaulting to English.\n"]}],"source":["from langchain_community.document_loaders import UnstructuredPDFLoader\n","\n","loader = UnstructuredPDFLoader('./layoutparser_paper.pdf')\n","data = loader.load()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nDWTGeMFMbI7","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f8b60179-54c5-404d-ed9a-960553fa4460","executionInfo":{"status":"ok","timestamp":1756900796699,"user_tz":-330,"elapsed":24,"user":{"displayName":"Prashant Sahu","userId":"17861245123904241513"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["1"]},"metadata":{},"execution_count":52}],"source":["len(data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UUmEA9qfMdIc","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d3f0b945-4a77-4c2d-e930-5880da99f4e9","executionInfo":{"status":"ok","timestamp":1756900796755,"user_tz":-330,"elapsed":28,"user":{"displayName":"Prashant Sahu","userId":"17861245123904241513"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["1 2 0 2\n","\n","n u J\n","\n","1 2\n","\n","]\n","\n","V C . s c [\n","\n","2 v 8 4 3 5 1 . 3 0 1 2 : v i X r a\n","\n","LayoutParser: A Uniﬁed Toolkit for Deep Learning Based Document Image Analysis\n","\n","Zejiang Shen1 ((cid:0)), Ruochen Zhang2, Melissa Dell3, Benjamin Charles Germain Lee4, Jacob Carlson3, and Weining Li5\n","\n","1 Allen Institute for AI shannons@allenai.org 2 Brown University ruochen zhang@brown.edu 3 Harvard University {melissadell,jacob carlson}@fas.harvard.edu 4 University of Washington bcgl@cs.washington.edu 5 University of Waterloo w422li@uwaterloo.ca\n","\n","Abstract. Recent advances in document image analysis (DIA) have been primarily driven by the application of neural networks. Ideally, research outcomes could be easily deployed in production and extended for further investigation. However, various factors like loosely organized codebases and sophisticated model conﬁgurations complicate the easy reuse of im- portant innovations by a wide audience. Though there have been on-going eﬀorts to improve reusability and simplify d\n"]}],"source":["print(data[0].page_content[:1000])"]},{"cell_type":"markdown","metadata":{"id":"CeB_kHTjR6u5"},"source":["Load PDF with complex parsing, table detection and chunking by sections"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":212,"referenced_widgets":["69edc49bec3b474ebfdc3c6329f1589d","1aa4336ab39e4c3181fecfd0808cadf8","d5c4cb901bd14cbebf6ae78cbf69d25b","f9c28103f75a4110a55f2788240717c3","df06a9a57fa54b728ab21cceaff76e1b","881af4a554fe410ba498cdab150c8426","f5f92c39595a4739b0e70ca6f3213bec","32e6a8d96b7442e1b727a7e434193d4a","92778237882947f9ac51ec2c8f9823c4","638f39df0ee24caabae74e19e2058cd5","a6c8db6c7de741158f9d7ec6ff5e7d5f","b041978e0fd4452c9b2d28d542f2466e","45c5534016694859a73645f9fade2f2a","2868b894ad3b4f5e8e63d33f4b9d690a","c7219f3f8e634a65a56ebf2478780770","51547d43d3bd46e28b7fa1eff73fa2ee","ed225592e2d74b5b9a9cac8b923f161c","8961f78d05644cce9dca059ddcaa3b2d","629ac16e629b43ddab65abfc6b7abd01","70cbd08f030f4907a29770f70e0ca7c1","91c5d5d13fc94917b959d35587d50dbb","3fa6b18395a34aaab65655dbd35bedc5","19c392acd46a42bc981b3e1682ecb8a8","4ed32f13ba454528a6b40b4357403f1e","7088919464924438a410f4917ddfa2ec","48621baf9caf4fa8853aea2c70f952cd","a3142d6d13104befa7e93df1ea5624f3","8086baaf17384d81aa352b922fcc6dae","5a48691f0dfc46b1bc3695f183ab98e5","d479bf0fcb7147ed9592bea7601faacb","d5df623affaf42d2859961ffbbb7ef5a","15f9a4a51371449493fa18b343176f09","378fd25f62f84cb0b4794b3aef72a038","fdc0a8d0da7b4dfaacc52fac46debc1e","c28ab3de323847b2b51767408c92ed3a","ab960021f332469cbe3e27e6bd8395fe","d63b9689725643848fa247bed8e4c3d1","ece582d8020a48febc3055287503c321","7589c7f1d50c42f6bbba966301db05ee","1673074d5d134e7c92b6bf1f8290aac2","146fd670886041b7908f8d1edc7e3799","7fec036937144091acc279aea4f99c15","3240f40fa2724232b8b6f5dacc7801d7","f41f76efbdb3417ca7cfda921f5258fa","60bd2db3afff41fabd142b679177e57a","f0bfecbfd0554a79ab2c7e3dcaf185c3","f925eebf52af48b481628aaf7731a207","04d1c4ff65114c088d2f761a85ca6479","c4a7974fd7a149b0901b1c5b133d8725","ceeee1be7dde4a849284d4da99824b58","053bfb601d3540f1b3e3c3ec1a658e5d","d6f84bb1800c49d6bb69cc9fe0bb35f6","d33ffcea13c845478c24713f2c9852b4","6767f5d1c19541e6b6e055de75ee8317","454c1d9c7d3d4611a9d196ac1794dda4"]},"id":"jbH6ZJLcNs0s","outputId":"6da857be-2223-4969-e637-b5cb46234a41","executionInfo":{"status":"ok","timestamp":1756900936441,"user_tz":-330,"elapsed":133104,"user":{"displayName":"Prashant Sahu","userId":"17861245123904241513"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Warning: No languages specified, defaulting to English.\n"]},{"output_type":"display_data","data":{"text/plain":["yolox_l0.05.onnx:   0%|          | 0.00/217M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69edc49bec3b474ebfdc3c6329f1589d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["preprocessor_config.json:   0%|          | 0.00/274 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b041978e0fd4452c9b2d28d542f2466e"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n"]},{"output_type":"display_data","data":{"text/plain":["config.json: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19c392acd46a42bc981b3e1682ecb8a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/115M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fdc0a8d0da7b4dfaacc52fac46debc1e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/46.8M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60bd2db3afff41fabd142b679177e57a"}},"metadata":{}}],"source":["# takes 3-4 mins on Colab\n","loader = UnstructuredPDFLoader('./layoutparser_paper.pdf',\n","                               strategy='hi_res',\n","                               extract_images_in_pdf=False,\n","                               infer_table_structure=True,\n","                               chunking_strategy=\"by_title\",\n","                               max_characters=4000, # max size of chunks\n","                               new_after_n_chars=3800, # preferred size of chunks\n","                               combine_text_under_n_chars=2000, # smaller chunks < 2000 chars will be combined into a larger chunk\n","                               mode='elements')\n","data = loader.load()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0WvtzXCgMots","colab":{"base_uri":"https://localhost:8080/"},"outputId":"6c2ef081-0e2a-4e1e-b99a-fea112467b17","executionInfo":{"status":"ok","timestamp":1756900936449,"user_tz":-330,"elapsed":5,"user":{"displayName":"Prashant Sahu","userId":"17861245123904241513"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["16"]},"metadata":{},"execution_count":55}],"source":["len(data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JM5fCpJNPPiv","colab":{"base_uri":"https://localhost:8080/"},"outputId":"df45653c-b68e-4e79-befd-e6581bdd7d26","executionInfo":{"status":"ok","timestamp":1756900947522,"user_tz":-330,"elapsed":11,"user":{"displayName":"Prashant Sahu","userId":"17861245123904241513"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['CompositeElement',\n"," 'CompositeElement',\n"," 'CompositeElement',\n"," 'CompositeElement',\n"," 'CompositeElement',\n"," 'CompositeElement',\n"," 'CompositeElement',\n"," 'CompositeElement',\n"," 'CompositeElement',\n"," 'CompositeElement',\n"," 'CompositeElement',\n"," 'CompositeElement',\n"," 'CompositeElement',\n"," 'CompositeElement',\n"," 'CompositeElement',\n"," 'CompositeElement']"]},"metadata":{},"execution_count":57}],"source":["[doc.metadata['category'] for doc in data]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9f7zcYUxMq0t","colab":{"base_uri":"https://localhost:8080/"},"outputId":"20471a17-d559-4b02-ddff-7575ce26492d","executionInfo":{"status":"ok","timestamp":1756901309028,"user_tz":-330,"elapsed":54,"user":{"displayName":"Prashant Sahu","userId":"17861245123904241513"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Document(metadata={'source': './layoutparser_paper.pdf', 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2023-01-23T09:15:33', 'page_number': 4, 'text_as_html': \"<table><thead><tr><th>Dataset</th><th>| Base Model'|</th><th>| Notes</th></tr></thead><tbody><tr><td>PubLayNet B8]|</td><td>F/M</td><td>Layouts of modern scientific documents</td></tr><tr><td>PRImA</td><td>M</td><td>Layouts of scanned modern magazines and scientific report</td></tr><tr><td>Newspaper</td><td>F</td><td>Layouts of scanned US newspapers from the 20th century</td></tr><tr><td>TableBank</td><td>F</td><td>Table region on modern scientific and business document</td></tr><tr><td>HJDataset</td><td>F/M</td><td>Layouts of history Japanese documents</td></tr></tbody></table>\", 'orig_elements': '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', 'file_directory': '.', 'filename': 'layoutparser_paper.pdf', 'category': 'CompositeElement', 'element_id': '871de4ec5ecffd0e1fe5924355785a2b'}, page_content='There have been a variety of document data collections to facilitate the development of DL models. Some examples include PRImA [3](magazine layouts), PubLayNet [38](academic paper layouts), Table Bank [18](tables in academic papers), Newspaper Navigator Dataset [16, 17](newspaper ﬁgure layouts) and HJDataset [31](historical Japanese document layouts). A spectrum of models trained on these datasets are currently available in the LayoutParser model zoo to support diﬀerent use cases.\\n\\n3 The Core LayoutParser Library\\n\\nAt the core of LayoutParser is an oﬀ-the-shelf toolkit that streamlines DL- based document image analysis. Five components support a simple interface with comprehensive functionalities: 1) The layout detection models enable using pre-trained or self-trained DL models for layout detection with just four lines of code. 2) The detected layout information is stored in carefully engineered\\n\\nLayoutParser: A Uniﬁed Toolkit for DL-Based DIA\\n\\nTable 1: Current layout detection models in the LayoutParser model zoo\\n\\nDataset Base Model1 Large Model Notes PubLayNet [38] F / M M Layouts of modern scientiﬁc documents PRImA [3] M - Layouts of scanned modern magazines and scientiﬁc reports Newspaper [17] F - Layouts of scanned US newspapers from the 20th century TableBank [18] F F Table region on modern scientiﬁc and business document HJDataset [31] F / M - Layouts of history Japanese documents\\n\\n1 For each dataset, we train several models of diﬀerent sizes for diﬀerent needs (the trade-oﬀ between accuracy vs. computational cost). For “base model” and “large model”, we refer to using the ResNet 50 or ResNet 101 backbones [13], respectively. One can train models of diﬀerent architectures, like Faster R-CNN [28] (F) and Mask R-CNN [12] (M). For example, an F in the Large Model column indicates it has a Faster R-CNN model trained using the ResNet 101 backbone. The platform is maintained and a number of additions will be made to the model zoo in coming months.\\n\\nlayout data structures, which are optimized for eﬃciency and versatility. 3) When necessary, users can employ existing or customized OCR models via the uniﬁed API provided in the OCR module. 4) LayoutParser comes with a set of utility functions for the visualization and storage of the layout data. 5) LayoutParser is also highly customizable, via its integration with functions for layout data annotation and model training. We now provide detailed descriptions for each component.')"]},"metadata":{},"execution_count":73}],"source":["data[4]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XdjS77Fh0oKJ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f25c0b55-1a42-402c-9e44-8481cb57ccf3","executionInfo":{"status":"ok","timestamp":1756901297551,"user_tz":-330,"elapsed":22,"user":{"displayName":"Prashant Sahu","userId":"17861245123904241513"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["There have been a variety of document data collections to facilitate the development of DL models. Some examples include PRImA [3](magazine layouts), PubLayNet [38](academic paper layouts), Table Bank [18](tables in academic papers), Newspaper Navigator Dataset [16, 17](newspaper ﬁgure layouts) and HJDataset [31](historical Japanese document layouts). A spectrum of models trained on these datasets are currently available in the LayoutParser model zoo to support diﬀerent use cases.\n","\n","3 The Core LayoutParser Library\n","\n","At the core of LayoutParser is an oﬀ-the-shelf toolkit that streamlines DL- based document image analysis. Five components support a simple interface with comprehensive functionalities: 1) The layout detection models enable using pre-trained or self-trained DL models for layout detection with just four lines of code. 2) The detected layout information is stored in carefully engineered\n","\n","LayoutParser: A Uniﬁed Toolkit for DL-Based DIA\n","\n","Table 1: Current layout detection models in the LayoutParser model zoo\n","\n","Dataset Base Model1 Large Model Notes PubLayNet [38] F / M M Layouts of modern scientiﬁc documents PRImA [3] M - Layouts of scanned modern magazines and scientiﬁc reports Newspaper [17] F - Layouts of scanned US newspapers from the 20th century TableBank [18] F F Table region on modern scientiﬁc and business document HJDataset [31] F / M - Layouts of history Japanese documents\n","\n","1 For each dataset, we train several models of diﬀerent sizes for diﬀerent needs (the trade-oﬀ between accuracy vs. computational cost). For “base model” and “large model”, we refer to using the ResNet 50 or ResNet 101 backbones [13], respectively. One can train models of diﬀerent architectures, like Faster R-CNN [28] (F) and Mask R-CNN [12] (M). For example, an F in the Large Model column indicates it has a Faster R-CNN model trained using the ResNet 101 backbone. The platform is maintained and a number of additions will be made to the model zoo in coming months.\n","\n","layout data structures, which are optimized for eﬃciency and versatility. 3) When necessary, users can employ existing or customized OCR models via the uniﬁed API provided in the OCR module. 4) LayoutParser comes with a set of utility functions for the visualization and storage of the layout data. 5) LayoutParser is also highly customizable, via its integration with functions for layout data annotation and model training. We now provide detailed descriptions for each component.\n"]}],"source":["print(data[4].page_content)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"six4KwFGPX-S","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b2158ce3-45b0-4180-b079-849849539ca2","executionInfo":{"status":"ok","timestamp":1756901291175,"user_tz":-330,"elapsed":48,"user":{"displayName":"Prashant Sahu","userId":"17861245123904241513"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Document(metadata={'source': './layoutparser_paper.pdf', 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2023-01-23T09:15:33', 'page_number': 4, 'text_as_html': \"<table><thead><tr><th>Dataset</th><th>| Base Model'|</th><th>| Notes</th></tr></thead><tbody><tr><td>PubLayNet B8]|</td><td>F/M</td><td>Layouts of modern scientific documents</td></tr><tr><td>PRImA</td><td>M</td><td>Layouts of scanned modern magazines and scientific report</td></tr><tr><td>Newspaper</td><td>F</td><td>Layouts of scanned US newspapers from the 20th century</td></tr><tr><td>TableBank</td><td>F</td><td>Table region on modern scientific and business document</td></tr><tr><td>HJDataset</td><td>F/M</td><td>Layouts of history Japanese documents</td></tr></tbody></table>\", 'orig_elements': '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', 'file_directory': '.', 'filename': 'layoutparser_paper.pdf', 'category': 'CompositeElement', 'element_id': '871de4ec5ecffd0e1fe5924355785a2b'}, page_content='There have been a variety of document data collections to facilitate the development of DL models. Some examples include PRImA [3](magazine layouts), PubLayNet [38](academic paper layouts), Table Bank [18](tables in academic papers), Newspaper Navigator Dataset [16, 17](newspaper ﬁgure layouts) and HJDataset [31](historical Japanese document layouts). A spectrum of models trained on these datasets are currently available in the LayoutParser model zoo to support diﬀerent use cases.\\n\\n3 The Core LayoutParser Library\\n\\nAt the core of LayoutParser is an oﬀ-the-shelf toolkit that streamlines DL- based document image analysis. Five components support a simple interface with comprehensive functionalities: 1) The layout detection models enable using pre-trained or self-trained DL models for layout detection with just four lines of code. 2) The detected layout information is stored in carefully engineered\\n\\nLayoutParser: A Uniﬁed Toolkit for DL-Based DIA\\n\\nTable 1: Current layout detection models in the LayoutParser model zoo\\n\\nDataset Base Model1 Large Model Notes PubLayNet [38] F / M M Layouts of modern scientiﬁc documents PRImA [3] M - Layouts of scanned modern magazines and scientiﬁc reports Newspaper [17] F - Layouts of scanned US newspapers from the 20th century TableBank [18] F F Table region on modern scientiﬁc and business document HJDataset [31] F / M - Layouts of history Japanese documents\\n\\n1 For each dataset, we train several models of diﬀerent sizes for diﬀerent needs (the trade-oﬀ between accuracy vs. computational cost). For “base model” and “large model”, we refer to using the ResNet 50 or ResNet 101 backbones [13], respectively. One can train models of diﬀerent architectures, like Faster R-CNN [28] (F) and Mask R-CNN [12] (M). For example, an F in the Large Model column indicates it has a Faster R-CNN model trained using the ResNet 101 backbone. The platform is maintained and a number of additions will be made to the model zoo in coming months.\\n\\nlayout data structures, which are optimized for eﬃciency and versatility. 3) When necessary, users can employ existing or customized OCR models via the uniﬁed API provided in the OCR module. 4) LayoutParser comes with a set of utility functions for the visualization and storage of the layout data. 5) LayoutParser is also highly customizable, via its integration with functions for layout data annotation and model training. We now provide detailed descriptions for each component.')"]},"metadata":{},"execution_count":71}],"source":["data[4]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AYe9e5K3Pfje","colab":{"base_uri":"https://localhost:8080/","height":140},"outputId":"9860adff-8f32-4be8-b6d8-83c1d3803f4c","executionInfo":{"status":"ok","timestamp":1756901282923,"user_tz":-330,"elapsed":60,"user":{"displayName":"Prashant Sahu","userId":"17861245123904241513"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'There have been a variety of document data collections to facilitate the development of DL models. Some examples include PRImA [3](magazine layouts), PubLayNet [38](academic paper layouts), Table Bank [18](tables in academic papers), Newspaper Navigator Dataset [16, 17](newspaper ﬁgure layouts) and HJDataset [31](historical Japanese document layouts). A spectrum of models trained on these datasets are currently available in the LayoutParser model zoo to support diﬀerent use cases.\\n\\n3 The Core LayoutParser Library\\n\\nAt the core of LayoutParser is an oﬀ-the-shelf toolkit that streamlines DL- based document image analysis. Five components support a simple interface with comprehensive functionalities: 1) The layout detection models enable using pre-trained or self-trained DL models for layout detection with just four lines of code. 2) The detected layout information is stored in carefully engineered\\n\\nLayoutParser: A Uniﬁed Toolkit for DL-Based DIA\\n\\nTable 1: Current layout detection models in the LayoutParser model zoo\\n\\nDataset Base Model1 Large Model Notes PubLayNet [38] F / M M Layouts of modern scientiﬁc documents PRImA [3] M - Layouts of scanned modern magazines and scientiﬁc reports Newspaper [17] F - Layouts of scanned US newspapers from the 20th century TableBank [18] F F Table region on modern scientiﬁc and business document HJDataset [31] F / M - Layouts of history Japanese documents\\n\\n1 For each dataset, we train several models of diﬀerent sizes for diﬀerent needs (the trade-oﬀ between accuracy vs. computational cost). For “base model” and “large model”, we refer to using the ResNet 50 or ResNet 101 backbones [13], respectively. One can train models of diﬀerent architectures, like Faster R-CNN [28] (F) and Mask R-CNN [12] (M). For example, an F in the Large Model column indicates it has a Faster R-CNN model trained using the ResNet 101 backbone. The platform is maintained and a number of additions will be made to the model zoo in coming months.\\n\\nlayout data structures, which are optimized for eﬃciency and versatility. 3) When necessary, users can employ existing or customized OCR models via the uniﬁed API provided in the OCR module. 4) LayoutParser comes with a set of utility functions for the visualization and storage of the layout data. 5) LayoutParser is also highly customizable, via its integration with functions for layout data annotation and model training. We now provide detailed descriptions for each component.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":70}],"source":["data[4].page_content"]},{"cell_type":"code","source":[],"metadata":{"id":"ePjtBzFKdXE3"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nFI5BBDjPoHK","colab":{"base_uri":"https://localhost:8080/","height":146},"outputId":"a7ab5b32-f925-4a2e-8b21-c1ea8f1708f6","executionInfo":{"status":"ok","timestamp":1756901275992,"user_tz":-330,"elapsed":34,"user":{"displayName":"Prashant Sahu","userId":"17861245123904241513"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<table><thead><tr><th>Dataset</th><th>| Base Model'|</th><th>| Notes</th></tr></thead><tbody><tr><td>PubLayNet B8]|</td><td>F/M</td><td>Layouts of modern scientific documents</td></tr><tr><td>PRImA</td><td>M</td><td>Layouts of scanned modern magazines and scientific report</td></tr><tr><td>Newspaper</td><td>F</td><td>Layouts of scanned US newspapers from the 20th century</td></tr><tr><td>TableBank</td><td>F</td><td>Table region on modern scientific and business document</td></tr><tr><td>HJDataset</td><td>F/M</td><td>Layouts of history Japanese documents</td></tr></tbody></table>"]},"metadata":{},"execution_count":69}],"source":["from IPython.display import HTML\n","\n","HTML(data[4].metadata['text_as_html'])"]},{"cell_type":"markdown","metadata":{"id":"JbzDKmPzSA1H"},"source":["Load using raw unstructured.io APIs for PDFs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_-wpOxHUMwOw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1756901082111,"user_tz":-330,"elapsed":117208,"user":{"displayName":"Prashant Sahu","userId":"17861245123904241513"}},"outputId":"98101b84-8357-4bec-c66e-9b48a1615fc9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Warning: No languages specified, defaulting to English.\n"]}],"source":["from unstructured.partition.pdf import partition_pdf\n","\n","# Get elements - takes 3-4 mins\n","raw_pdf_elements = partition_pdf(\n","    filename=\"./layoutparser_paper.pdf\",\n","    strategy='hi_res',\n","    # Unstructured first finds embedded image blocks\n","    extract_images_in_pdf=False,\n","    # Use layout model (YOLOX) to get bounding boxes (for tables) and find titles\n","    # Titles are any sub-section of the document\n","    infer_table_structure=True,\n","    # Post processing to aggregate text once we have the title\n","    chunking_strategy=\"by_title\",\n","    # Chunking params to aggregate text blocks\n","    # Attempt to create a new chunk 3800 chars\n","    # Attempt to keep chunks > 2000 chars\n","    max_characters=4000,\n","    new_after_n_chars=3800,\n","    combine_text_under_n_chars=2000,\n","    image_output_dir_path=\"./\",\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u7QC4wt-St_K","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7a7836d1-a2a5-4506-979f-bea6717863a5","executionInfo":{"status":"ok","timestamp":1756901225827,"user_tz":-330,"elapsed":47,"user":{"displayName":"Prashant Sahu","userId":"17861245123904241513"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["16"]},"metadata":{},"execution_count":64}],"source":["len(raw_pdf_elements)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bC6seR7TSZY4","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c1543ff0-efaf-45db-9b75-df6337147d54","executionInfo":{"status":"ok","timestamp":1756901227178,"user_tz":-330,"elapsed":23,"user":{"displayName":"Prashant Sahu","userId":"17861245123904241513"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[<unstructured.documents.elements.CompositeElement at 0x7a893c7e6b10>,\n"," <unstructured.documents.elements.CompositeElement at 0x7a893ca82d50>,\n"," <unstructured.documents.elements.CompositeElement at 0x7a893c646a80>,\n"," <unstructured.documents.elements.CompositeElement at 0x7a893c647500>,\n"," <unstructured.documents.elements.CompositeElement at 0x7a893cbb5820>,\n"," <unstructured.documents.elements.CompositeElement at 0x7a893cbb6540>,\n"," <unstructured.documents.elements.CompositeElement at 0x7a893cbb7aa0>,\n"," <unstructured.documents.elements.CompositeElement at 0x7a893cbb4f80>,\n"," <unstructured.documents.elements.CompositeElement at 0x7a893cbb5940>,\n"," <unstructured.documents.elements.CompositeElement at 0x7a893cbb65d0>,\n"," <unstructured.documents.elements.CompositeElement at 0x7a8884e91a90>,\n"," <unstructured.documents.elements.CompositeElement at 0x7a893cb0d040>,\n"," <unstructured.documents.elements.CompositeElement at 0x7a893cb0f020>,\n"," <unstructured.documents.elements.CompositeElement at 0x7a893cb0ea50>,\n"," <unstructured.documents.elements.CompositeElement at 0x7a893cb0c530>,\n"," <unstructured.documents.elements.CompositeElement at 0x7a893cbd57f0>]"]},"metadata":{},"execution_count":65}],"source":["raw_pdf_elements"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8iPAwY5YS0CC","colab":{"base_uri":"https://localhost:8080/"},"outputId":"11a3f5a1-1a1c-4411-e5aa-ba5cf29d482e","executionInfo":{"status":"ok","timestamp":1756901342752,"user_tz":-330,"elapsed":80,"user":{"displayName":"Prashant Sahu","userId":"17861245123904241513"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'type': 'CompositeElement',\n"," 'element_id': '871de4ec5ecffd0e1fe5924355785a2b',\n"," 'text': 'There have been a variety of document data collections to facilitate the development of DL models. Some examples include PRImA [3](magazine layouts), PubLayNet [38](academic paper layouts), Table Bank [18](tables in academic papers), Newspaper Navigator Dataset [16, 17](newspaper ﬁgure layouts) and HJDataset [31](historical Japanese document layouts). A spectrum of models trained on these datasets are currently available in the LayoutParser model zoo to support diﬀerent use cases.\\n\\n3 The Core LayoutParser Library\\n\\nAt the core of LayoutParser is an oﬀ-the-shelf toolkit that streamlines DL- based document image analysis. Five components support a simple interface with comprehensive functionalities: 1) The layout detection models enable using pre-trained or self-trained DL models for layout detection with just four lines of code. 2) The detected layout information is stored in carefully engineered\\n\\nLayoutParser: A Uniﬁed Toolkit for DL-Based DIA\\n\\nTable 1: Current layout detection models in the LayoutParser model zoo\\n\\nDataset Base Model1 Large Model Notes PubLayNet [38] F / M M Layouts of modern scientiﬁc documents PRImA [3] M - Layouts of scanned modern magazines and scientiﬁc reports Newspaper [17] F - Layouts of scanned US newspapers from the 20th century TableBank [18] F F Table region on modern scientiﬁc and business document HJDataset [31] F / M - Layouts of history Japanese documents\\n\\n1 For each dataset, we train several models of diﬀerent sizes for diﬀerent needs (the trade-oﬀ between accuracy vs. computational cost). For “base model” and “large model”, we refer to using the ResNet 50 or ResNet 101 backbones [13], respectively. One can train models of diﬀerent architectures, like Faster R-CNN [28] (F) and Mask R-CNN [12] (M). For example, an F in the Large Model column indicates it has a Faster R-CNN model trained using the ResNet 101 backbone. The platform is maintained and a number of additions will be made to the model zoo in coming months.\\n\\nlayout data structures, which are optimized for eﬃciency and versatility. 3) When necessary, users can employ existing or customized OCR models via the uniﬁed API provided in the OCR module. 4) LayoutParser comes with a set of utility functions for the visualization and storage of the layout data. 5) LayoutParser is also highly customizable, via its integration with functions for layout data annotation and model training. We now provide detailed descriptions for each component.',\n"," 'metadata': {'filetype': 'application/pdf',\n","  'languages': ['eng'],\n","  'last_modified': '2023-01-23T09:15:33',\n","  'page_number': 4,\n","  'text_as_html': \"<table><thead><tr><th>Dataset</th><th>| Base Model'|</th><th>| Notes</th></tr></thead><tbody><tr><td>PubLayNet B8]|</td><td>F/M</td><td>Layouts of modern scientific documents</td></tr><tr><td>PRImA</td><td>M</td><td>Layouts of scanned modern magazines and scientific report</td></tr><tr><td>Newspaper</td><td>F</td><td>Layouts of scanned US newspapers from the 20th century</td></tr><tr><td>TableBank</td><td>F</td><td>Table region on modern scientific and business document</td></tr><tr><td>HJDataset</td><td>F/M</td><td>Layouts of history Japanese documents</td></tr></tbody></table>\",\n","  'orig_elements': 'eJy9WVtv27gS/iuEXzYBIoW6S0VRoBcE20WSDdos9iEbGBRJ2TzRDSLl1O3ufz9D0rLlWE3bAAn6UJPiDIdz/yY332a85BWv1Vyw2Ss0SyhjURR6Tsh46oSExE6aE+ZEBOMc+wXFXjg7QbOKK8KIIkDzbUabpmOiJopLsy7JuunVfMnFYqlgx/cxBprN9r1gagm7XmJ220bUStPd3ARJ6AYnyAv81I1uT9B2I4oiN9QbXhCkbjq5Y4lgaybXUvFKv+ZKfOHl55ZQPvsPPjCuOFWiqee0JFLO267J4Rh2szDwQzhQiJKrdcsN7dXFzAhdL3qyMC+7mfF6Mbs1u1LNq4aJQnCjNx/7gYM9xw+ucfbKi14FgaEW9Z2h/DaTinSg5ZrxL/rxPsg8U/yL1s/MnO27Uv+mQnGX1KqpCW3api8bCerLOk5KIZWg+iUH3GJvzC19yO7rsqkXPvayts/BCjVXU1zgDSMu3gGXUmgWiuQlz0l9N8kiDsYs4ocsan4vW9Lybl6TlVgQ1XRzyUlHl3PStqWgRJtnmnMy5pz8DGftoHL6rYG3pzHvITu55DXoA5ekW4DzaJu34AXzuq9y3sGx8L8R/fWSdxwtyYqjnPMaEbQinQBXQk2BWEN7HWFIi4NoU5bWCSVSDSoIFaVQEDpILTlifMXLpjXHgfTDOQIn46V00eem4oh/IVVbcolETcuecXT16WP1FuLk9qgiC/JV1BzZKJPHJ+iqz8/J+pIrOJDeHhFKGK8ERUZNo3PX2qLoHZgUwgkOGgvrO9A+iT57OagZXQ5qRh+smoE4hjhMbo+2tkD/9EWOvUXf7eRCpGbo9z+2RIF3e7QEz246sH6J/iAtqbnkO7UNhC56i2QLuuv6SivHagapjsCzGWpqrUFNaDlLROBW2ncdMCnXiKyIKM1LhTmJzg3fK9JJENQwQ1+bRhtF9m3bdGAwYeTHXLNAPfCmwFm62lmGNHFJug6cdsWvtS8YT9vPqJzwrEgxdmiaESeMCHdITH0nKLKAZNTn2I+eMaN6JoHGXuDGNqNuNsLETfVGBukzm9wwJE/Lp2mchNFz5NNHYjBAEIXofdM9sOy5yDvSrcc2uxaq5FO2igNS+IwnTpDE2Alxkjp54UeOFyVF7rMoxDx/Rlv5rg6gOI3dwNrKbqRhbE3jBSF2vckdS/TE6hcEWfzC1nqrTAxSbS6I5T2LCQjdGjU29hw45kA2LguIzKa8E5qQKCQVVMQK6itkqg/nDsohMtkuaQjIhxzYkHItBaTPM4hQuK1qmxo+y22IEySFTqmQFBTvIBtzdC/U0hztOBQBqQmLvjaagxKsBJdgvWPjbdamaKvZISfx2iSaXop6gYCPs01SHZLwlO16m+BR0XSH7Iwo/+ulgs89fDevBXVRoHGRb4Wwx4HZhlzUwKsyhVSrUmdW+AhJj0JCLPoSciEYElhBWmO/lMoS7DOSpamTpUXghH5awK+YO0EWxsyLWV7k5NnCI44SF7Z8+M80fsM6DVy8aQRjN5raMBRPC40oyYLk+UMjGofGOBZeQc37q7ZlyAMrXm9CQHsLeP074/QfPr4dm/EciulH/c4JC3oMFMtw4kS0iJwwY7mTpxlUJEyDJMcBw8R7NguGOHRBmUGErQWHdZIOyQyb4nS4YSieZsE4Ck2r/ZIWtA2V9wq9t93Hd/PEo53I2KZnQvdQ70m7a48fVC6esIhAQGKKQzBsjp3MS2Mn5ClOCU0y8qxdRqYDLRgib7OOEmybDsBmnhtObFiKJ1Yt7Kfey2I2PLLxDQiPbp8ItLx9Rod8fhH/BdEeQy855PiL+Cjaf6s38dafgIPxvlyBd8jlh0BrYDEncr5UlaF8bW598xqChzD4r9M/32wgxetT+K3X/yKdH9GFDqjf/h1tXzbg8XZ9qmlPBz55w9YbduzNDj69S281OTPbZ6cX2982cOWARroaSSogKMHF6LYXkfa4uWlgrWHblsskP0lJrTuEDd8B3kkDn0a3dFz3MYdXbIHaTvDHrvnrM9p6CDQjXVOZzORj3QvBXX23PrzD5DmNGyfusDmw4wud7jYZb18/+iG5bpG4lFtlHV6yhYo/soAFketDBDk2wOnGxqfWg2bjtD1A0p3XeJCawS3twrrNA1SNztApuoB/33UFW7tH7rAD7UDmoF+0+cDO2l2OAPkNQG8QZ5LjT5gXba1ppgDA6Qw9bsRBlElDPoD4Gz3tCfd9g+2BNS3DVMkrvCKhISsclnoA1qIIulGa5A6OEkgoXhSH2fN1o0GcaZQcJdDCmJJn14mHXX8LzJKJDUvxxF4Gh77/oiXP97P9OnAwqVxyqAIx47ydHrPtVwB/oo50ugB4UQGyQsKfYhI8EMI/ZGLESCoi7yZryDjMPXQG3TMndDmMik7QPbdzJEBnK96RcmjQ9PhubwgkxVduwdr+PqApJtGRDipgxLizga8o5+rejAQp7TtC12gFaFSjy14RCylhJdWxa6T6p4dnUI1lrQRmzUyE2U+mTI6/GdkB10ECUM0GcWopPnGpc1SENebcLDzsAU6md3mjkwq4JGiq42amBqivXLvoz1rPuOqNMr6nBD2vFdpHoSeVJwBL7zg6M+ZDn5z3l5foxocEcnRmZ30XYJRh3/Nh/2Lz2M0080Sj/bNdL7xLuLQp+wpAbM30YFiPJBVaEkiH+7fZjnmA1AcqGL/aNYi5LYnSGFnD4wqolKXUwhJknUY/mjAm7KD2XpQlWBIOM67VrLnvJoYaWjeVvrVqarXcnw/+sHOPsiBikLGcnBEA1SlmTh4COItinFKCC8Jx8awzJ8goSRK53jBygrWHsb8F0ZkZGB7sWJqn/73lBSZOe2E/YDA9hJeq6wfvvV8KyAR6VtyAjSqIb2bimxuPD3Spg6jVvgGZQULMlkJBpATH6G9oXSHwKZQ90q1P9IAYiqsOHw5+3azBwXXHDn4B7GgPxc5y//P9pyG0VoIYZ+pHGP/t1UcEmloJZsc2+vuGpC/Bg8PjfcAIvselHRXBy7j5w0FvxdwOrmzK0pxWQvaAJL7a+ZBpLKAI62EZkKnRTAv05KLo+HA0V8oGSvdiWa63j9Il+sQ8RihpRmmLjuwmWPtSjA0B3UmjdqKMIhnU5qK/Oaqb+0EbGj8T8BgGPyTtRLvjabL5drz32ID+9v9T9zqH',\n","  'file_directory': '.',\n","  'filename': 'layoutparser_paper.pdf'}}"]},"metadata":{},"execution_count":74}],"source":["raw_pdf_elements[4].to_dict()"]},{"cell_type":"markdown","metadata":{"id":"QLKjlNBQTHwE"},"source":["Convert into LangChain `document`format"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iCFjyenoTMC7","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3f98dda8-8c2b-4d63-ccbf-de48a4e396c4","executionInfo":{"status":"ok","timestamp":1756901351975,"user_tz":-330,"elapsed":71,"user":{"displayName":"Prashant Sahu","userId":"17861245123904241513"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Document(metadata={'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2023-01-23T09:15:33', 'page_number': 4, 'text_as_html': \"<table><thead><tr><th>Dataset</th><th>| Base Model'|</th><th>| Notes</th></tr></thead><tbody><tr><td>PubLayNet B8]|</td><td>F/M</td><td>Layouts of modern scientific documents</td></tr><tr><td>PRImA</td><td>M</td><td>Layouts of scanned modern magazines and scientific report</td></tr><tr><td>Newspaper</td><td>F</td><td>Layouts of scanned US newspapers from the 20th century</td></tr><tr><td>TableBank</td><td>F</td><td>Table region on modern scientific and business document</td></tr><tr><td>HJDataset</td><td>F/M</td><td>Layouts of history Japanese documents</td></tr></tbody></table>\", 'orig_elements': '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', 'file_directory': '.', 'filename': 'layoutparser_paper.pdf'}, page_content='There have been a variety of document data collections to facilitate the development of DL models. Some examples include PRImA [3](magazine layouts), PubLayNet [38](academic paper layouts), Table Bank [18](tables in academic papers), Newspaper Navigator Dataset [16, 17](newspaper ﬁgure layouts) and HJDataset [31](historical Japanese document layouts). A spectrum of models trained on these datasets are currently available in the LayoutParser model zoo to support diﬀerent use cases.\\n\\n3 The Core LayoutParser Library\\n\\nAt the core of LayoutParser is an oﬀ-the-shelf toolkit that streamlines DL- based document image analysis. Five components support a simple interface with comprehensive functionalities: 1) The layout detection models enable using pre-trained or self-trained DL models for layout detection with just four lines of code. 2) The detected layout information is stored in carefully engineered\\n\\nLayoutParser: A Uniﬁed Toolkit for DL-Based DIA\\n\\nTable 1: Current layout detection models in the LayoutParser model zoo\\n\\nDataset Base Model1 Large Model Notes PubLayNet [38] F / M M Layouts of modern scientiﬁc documents PRImA [3] M - Layouts of scanned modern magazines and scientiﬁc reports Newspaper [17] F - Layouts of scanned US newspapers from the 20th century TableBank [18] F F Table region on modern scientiﬁc and business document HJDataset [31] F / M - Layouts of history Japanese documents\\n\\n1 For each dataset, we train several models of diﬀerent sizes for diﬀerent needs (the trade-oﬀ between accuracy vs. computational cost). For “base model” and “large model”, we refer to using the ResNet 50 or ResNet 101 backbones [13], respectively. One can train models of diﬀerent architectures, like Faster R-CNN [28] (F) and Mask R-CNN [12] (M). For example, an F in the Large Model column indicates it has a Faster R-CNN model trained using the ResNet 101 backbone. The platform is maintained and a number of additions will be made to the model zoo in coming months.\\n\\nlayout data structures, which are optimized for eﬃciency and versatility. 3) When necessary, users can employ existing or customized OCR models via the uniﬁed API provided in the OCR module. 4) LayoutParser comes with a set of utility functions for the visualization and storage of the layout data. 5) LayoutParser is also highly customizable, via its integration with functions for layout data annotation and model training. We now provide detailed descriptions for each component.')"]},"metadata":{},"execution_count":75}],"source":["from langchain_core.documents import Document\n","\n","lc_docs = [Document(page_content=doc.text,\n","                    metadata=doc.metadata.to_dict())\n","              for doc in raw_pdf_elements]\n","lc_docs[4]"]},{"cell_type":"markdown","metadata":{"id":"QQV0X4grW971"},"source":["### Microsoft Office Document Loaders\n","\n","The Microsoft Office suite of productivity software includes Microsoft Word, Microsoft Excel, Microsoft PowerPoint, Microsoft Outlook, and Microsoft OneNote. It is available for Microsoft Windows and macOS operating systems. It is also available on Android and iOS.\n","\n","[Unstructured.io](https://docs.unstructured.io/open-source/introduction/overview) provides a variety of document loaders to load MS Office documents. Check them out [here](https://docs.unstructured.io/open-source/core-functionality/partitioning).\n","\n","Here we will leverage LangChain's [`UnstructuredWordDocumentLoader`](https://api.python.langchain.com/en/latest/document_loaders/langchain_community.document_loaders.word_document.UnstructuredWordDocumentLoader.html) to load data from a MS Word document."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V37elBlCT6Mn","colab":{"base_uri":"https://localhost:8080/"},"outputId":"84ee8f89-8fa4-44f5-ebae-fbb170d5cff7","executionInfo":{"status":"ok","timestamp":1756901535069,"user_tz":-330,"elapsed":2813,"user":{"displayName":"Prashant Sahu","userId":"17861245123904241513"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading...\n","From: https://drive.google.com/uc?id=1DEz13a7k4yX9yFrWaz3QJqHdfecFYRV-\n","To: /content/Quantum Computing.docx\n","\r  0% 0.00/11.4k [00:00<?, ?B/s]\r100% 11.4k/11.4k [00:00<00:00, 42.8MB/s]\n"]}],"source":["!gdown 1DEz13a7k4yX9yFrWaz3QJqHdfecFYRV-"]},{"cell_type":"markdown","metadata":{"id":"uv2b1SZcX4zS"},"source":["Load word doc as a single document"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BqYBSoqHT6TY"},"outputs":[],"source":["from langchain_community.document_loaders import UnstructuredWordDocumentLoader\n","\n","loader = UnstructuredWordDocumentLoader('./Quantum Computing.docx')\n","data = loader.load()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zfi8_V9MT6Vl","colab":{"base_uri":"https://localhost:8080/"},"outputId":"27730ac5-e571-4341-e8b6-e34215a9508c","executionInfo":{"status":"ok","timestamp":1756901574057,"user_tz":-330,"elapsed":9,"user":{"displayName":"Prashant Sahu","userId":"17861245123904241513"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["1"]},"metadata":{},"execution_count":79}],"source":["len(data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kznFjlj6T6Xw","colab":{"base_uri":"https://localhost:8080/","height":140},"outputId":"799bd9db-795c-4c37-a99b-cb35684ae871","executionInfo":{"status":"ok","timestamp":1756901575024,"user_tz":-330,"elapsed":30,"user":{"displayName":"Prashant Sahu","userId":"17861245123904241513"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'The Rise of Quantum Computing: A New Era of Innovation\\n\\nFor decades, classical computing has driven technological advancements, but the limitations of traditional binary processing are becoming evident as the world demands more computational power. Enter quantum computing—a revolutionary approach that leverages the principles of quantum mechanics to solve complex problems at unprecedented speeds.\\n\\nUnderstanding Quantum Computing\\n\\nUnlike classical computers that process information using bits (0s and 1s), quantum computers use qubits, which can exist in multiple states simultaneously due to superposition. This unique property allows quantum systems to process vast amounts of data in parallel, making them exponentially more powerful for specific tasks.\\n\\nAnother key principle, entanglement, enables qubits to be interconnected, meaning the state of one qubit is dependent on another, regardless of distance. This drastically enhances processing efficiency and speed, paving the way for breakt'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":80}],"source":["data[0].page_content[:1000]"]},{"cell_type":"markdown","metadata":{"id":"ILz-pXIyX8e0"},"source":["Load word doc with complex parsing and section based chunks"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7cKYWNjAUo1A"},"outputs":[],"source":["loader = UnstructuredWordDocumentLoader('./Quantum Computing.docx',\n","                                        strategy='fast',\n","                                        chunking_strategy=\"by_title\",\n","                                        max_characters=3000, # max limit of a document chunk\n","                                        new_after_n_chars=2500, # preferred document chunk size\n","                                        mode='elements')\n","data = loader.load()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CkOn6t0QVOyE","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ec7e5e98-e038-46eb-82f6-d6589941d3ee","executionInfo":{"status":"ok","timestamp":1756901589736,"user_tz":-330,"elapsed":9,"user":{"displayName":"Prashant Sahu","userId":"17861245123904241513"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{},"execution_count":82}],"source":["len(data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IV4cp6yqVQZ1","colab":{"base_uri":"https://localhost:8080/"},"outputId":"aaa48e9c-edc8-4f85-d449-4bdd8b8394a4","executionInfo":{"status":"ok","timestamp":1756901593403,"user_tz":-330,"elapsed":35,"user":{"displayName":"Prashant Sahu","userId":"17861245123904241513"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Document(metadata={'source': './Quantum Computing.docx', 'emphasized_text_contents': ['Understanding Quantum Computing', 'qubits', 'superposition', 'entanglement', 'Applications Transforming Industries', 'Drug Discovery & Healthcare', 'Financial Modeling', 'Cybersecurity & Cryptography', 'post-quantum cryptography', 'Climate Modeling & Sustainability', 'AI & Machine Learning'], 'emphasized_text_tags': ['b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b'], 'file_directory': '.', 'filename': 'Quantum Computing.docx', 'last_modified': '2025-02-19T14:45:48', 'orig_elements': 'eJzNWO9v4zYS/VeIfDj0ACuQ9cOy9lu6be8CdAvcXe5TtwgocmQTkUgtRTnrLe5/vzeU7XibdJMFFnf5ZovkcDjz5s0jf/39gjrqyYZboy/eiItl3eic1jKpdFkkhczaZC2VTpRalaqqWtJZfrEQFz0FqWWQWPP7hZKBNs7vbzUNYYtPKWa0pqNbbTypgCG2fXlx+GxlT/zhH5O0YerFW9cPUzB2c6md+sizOjmG295p0xqKfmVpViZplizrm2XxpijfFOuL/2BioI+Bx2+2JP5pRhKuFY/MvhFX4he6Fz96yePX1rqdDMZZ3irsh+jMjQkdsc0/hmRVNlWWNtg+V2lSZJKSpizLhGqZSt1U1GTtawzJT84LTUpqGhdCYflolOyEOloWWzkK7c2OrAikttZ1bhOnSL2TVsUYYGkzBREQ3s70JsSwjRzF4KU2/A8LGmOl34vBO0XYBralJ9EQ9uI/tDMatgT2Y0P3zncarvXS6lH0DlNnp+TB3ODuyV+KH20gLz4cInLy+/2UpctCCk87101xCfaWA3aXaosdZBAd7cjLDc0bDt5YZYaOouNHgz3OLK1RmOPE6Lrd7EZHH/kgDTAwCpia7ICEER+AtBgHIj1engPnF+k9PN/RDQf+CQBVVdqmql4ny2pdJUW9XiXr9TJNcIx1lpU1FXXzPICWmEH9gKSZT6RvOcu3ysEpJAnDv17822ryY0BQOeaPgHTx2xMGgtzMi5s4/L8F6HMOv6g4pazSGlRVFbRKinWZJc2yyZNiVaerVq9a2bywOL8Y2w9TY/AT08ZpID+4MUL/uZhi9P8T2M7c0aOaR7Dn6jjUqTC2db6PVSemWLZ8SvFdCuBbLZbjXxd/KD82MYFl53gsxP3WoOSUtII+mjHAouinLnCtCWQ2oOJGw1+kJTeN3V7oiWK9nYfxUtxsDQxb82HianUYCyjprnP348mDcT8GrkmsPh5gh8gI2bsJeeLS5gyzD4P0WEzdQvTyjs8FFujh4uAsUmowtp95JzJNO3UCgeDaVoixEkGOd19Z4+uskVXWyqSuU0qKUqNJZEWe5FnW1A01a03VN8Ahfki7mXd+fQV9ZR0C7cUd7R84dyHOneZ/smEmniHE2WwIOQO0cFILR0kjbQRmnvM244izi+TNqwTAgpCRjV0F4JXzxgv0hI30umNsMBwMs4uiA760x4G4HpB9slseGc97FrVIviGr9hH+keoXwNLu6Mm93EegNJ7kXdh6N222XEVC+f0Q3MbLYbtnzOEwABmwH80hBGxP+sDowkA8bteZDQ9+Hc5kUa7rlSoTvcrUoZeUuk7yuoVOy5ZNSy/QZ8/2kqth6BCpudnfeGlHZgqOw7XV0xi8ofEV4u8lXr+kqyzzfN2kWZVA8S6TQiPeskzTZL1cybxZUZ2l9Teo5h/8tBE/mFE5qJW9+Iv4O8kubBXE0+sL7hecfW+P+0SyPySAu4KEnNt/IpBtRwojINngJxUmT3OPiYUv1bwCrYnXBKhGFSUcCFwqhdxwUSCNmn3QJx/YAFrFyKKRowTa0Cgwi4LnA4NCsEtshdffv4uz/+YciCiqU9mhivWeu0LnPFs/NpqjlKTITq3ZbANvSnI8mlPMHT5avOo+bcn05KMsrT9vGz+DgK7RtJ7CWFHlZZlqCMK0apMC3QK9Y1km7UoX6zavc6q+Rcf4CdLcRtJ55zR1r1IIPvbxvX341kt/R4F5dncu0hezMkfCAzcUaAFwLneWy5OUlB1iZcK2n8HohoArzCfu+j60rjNuZLK2uCcIb8a7maYPNg/birZjuB4gfQ9jYgPc8MUEyJwA3T1rKvvkBQup2zr9ckjkTbOUZbNMshbXg6Imlch6Ddqp13WaqZaq/FvQztt9g6JBPSI2XMhvz7oXm4csC8lJ950Pvk65+6XzvLc3Z/dU9FseY717yA0UA9QA/5esEWU4pRAQg7mjxH0sg5WbcIuNUoAlAPZ3uM4epAZfrceBEaSjcvCHp4k/DS2wBxLRTIGaic8NDKB4KDrHMXgziprA1HQirBCkuvsKnNUFFXmTJrqqyqTI0d6aNG2TtKH1itJc5kp9C5x1hqXQqaiRmn+hC0tUdmM6BPf1MdGzLj+0uocyZ2453kh6ZgEmqYMhDt9IfFXaUjfnlDVhQHb46QO7nKbyawSYaFQ4mAc3fc42/AhhxofL0r3poCN77IvdPFm6jyyIH36zP1JdpK0FNy8EYjbC3BY/MteR3RnvLENDRmvoxAInwEn424sBtWzLLC+aFW7hWqOXpehlugS+iiwtNBVpXbzgheNZQF1dIx/v0JnR4sXPJL19le3sSTdPwEkOVw9Ih+sZAeNnCIp32Dbmi+8zym1s7FkocvL28F52vKqEbj9n8uExjCdrogECat4ZageuBJpV14K/xwcXEMj5Qx+TmHeNAwNikpygwuQDUqzEamCkA0YnbpgP96Y/Qclv/wWxxppn', 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'category': 'CompositeElement', 'element_id': '4c5a643b25b93f23992b2760281d9509'}, page_content='The Rise of Quantum Computing: A New Era of Innovation\\n\\nFor decades, classical computing has driven technological advancements, but the limitations of traditional binary processing are becoming evident as the world demands more computational power. Enter quantum computing—a revolutionary approach that leverages the principles of quantum mechanics to solve complex problems at unprecedented speeds.\\n\\nUnderstanding Quantum Computing\\n\\nUnlike classical computers that process information using bits (0s and 1s), quantum computers use qubits, which can exist in multiple states simultaneously due to superposition. This unique property allows quantum systems to process vast amounts of data in parallel, making them exponentially more powerful for specific tasks.\\n\\nAnother key principle, entanglement, enables qubits to be interconnected, meaning the state of one qubit is dependent on another, regardless of distance. This drastically enhances processing efficiency and speed, paving the way for breakthroughs in cryptography, materials science, and artificial intelligence.\\n\\nApplications Transforming Industries\\n\\nDrug Discovery & Healthcare Quantum simulations can analyze molecular structures and interactions at an atomic level, accelerating drug discovery and personalized medicine. Companies like IBM and Google are already exploring quantum approaches to fight diseases like cancer and Alzheimer’s.\\n\\nFinancial Modeling Financial markets involve complex, unpredictable variables. Quantum algorithms can optimize portfolios, manage risk, and predict market fluctuations with greater accuracy than classical computing methods.\\n\\nCybersecurity & Cryptography Traditional encryption methods rely on mathematical complexity, which quantum computers could break in seconds. This has sparked the rise of post-quantum cryptography, aimed at developing secure algorithms resistant to quantum attacks.\\n\\nClimate Modeling & Sustainability Quantum computing can process massive climate datasets, helping scientists model climate change scenarios with greater precision. This will improve renewable energy optimization, disaster prediction, and environmental impact assessment.\\n\\nAI & Machine Learning Quantum-enhanced AI models can process data faster, recognize patterns more efficiently, and revolutionize deep learning architectures, leading to advancements in robotics, automation, and natural language processing.')"]},"metadata":{},"execution_count":83}],"source":["data[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mW5UhN9BVTaE","colab":{"base_uri":"https://localhost:8080/"},"outputId":"55b340b2-5946-49d4-d3cd-a636e0ae700c","executionInfo":{"status":"ok","timestamp":1756901662926,"user_tz":-330,"elapsed":26,"user":{"displayName":"Prashant Sahu","userId":"17861245123904241513"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Document(metadata={'source': './Quantum Computing.docx', 'emphasized_text_contents': ['Challenges & The Road Ahead', 'Quantum Revolution'], 'emphasized_text_tags': ['b', 'b'], 'file_directory': '.', 'filename': 'Quantum Computing.docx', 'last_modified': '2025-02-19T14:45:48', 'orig_elements': 'eJzNVMtu2zAQ/JWFDj1ZgWzLtpxbkALtqUCD3NLCWJEriQhFKiTlxA36791V4rRogrS9FLkIEDn7mhnu1X1GlnpyaWd0dgrZmubzQpVFrjdK5SVWixwrvc0rXWO5qSq1nqtsBllPCTUm5Jj7TGGi1ofDTtOQOj6aM4L6ocNovpHeJbpLO+Vd4jqRr6+y8w6tJddShHdw2RFceNRw1hHq7OsLwQnbh8B6um6MpZ02gVTistL3SfZ47LAnOfg8oktjD+e+H8ZkXHuivboTlMWYdr3XpjE0zbwoFqu8WOTz7eW8PC1Xp2WVfWegVJb715oV2GGYKl6aZEkCf+d02zRFpfUybxpd5WWJmxyXzK5eLbfL7WazrFT9Z06L/z73e4qDSQQmRRiC702kGdw85lfH/NCgYmaiaR1nVnwL6omwGRin7KgFdzPWJkFMWBtr0mEGFIIPnCjIPMa7GaDTYDG0lEeFlqDDoG8xEGjak/WDkHoCH/0t/4YZ3JrUgfjKuNGPEepAeJ264Me2i1wZ4jhQYIAeVXpqgZtKAYeBNHDR+FA1+cFb33L/9vmIXIlA2uj9XtIo6yMFjgEUUM81DE98gL3Bmts+JmjGNAY6+dUknzAETGZPl0LyC2ZZqrUmVVBe4JLNUpSUV8ttlS/qVbNtqnJV0PotmuWDZ0mcDBIfCCXVQWtw+mfqjNtTnERg+a0Qz0fM4JGrQJEwqI5t4eIYBJg6TPwxUfRysfGhn6ibcjuR68AWsFZiOxykhh5jCkZ8F5XhXsSRoE1U0t3hUWp+wdq0JrHUlg/YalMs3OIhitIdco0DJRHY9Nga948alvV6s9roJl+rus7LhV7ntSL+INF2Ua1p1ei/0/DVJXpU74L23o7ygt7e7jybGOVHz8R7N3Ffs7jX4Jvp5/kMM8aRyM4OYOmVZVucTljHOaEJ8t759XGCnzuIgc6D9bxzAj9KVjzJItK8EPov46KYl7x6GFQTx0gAX8gSek3Xrz8AgDlkLQ==', 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'category': 'CompositeElement', 'element_id': 'cad0182fba34f6c6d546a11eb4444c39'}, page_content='Challenges & The Road Ahead\\n\\nDespite its promise, quantum computing faces significant challenges, including qubit stability, error correction, and large-scale hardware development. However, with continuous breakthroughs in superconducting qubits, trapped ions, and topological quantum computing, we are moving closer to a commercially viable quantum future.\\n\\nGovernments and tech giants are investing billions into quantum research, ensuring that this transformative technology will reshape industries, scientific discovery, and the digital landscape in ways we have yet to imagine.\\n\\nAs we stand on the brink of the Quantum Revolution, one thing is clear: the next frontier of computing is no longer a distant dream—it is becoming reality.')"]},"metadata":{},"execution_count":85}],"source":["data[1]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vVP9nzW_WwhO","colab":{"base_uri":"https://localhost:8080/"},"outputId":"25416826-7929-4031-c1a8-98d0b4a29494"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Document(metadata={'source': './Intel Strategy.docx', 'emphasized_text_contents': ['Intel Sustainability'], 'emphasized_text_tags': ['b'], 'file_directory': '.', 'filename': 'Intel Strategy.docx', 'languages': ['eng'], 'last_modified': '2024-05-21T22:51:21', 'orig_elements': 'eJzVVk1v3DYQ/SuDRT8ulqGvlVa+pU0QBDCKonZOcWBQ5EhLhCIVkvJ6G+S/91Fet0mRAu4pMHQRSM7M45vHR777tGHDE9t4q9XmgjZ5mVe8a/qsrmuV1ZJVJvJCZTtZd6Ks8rZVw+aMNhNHoUQUiPm0kSLy6PzxVvEc9xjKsYKneS+C/pPVbeT7eCudjagTMP1u8wb/hq6WEIW2otdGx+Pm/TeiohgfIvp1etCGb5X2LCPqJcDnm9OwFROngVPq6BOo47ly8j4tMcKOixj5IRvbcc1nRIi3k1N60Lzuv8zLOsu3WVlcl+XFtrgoi81nLExgvsj+NfA0f5zX6tc6Gk4R/yZW9qpuu2bImp0SWb1r86yTKFSonSr7Yde1Xfs0Yr87BzdLmRfyes+kp1nISG4gafSEYiT3qIGJQMLS4kcQQKNxvTAU955FPKffvYvAru1IbvE0AxVHUjwJqwJSTqx0SoXM2lnkUTR4Dnsk0PZDChO9WyLt3QFDTAfnjSI3c9ptOKcXgZzlBOrv2Z8DGRYqxQaeNKSolsQeqgY9PtRA+WVAzcWnZdJha1ZzOKMHMrEjjYW0WP1xYZpd0Cu86BD5AWgJBA7s2Uom60CKNccUkvboDvYEECFI2QP+mu0gjkAp4poDFSIx1M+eBoCTUJmb2CNgFj5a/K1IU8bD3hmmO2GWlXO9AokJyMTAbcdhMY8MRufO1p4pes0mYBIFgtDqnL6U7m/CJ4B3fJ36/C0Jd0pUW95mTZW8oRiarN9WXVZIhoYlvip/JhK+dgQxGvhMUhVonfrUziXQ6IR57DmcCHqJq44GZ4w7JGlozHk90QS0aJBF31K3YJz5xY29WfJc5P+L14L7fOjqPqsEw3iHrsuEqsusHbohr7Z9O2zlM+H1hdxrvmMq8vxHMGz5IHroFPuV0WsJs6QlpKPtXUhyD4/m8M/pOP+Su0sd4pvI07doa6u2FE23zVRXdlkN2rOdKpoMNstcF2rb9uUzoe2NvYOUSMyzd/erkcI7fqjyPInMpDOMAw42/XiEM9nA/k48mGNcSYRvpZsIdpVsQJyaUFN/ipbLtJhVg/RBQ8YiJvtc4CdwyVPeIO6g7qfTz03Hsm/7rC36Kqu3O7gBF3W2bcuyqSpVNu1zudB+WTRuEIg1EQkMicf12vmK1olhBW+v6LVntrTGJDv41S0Wy+jy1auXhP6NXkyE54FVwitYt7bSLOtKbBD0obPC2hTECpOp8YnV9XpJRvP26oxeLR4HYsXwImjx5J40jcQTIz3X6naLnsgy68tSZqIXtSwatWuKJ77evntPLgUY2uOKXK0i07ixQ/RH+uOnl2AKTXmQM/qiFQjQw5HG1BhcbXKf7nhhAh103BP0jsGTzxyEn1IvZpceoxojiWRkUTgxxs2rCkBXXGkl/rjoOf39Rwve/wWsUavu', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'category': 'CompositeElement', 'element_id': 'b8ae863a5df10800d4307a8d409a5034'}, page_content=\"Intel Sustainability\\n\\n“The impact of climate change is an urgent global threat. Protecting our planet demands immediate action and fresh thinking about how the world operates. As one of the world's leading semiconductor design and manufacturing companies, Intel is in a unique position to make a difference not only in our own operations, but in a way that makes it easier for customers, partners and our whole value chain to take meaningful action too,” Gelsinger said. \\n\\nTo realize this ambitious goal, Intel has set the following interim milestones for 2030:\\n\\xa0\\n\\nAchieve 100% renewable electricity use across its global operations.\\n\\nInvest approximately $300 million in energy conservation at its facilities to achieve 4 billion cumulative kilowatt hours of energy savings.\\n\\nBuild new factories and facilities to meet US Green Building Council LEED program standards, including recently announced investments in the US, Europe and Asia.\\n\\nLaunch a cross-industry R&D initiative to identify greener chemicals with lower global warming potential and to develop new abatement equipment.\")"]},"metadata":{},"execution_count":80}],"source":["data[4]"]},{"cell_type":"markdown","metadata":{"id":"ewBJD6nbfYxg"},"source":["### Directory Loaders\n","\n","LangChain's [`DirectoryLoader`](https://api.python.langchain.com/en/latest/document_loaders/langchain_community.document_loaders.directory.DirectoryLoader.html) implements functionality for reading files from disk into LangChain [`Document`](https://api.python.langchain.com/en/latest/documents/langchain_core.documents.base.Document.html#langchain_core.documents.base.Document) objects."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uvblyc6OZqpe","colab":{"base_uri":"https://localhost:8080/"},"outputId":"08e79d6c-b3b1-455f-f633-54bd47f95fa1","executionInfo":{"status":"ok","timestamp":1756901677056,"user_tz":-330,"elapsed":679,"user":{"displayName":"Prashant Sahu","userId":"17861245123904241513"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-09-03 12:14:36--  https://arxiv.org/pdf/2010.11929.pdf\n","Resolving arxiv.org (arxiv.org)... 151.101.131.42, 151.101.3.42, 151.101.67.42, ...\n","Connecting to arxiv.org (arxiv.org)|151.101.131.42|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: /pdf/2010.11929 [following]\n","--2025-09-03 12:14:37--  https://arxiv.org/pdf/2010.11929\n","Reusing existing connection to arxiv.org:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 3743814 (3.6M) [application/pdf]\n","Saving to: ‘Vision Transformers.pdf’\n","\n","Vision Transformers 100%[===================>]   3.57M  13.1MB/s    in 0.3s    \n","\n","2025-09-03 12:14:37 (13.1 MB/s) - ‘Vision Transformers.pdf’ saved [3743814/3743814]\n","\n"]}],"source":["!wget -O 'Vision Transformers.pdf' 'https://arxiv.org/pdf/2010.11929.pdf'"]},{"cell_type":"markdown","metadata":{"id":"eCXVqm3Ba2I8"},"source":["We first define and assign specific loaders which can be used by LangChain when processing the files for a specific file type. We follow this format\n","\n","```\n","loaders = {\n","  'file_format_extension' : (LoaderClass, LoaderKeywordArguments)\n","}\n","```\n","\n","Where:\n","\n","- `file_format_extension` can be anything like `.docx`, `.pdf`etc.\n","- `LoaderClass` is a specific data loader like `PyMuPDFLoader`\n","- `LoaderKeywordArguments` are any specific keyword arguments which needs to be passed into that loader at runtime"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1zzlBioka1HR"},"outputs":[],"source":["# Define a dictionary to map file extensions to their respective loaders\n","loaders = {\n","    '.pdf': (PyMuPDFLoader, {}),\n","    '.docx': (UnstructuredWordDocumentLoader, {'strategy': 'fast',\n","                                              'chunking_strategy' : 'by_title',\n","                                              'max_characters' : 3000, # max limit of a document chunk\n","                                              'new_after_n_chars' : 2500, # preferred document chunk size\n","                                              'mode' : 'elements'\n","                                              })\n","}"]},{"cell_type":"markdown","metadata":{"id":"EBJZJtsGbnr7"},"source":["`DirectoryLoader` accepts a `loader_cls` argument, which defaults to `UnstructuredLoader` but we can pass our own loaders which we defined above in the `loader_cls`argument and any keyword args for the loader can be passed in the `loader_kwargs` argument.\n","\n","We can also show a progress bar by setting `show_progress=True`\n","\n","We can use the `glob` parameter to control which files to load based on file patterns"]},{"cell_type":"markdown","metadata":{"id":"-mOmj9K-cB14"},"source":["Here we create two separate loaders to load files which are word documents and PDFs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jbnt25HdXmkc","colab":{"base_uri":"https://localhost:8080/"},"outputId":"371db232-55a6-43e2-e11b-3d4876983680","executionInfo":{"status":"ok","timestamp":1756901713744,"user_tz":-330,"elapsed":276,"user":{"displayName":"Prashant Sahu","userId":"17861245123904241513"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 2/2 [00:00<00:00,  9.82it/s]\n","100%|██████████| 1/1 [00:00<00:00, 29.58it/s]\n"]}],"source":["from langchain_community.document_loaders import DirectoryLoader\n","\n","# Define a function to create a DirectoryLoader for a specific file type\n","def create_directory_loader(file_type, directory_path):\n","    return DirectoryLoader(\n","        path=directory_path,\n","        glob=f\"**/*{file_type}\",\n","        loader_cls=loaders[file_type][0],\n","        loader_kwargs=loaders[file_type][1],\n","        show_progress=True\n","    )\n","\n","# Create DirectoryLoader instances for each file type\n","pdf_loader = create_directory_loader('.pdf', './')\n","docx_loader = create_directory_loader('.docx', './')\n","\n","# Load the files\n","pdf_documents = pdf_loader.load()\n","docx_documents = docx_loader.load()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GXYc4p_eXmmt","colab":{"base_uri":"https://localhost:8080/"},"outputId":"cb878fa2-c9bb-461d-b3e5-ad7644be6823","executionInfo":{"status":"ok","timestamp":1756901718951,"user_tz":-330,"elapsed":14,"user":{"displayName":"Prashant Sahu","userId":"17861245123904241513"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["38"]},"metadata":{},"execution_count":89}],"source":["len(pdf_documents)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ze6DA3dGaTln","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3c1c2126-ff76-4b77-a41a-99b17ac70a96","executionInfo":{"status":"ok","timestamp":1756901721072,"user_tz":-330,"elapsed":25,"user":{"displayName":"Prashant Sahu","userId":"17861245123904241513"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-04T00:19:58+00:00', 'source': 'Vision Transformers.pdf', 'file_path': 'Vision Transformers.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-04T00:19:58+00:00', 'trapped': '', 'modDate': 'D:20210604001958Z', 'creationDate': 'D:20210604001958Z', 'page': 18}, page_content='Published as a conference paper at ICLR 2021\\nwe perform timing of inference speed for the main models of interest, on a TPUv3 accelerator; the\\ndifference between inference and backprop speed is a constant model-independent factor.\\nFigure 12 (left) shows how many images one core can handle per second, across various input sizes.\\nEvery single point refers to the peak performance measured across a wide range of batch-sizes. As\\ncan be seen, the theoretical bi-quadratic scaling of ViT with image size only barely starts happening\\nfor the largest models at the largest resolutions.\\nAnother quantity of interest is the largest batch-size each model can ﬁt onto a core, larger being\\nbetter for scaling to large datasets. Figure 12 (right) shows this quantity for the same set of models.\\nThis shows that large ViT models have a clear advantage in terms of memory-efﬁciency over ResNet\\nmodels.\\n64\\n128\\n224\\n384\\n512\\nInput size [px]\\n102\\n103\\n104\\nPeak inference speed [img/sec/core]\\n64\\n128\\n224\\n384\\n512\\nInput size [px]\\n102\\n103\\nLargest per-core batch-size\\nR50x1\\nR50x2\\nViT-B/32\\nViT-L/32\\nViT-B/16\\nViT-L/16\\nViT-H/14\\nR152x4\\nFigure 12: Left: Real wall-clock timings of various architectures across input sizes. ViT models\\nhave speed comparable to similar ResNets. Right: Largest per-core batch-size ﬁtting on device with\\nvarious architectures across input sizes. ViT models are clearly more memory-efﬁcient.\\nD.6\\nAXIAL ATTENTION\\nAxial Attention (Huang et al., 2020; Ho et al., 2019) is a simple, yet effective technique to run self-\\nattention on large inputs that are organized as multidimensional tensors. The general idea of axial\\nattention is to perform multiple attention operations, each along a single axis of the input tensor,\\ninstead of applying 1-dimensional attention to the ﬂattened version of the input. In axial attention,\\neach attention mixes information along a particular axis, while keeping information along the other\\naxes independent. Along this line, Wang et al. (2020b) proposed the AxialResNet model in which\\nall the convolutions with kernel size 3 × 3 in a ResNet50 are replaced by axial self-attention, i.e.\\na row and column attention, augmented by relative positional encoding. We have implemented\\nAxialResNet as a baseline model.3.\\nMoreover, we have modiﬁed ViT to process inputs in the 2-dimensional shape, instead of a 1-\\ndimensional sequence of patches, and incorporate Axial Transformer blocks, in which instead of\\na self-attention followed by an MLP, we have a a row-self-attention plus an MLP followed by a\\ncolumn-self-attention plus an MLP.\\nFigure 13, present the performance of Axial ResNet, Axial-ViT-B/32 and Axial-ViT-B/16 on Ima-\\ngeNet 5shot linear, when pretrained on JFT dataset, verses the pretraining compute, both in terms of\\nnumber of FLOPs and inference time (example per seconds). As we can see, both Axial-ViT-B/32\\nand Axial-ViT-B/16 do better than their ViT-B counterpart in terms of performance, but it comes at\\n3Our implementation is based on the open-sourced PyTorch implementation in https://github.com/\\ncsrhddlam/axial-deeplab. In our experiments, we reproduced the scores reported in (Wang et al.,\\n2020b) in terms of accuracy, however, our implementation, similar to the open-source implementation, is very\\nslow on TPUs. Therefore, we were not able to use it for extensive large-scale experiments. These may be\\nunlocked by a carefully optimized implementation.\\n19')"]},"metadata":{},"execution_count":90}],"source":["pdf_documents[18]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GRVEmiTvXmpn","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e9ba4bb4-fafd-43d5-d359-c436d3b55c1e","executionInfo":{"status":"ok","timestamp":1756901724808,"user_tz":-330,"elapsed":22,"user":{"displayName":"Prashant Sahu","userId":"17861245123904241513"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{},"execution_count":91}],"source":["len(docx_documents)"]},{"cell_type":"code","source":[],"metadata":{"id":"7Eenp1p1gNB4"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"},"widgets":{"application/vnd.jupyter.widget-state+json":{"69edc49bec3b474ebfdc3c6329f1589d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1aa4336ab39e4c3181fecfd0808cadf8","IPY_MODEL_d5c4cb901bd14cbebf6ae78cbf69d25b","IPY_MODEL_f9c28103f75a4110a55f2788240717c3"],"layout":"IPY_MODEL_df06a9a57fa54b728ab21cceaff76e1b"}},"1aa4336ab39e4c3181fecfd0808cadf8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_881af4a554fe410ba498cdab150c8426","placeholder":"​","style":"IPY_MODEL_f5f92c39595a4739b0e70ca6f3213bec","value":"yolox_l0.05.onnx: 100%"}},"d5c4cb901bd14cbebf6ae78cbf69d25b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_32e6a8d96b7442e1b727a7e434193d4a","max":216625723,"min":0,"orientation":"horizontal","style":"IPY_MODEL_92778237882947f9ac51ec2c8f9823c4","value":216625723}},"f9c28103f75a4110a55f2788240717c3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_638f39df0ee24caabae74e19e2058cd5","placeholder":"​","style":"IPY_MODEL_a6c8db6c7de741158f9d7ec6ff5e7d5f","value":" 217M/217M [00:05&lt;00:00, 24.3MB/s]"}},"df06a9a57fa54b728ab21cceaff76e1b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"881af4a554fe410ba498cdab150c8426":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f5f92c39595a4739b0e70ca6f3213bec":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"32e6a8d96b7442e1b727a7e434193d4a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"92778237882947f9ac51ec2c8f9823c4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"638f39df0ee24caabae74e19e2058cd5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a6c8db6c7de741158f9d7ec6ff5e7d5f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b041978e0fd4452c9b2d28d542f2466e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_45c5534016694859a73645f9fade2f2a","IPY_MODEL_2868b894ad3b4f5e8e63d33f4b9d690a","IPY_MODEL_c7219f3f8e634a65a56ebf2478780770"],"layout":"IPY_MODEL_51547d43d3bd46e28b7fa1eff73fa2ee"}},"45c5534016694859a73645f9fade2f2a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ed225592e2d74b5b9a9cac8b923f161c","placeholder":"​","style":"IPY_MODEL_8961f78d05644cce9dca059ddcaa3b2d","value":"preprocessor_config.json: 100%"}},"2868b894ad3b4f5e8e63d33f4b9d690a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_629ac16e629b43ddab65abfc6b7abd01","max":274,"min":0,"orientation":"horizontal","style":"IPY_MODEL_70cbd08f030f4907a29770f70e0ca7c1","value":274}},"c7219f3f8e634a65a56ebf2478780770":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_91c5d5d13fc94917b959d35587d50dbb","placeholder":"​","style":"IPY_MODEL_3fa6b18395a34aaab65655dbd35bedc5","value":" 274/274 [00:00&lt;00:00, 18.4kB/s]"}},"51547d43d3bd46e28b7fa1eff73fa2ee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ed225592e2d74b5b9a9cac8b923f161c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8961f78d05644cce9dca059ddcaa3b2d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"629ac16e629b43ddab65abfc6b7abd01":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"70cbd08f030f4907a29770f70e0ca7c1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"91c5d5d13fc94917b959d35587d50dbb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3fa6b18395a34aaab65655dbd35bedc5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"19c392acd46a42bc981b3e1682ecb8a8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4ed32f13ba454528a6b40b4357403f1e","IPY_MODEL_7088919464924438a410f4917ddfa2ec","IPY_MODEL_48621baf9caf4fa8853aea2c70f952cd"],"layout":"IPY_MODEL_a3142d6d13104befa7e93df1ea5624f3"}},"4ed32f13ba454528a6b40b4357403f1e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8086baaf17384d81aa352b922fcc6dae","placeholder":"​","style":"IPY_MODEL_5a48691f0dfc46b1bc3695f183ab98e5","value":"config.json: "}},"7088919464924438a410f4917ddfa2ec":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d479bf0fcb7147ed9592bea7601faacb","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d5df623affaf42d2859961ffbbb7ef5a","value":1}},"48621baf9caf4fa8853aea2c70f952cd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_15f9a4a51371449493fa18b343176f09","placeholder":"​","style":"IPY_MODEL_378fd25f62f84cb0b4794b3aef72a038","value":" 1.47k/? [00:00&lt;00:00, 132kB/s]"}},"a3142d6d13104befa7e93df1ea5624f3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8086baaf17384d81aa352b922fcc6dae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5a48691f0dfc46b1bc3695f183ab98e5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d479bf0fcb7147ed9592bea7601faacb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"d5df623affaf42d2859961ffbbb7ef5a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"15f9a4a51371449493fa18b343176f09":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"378fd25f62f84cb0b4794b3aef72a038":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fdc0a8d0da7b4dfaacc52fac46debc1e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c28ab3de323847b2b51767408c92ed3a","IPY_MODEL_ab960021f332469cbe3e27e6bd8395fe","IPY_MODEL_d63b9689725643848fa247bed8e4c3d1"],"layout":"IPY_MODEL_ece582d8020a48febc3055287503c321"}},"c28ab3de323847b2b51767408c92ed3a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7589c7f1d50c42f6bbba966301db05ee","placeholder":"​","style":"IPY_MODEL_1673074d5d134e7c92b6bf1f8290aac2","value":"model.safetensors: 100%"}},"ab960021f332469cbe3e27e6bd8395fe":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_146fd670886041b7908f8d1edc7e3799","max":115434268,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7fec036937144091acc279aea4f99c15","value":115434268}},"d63b9689725643848fa247bed8e4c3d1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3240f40fa2724232b8b6f5dacc7801d7","placeholder":"​","style":"IPY_MODEL_f41f76efbdb3417ca7cfda921f5258fa","value":" 115M/115M [00:01&lt;00:00, 110MB/s]"}},"ece582d8020a48febc3055287503c321":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7589c7f1d50c42f6bbba966301db05ee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1673074d5d134e7c92b6bf1f8290aac2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"146fd670886041b7908f8d1edc7e3799":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7fec036937144091acc279aea4f99c15":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3240f40fa2724232b8b6f5dacc7801d7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f41f76efbdb3417ca7cfda921f5258fa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"60bd2db3afff41fabd142b679177e57a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f0bfecbfd0554a79ab2c7e3dcaf185c3","IPY_MODEL_f925eebf52af48b481628aaf7731a207","IPY_MODEL_04d1c4ff65114c088d2f761a85ca6479"],"layout":"IPY_MODEL_c4a7974fd7a149b0901b1c5b133d8725"}},"f0bfecbfd0554a79ab2c7e3dcaf185c3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ceeee1be7dde4a849284d4da99824b58","placeholder":"​","style":"IPY_MODEL_053bfb601d3540f1b3e3c3ec1a658e5d","value":"model.safetensors: 100%"}},"f925eebf52af48b481628aaf7731a207":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d6f84bb1800c49d6bb69cc9fe0bb35f6","max":46807446,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d33ffcea13c845478c24713f2c9852b4","value":46807446}},"04d1c4ff65114c088d2f761a85ca6479":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6767f5d1c19541e6b6e055de75ee8317","placeholder":"​","style":"IPY_MODEL_454c1d9c7d3d4611a9d196ac1794dda4","value":" 46.8M/46.8M [00:00&lt;00:00, 184MB/s]"}},"c4a7974fd7a149b0901b1c5b133d8725":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ceeee1be7dde4a849284d4da99824b58":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"053bfb601d3540f1b3e3c3ec1a658e5d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d6f84bb1800c49d6bb69cc9fe0bb35f6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d33ffcea13c845478c24713f2c9852b4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6767f5d1c19541e6b6e055de75ee8317":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"454c1d9c7d3d4611a9d196ac1794dda4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}