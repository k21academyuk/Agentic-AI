{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/k21academyuk/Agentic-AI/blob/main/Advanced_Prompt_Engineering_Techniques.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbPQxe3Y0t4w"
      },
      "source": [
        "# Lab: Prompt Engineering Techniques\n",
        "Prompt engineering techniques focus on refining how AI models respond to specific queries, allowing users to better tailor the interaction and achieve more accurate, relevant, and contextually appropriate responses.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnOaKCRW0t4y"
      },
      "source": [
        "### What is Prompt Engineering?\n",
        "Prompt Engineering is the art of crafting and refining the input (or \"prompt\") you give to an AI model in order to get the best possible output. It’s about knowing how to ask the right questions or set up the right context for the AI so that it understands the task and gives a useful and relevant response.\n",
        "\n",
        "In simpler terms, prompt engineering is like teaching the AI how to do a task by giving it clear and specific instructions. The better your prompt, the better the AI’s response.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJ9h9BBL0t4z"
      },
      "source": [
        "### Prompt Engineering Techniques\n",
        "1. **Zero-shot Prompting:** The model performs a task without any examples or prior context, relying solely on the provided instructions. It uses its pre-trained knowledge to generate the answer.\n",
        "\n",
        "2. **Few-shot Learning:** The model is provided with a small set of examples, allowing it to better grasp the task. The more examples it gets, the more accurately it can predict or generate outcomes.\n",
        "\n",
        "3. **Chain-of-Thought Prompting:** The model is encouraged to think through the task step-by-step, breaking it down into smaller, logical parts. This process helps improve reasoning and results in more accurate answers.\n",
        "\n",
        "4. **Tree-of-Thoughts:** An advanced form of chain-of-thought prompting, where the model explores multiple possible solutions or paths. It generates different answers and evaluates them in a branched structure, allowing for deeper exploration and decision-making."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DH-X0AXN0t4z"
      },
      "source": [
        "## **Lab Execution**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gW0InbMf0t4z"
      },
      "source": [
        "### 1. Installing Dependencies\n",
        "\n",
        "In this cell, the necessary Python libraries are installed. These libraries include:\n",
        "\n",
        "```google-generativeai:```The library for interacting with Google's generative AI models.\n",
        "\n",
        "```langchain-google-genai:``` A connector library that integrates Google's generative AI models with LangChain for building LLM-based applications.\n",
        "\n",
        "```streamlit:``` A tool used to build interactive web applications, which can be helpful for visualizing and testing AI models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8EECJ4LvmMeB",
        "outputId": "c934952b-dd18-426b-f8d9-285cedaba76f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.5)\n",
            "Collecting langchain-google-genai\n",
            "  Downloading langchain_google_genai-2.1.8-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting streamlit\n",
            "  Downloading streamlit-1.47.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.25.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.176.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.11.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.14.1)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain-google-genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "INFO: pip is looking at multiple versions of langchain-google-genai to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting langchain-google-genai\n",
            "  Downloading langchain_google_genai-2.1.7-py3-none-any.whl.metadata (7.0 kB)\n",
            "  Downloading langchain_google_genai-2.1.6-py3-none-any.whl.metadata (7.0 kB)\n",
            "  Downloading langchain_google_genai-2.1.5-py3-none-any.whl.metadata (5.2 kB)\n",
            "  Downloading langchain_google_genai-2.1.4-py3-none-any.whl.metadata (5.2 kB)\n",
            "  Downloading langchain_google_genai-2.1.3-py3-none-any.whl.metadata (4.7 kB)\n",
            "  Downloading langchain_google_genai-2.1.2-py3-none-any.whl.metadata (4.7 kB)\n",
            "  Downloading langchain_google_genai-2.1.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "INFO: pip is still looking at multiple versions of langchain-google-genai to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading langchain_google_genai-2.1.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "  Downloading langchain_google_genai-2.0.11-py3-none-any.whl.metadata (3.6 kB)\n",
            "  Downloading langchain_google_genai-2.0.10-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.37 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (0.3.69)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m937.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.25.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.47.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (0.4.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (6.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.7.14)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.73.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (3.0.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.26.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (3.11.0)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (0.23.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (0.16.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (1.3.1)\n",
            "Downloading langchain_google_genai-2.0.10-py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading streamlit-1.47.0-py3-none-any.whl (9.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: filetype, watchdog, pydeck, streamlit, langchain-google-genai\n",
            "Successfully installed filetype-1.2.0 langchain-google-genai-2.0.10 pydeck-0.9.1 streamlit-1.47.0 watchdog-6.0.0\n"
          ]
        }
      ],
      "source": [
        "# Install necessary packages for the lab\n",
        "!pip install google-generativeai langchain-google-genai streamlit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "685YEuVo0t40"
      },
      "source": [
        "### 2. Importing Required Modules and Setting Up API Key\n",
        "This cell imports the required Python modules and sets up the API key for authentication:\n",
        "\n",
        "```os:``` Used to interact with the operating system, allowing the setting of environment variables like the API key.\n",
        "\n",
        "```google.generativeai:``` Imports Google's generative AI library for model interactions.\n",
        "\n",
        "**Note:** In the below code, please replace with your own API key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iARNcL7HmRLe"
      },
      "outputs": [],
      "source": [
        "# Import necessary modules\n",
        "import os  # For interacting with the operating system, setting environment variables\n",
        "import google.generativeai as genai  # Import Google's generative AI library\n",
        "\n",
        "# API Key for authentication\n",
        "# Set the environment variable for the API key securely\n",
        "os.environ['GOOGLE_API_KEY'] = \"AIzaSyAGGWI4B0dMzKrsDSoumJu48tKpwKtE2sQ\" #replace it with your own API Key\n",
        "\n",
        "# Configure the generative AI library with the provided API key\n",
        "genai.configure(api_key = os.environ['GOOGLE_API_KEY'])  # Authenticates using the API key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRAduuy30t40"
      },
      "source": [
        "This ensures that the API key is not hardcoded and is stored securely, making it ready for use in the following steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItYl8FKm0t41"
      },
      "source": [
        "### 3. Initializing the Generative AI Model\n",
        "\n",
        "In this cell, the generative AI model (gemini-2.0-flash) is initialized. This model will be used to generate text content based on the input provided."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YpN-6m-WmjPS"
      },
      "outputs": [],
      "source": [
        "# Initialize the generative AI model (gemini-2.0-flash) to generate text content\n",
        "model = genai.GenerativeModel('gemini-2.0-flash')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pF96mA-60t41"
      },
      "source": [
        "It initializes the model, making it ready for generating text when queried."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zedsiGd20t41"
      },
      "source": [
        "### 4. Zero-shot Prompting:\n",
        "\n",
        "**Definition**: Zero-shot learning involves asking the model to perform a task without any examples. The model understands the task just by reading the prompt.\n",
        "\n",
        "#### Example:\n",
        "Task: Translate a sentence from English to French."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "0hVWDtXuqUeL",
        "outputId": "5ebc09cb-ce58-442f-d1b9-d75ae5d2d9f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zero-Shot Response: There are several ways to translate \"How are you?\" into French, depending on the level of formality:\n",
            "\n",
            "*   **Formal:** Comment allez-vous ? (This is the most polite and formal option, using the \"vous\" form.)\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Zero-shot prompt\n",
        "prompt = \"Translate the following sentence into French: 'How are you?'\"\n",
        "\n",
        "# Generate a response from the model with advanced text generation settings\n",
        "response = model.generate_content(\n",
        "    prompt,\n",
        "    generation_config=genai.GenerationConfig(\n",
        "        max_output_tokens=50,  # Limit the response length to 50 tokens\n",
        "        temperature=0.7,       # Control the randomness of the response\n",
        "        top_k=40,              # Limit the model's choices to top 40 most probable words\n",
        "        top_p=0.9              # Use nucleus sampling with top-p=0.9 for diversity\n",
        "    )\n",
        ")\n",
        "\n",
        "# Print the response generated by the model\n",
        "print(\"Zero-Shot Response:\", response.text)  # Display the result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Key Parameters for Controlling Model Responses:**\n",
        "\n",
        "1. ```max_output_tokens:```\n",
        "\n",
        "* This parameter controls how long the model's response can be. It limits the number of words or smaller pieces (called tokens) the model generates.\n",
        "\n",
        " **Example:** If you're asking the model to translate a sentence, you might want to make sure it doesn't give you a super long response. So, the model will stop after a certain number of tokens, ensuring the translation doesn't go beyond what's necessary.\n",
        "\n",
        "\n",
        "2. ```temperature:```\n",
        "\n",
        "* Temperature controls how creative or random the model's answers are.\n",
        "\n",
        "* A higher temperature means the model can give more creative or unpredictable answers.\n",
        "\n",
        "* A lower temperature makes the model's answers more straightforward and predictable.\n",
        "\n",
        " **Example:** If you ask the model to write a story:\n",
        "\n",
        "* A high temperature might make the story more wild and imaginative (e.g., a dragon flying through the clouds).\n",
        "\n",
        "* A low temperature will keep it more basic and predictable (e.g., a dragon flying over a mountain).\n",
        "\n",
        "* A balance between high and low temperature keeps it creative but still relevant.\n",
        "\n",
        "3. ```top_k:```\n",
        "\n",
        "* This limits the number of word options the model can choose from when generating a response. It focuses on the most likely words.\n",
        "\n",
        " **Example:** Imagine you're typing a message, and you want the next word to be one of a few choices (like \"good,\" \"bad,\" or \"okay\"). The model can only choose from the top few possible words, making the response more focused and less random.\n",
        "\n",
        " This helps avoid the model selecting strange or irrelevant words.\n",
        "\n",
        "4. ```top_p:```\n",
        "\n",
        " This is a different way to control the model's choice of words. Instead of just picking from a set number of top words, it looks at the cumulative probability of words.\n",
        "\n",
        " **Example:** Instead of only picking the top 10 most likely words, the model can look at a larger set of words that together have a high chance of making sense.\n",
        "\n",
        " **Why it helps:** It gives the model more freedom to pick words, which can result in more diverse or natural-sounding responses, without going too far off-track."
      ],
      "metadata": {
        "id": "fXEX8HIGNnoa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjkI54Jt0t41"
      },
      "source": [
        "### 5. Few-shot Prompting:\n",
        "\n",
        "**Definition**: Few-shot learning involves providing the model with a few examples to enhance its understanding of the task. It helps the model generalize the task better by learning from several provided instances.\n",
        "\n",
        "#### Example:\n",
        "Task: Sentiment Analysis with multiple examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "MbZszWaPssWd",
        "outputId": "50937a02-7467-4a3c-90b0-be7585801443"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Few-Shot Sentiment Analysis Response: Neutral\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Few-shot example\n",
        "examples = \"\"\"\n",
        "Example 1: \"I love this movie! It's amazing.\"\n",
        "Answer: Positive\n",
        "\n",
        "Example 2: \"The food was terrible and I had to wait for hours.\"\n",
        "Answer: Negative\n",
        "\n",
        "Example 3: \"The book was okay, neither great nor bad.\"\n",
        "Answer: Neutral\n",
        "\"\"\"\n",
        "\n",
        "# Few-shot prompt\n",
        "prompt = f\"{examples}\\nNow, classify the sentiment of the following statement:\\n'I'm not sure if I like this movie, it's alright.'\"\n",
        "\n",
        "# Generate the response with advanced parameters\n",
        "response = model.generate_content(\n",
        "    prompt,\n",
        "    generation_config=genai.GenerationConfig(\n",
        "        max_output_tokens=50,  # Limit the response length to 50 tokens\n",
        "        temperature=0.7,       # Control the randomness of the response\n",
        "        top_k=40,              # Limit the model's choices to the top 40 most probable words\n",
        "        top_p=0.9              # Use nucleus sampling with top-p=0.9 for diversity\n",
        "    )\n",
        ")\n",
        "\n",
        "# Display the response\n",
        "print(\"Few-Shot Sentiment Analysis Response:\", response.text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYbcrJ4d0t42"
      },
      "source": [
        "### 6. Chain-of-Thought Prompting:\n",
        "\n",
        "**Definition**: Chain-of-thought prompting breaks down a task into logical steps. This technique encourages the model to think through the task, making the reasoning process more explicit.\n",
        "\n",
        "#### Example:\n",
        "Task: Solve a math problem step by step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "x6y17jZys9yF",
        "outputId": "5e0f32db-de28-45c3-f22b-c25b08fec545"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chain of Thought Response:\n",
            "1. You started with 10 apples.\n",
            "2. You gave away 2 + 2 = 4 apples.\n",
            "3. You had 10 - 4 = 6 apples left.\n",
            "4. You bought 5 more apples.\n",
            "5. You ended up with 6 + 5 = 11 apples.\n",
            "\n",
            "So the answer is 11\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# CoT (Chain of Thought) approach prompt\n",
        "cot_prompt = \"\"\"\n",
        "Question: I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples. How many apples did I remain with?\n",
        "Let's think step by step.\n",
        "\"\"\"\n",
        "\n",
        "# Generate the response with advanced parameters\n",
        "response = model.generate_content(\n",
        "    cot_prompt,  # Use the CoT prompt directly for reasoning\n",
        "    generation_config=genai.GenerationConfig(  # Set the model generation configurations\n",
        "        max_output_tokens=100,  # Limit the response length to 100 tokens\n",
        "        temperature=0.8,        # Moderate randomness to keep answers creative yet focused\n",
        "        top_k=50,               # Restrict choices to top 50 most likely words\n",
        "        top_p=0.9               # Nucleus sampling to add diversity while keeping relevance\n",
        "    )\n",
        ")\n",
        "\n",
        "# Display the model's response (with reasoning)\n",
        "print(\"Chain of Thought Response:\")\n",
        "print(response.text)  # Print the full answer with reasoning steps\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWnMokRs0t42"
      },
      "source": [
        "### 7. Tree of Thoughts Prompting:\n",
        "\n",
        "**Definition**: Tree of Thoughts extends the chain-of-thought approach by exploring multiple possible solutions or paths to a problem. It encourages the model to generate a structured approach with multiple branches of reasoning.\n",
        "\n",
        "#### Example:\n",
        "Task: Plan a road trip."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 926
        },
        "id": "ZXbL6adUuXPo",
        "outputId": "1602c6ee-63a5-4b6d-8d3c-f26cc5e6f08d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Road Trip Planning (Tree of Thought) Response:\n",
            "Okay, let's break down the pros and cons of each route to help you decide which one is best for your road trip.\n",
            "\n",
            "**Option 1: Scenic Route (Route A)**\n",
            "\n",
            "*   **Benefits:**\n",
            "\n",
            "    *   **Enjoyable Journey:** The journey itself becomes part of the vacation, not just a means to an end.\n",
            "    *   **Unique Experiences:** Discover hidden gems, local culture, and unexpected adventures.\n",
            "    *   **Photo Opportunities:** Capture beautiful landscapes, quirky roadside attractions, and memorable moments.\n",
            "    *   **Relaxation and De-stressing:** Slower pace allows for a more relaxed and less stressful travel experience.\n",
            "    *   **Support Local Businesses:** Patronize small-town restaurants, shops, and attractions along the way.\n",
            "    *   **Flexibility:** More opportunities to stop and explore whatever catches your eye.\n",
            "    *   **Memorable Stops:** You might see things you'll never forget.\n",
            "    *   **Discovery:** Learn more about the area you're traveling through - history, geography, culture.\n",
            "\n",
            "**Option 2: Faster Route (Route B)**\n",
            "\n",
            "*   **Benefits:**\n",
            "\n",
            "    *   **Time Savings:** Reach your destination sooner, allowing you to spend more time *at* your destination.\n",
            "    *   **Lower Fuel Costs (Potentially):** Shorter distance often translates to less fuel consumption (though this depends on speed and terrain).\n",
            "    *   **Less Driving Fatigue:** Shorter driving time can lead to less physical and mental fatigue.\n",
            "    *   **Predictability:** Often involves major highways with more consistent road conditions and readily available services (gas stations, restrooms, restaurants).\n",
            "    *   **Stick to a Schedule:** Easier to adhere to a strict itinerary if you have time constraints.\n",
            "    *   **Less Accommodation Cost:** Arriving sooner means less need to stop for an overnight stay.\n",
            "\n",
            "**Which Route Should You Take?**\n",
            "\n",
            "This depends entirely on your priorities and preferences. Consider these questions:\n",
            "\n",
            "*   **How much time do you have for the trip?** If you're short on time, the faster route is the obvious choice.\n",
            "*   **What is your budget?** Scenic routes can sometimes be more expensive due to lodging and meals in touristy areas. However, they can also be cheaper if you camp or pack your own food.\n",
            "*   **What are your interests?** If you enjoy sightseeing, photography, and exploring new places, the scenic route is a better fit.\n",
            "*   **Who are you traveling with?** Consider the preferences of your travel companions. Are they patient and adventurous, or do they prefer a more efficient trip?\n",
            "*   **What is your destination?** Is the destination more important than the journey, or vice versa?\n",
            "*   **What time of year are you traveling?** Scenic routes can be more enjoyable in good weather, while faster routes might be preferable in inclement weather.\n",
            "*   **What kind of car are you driving?** Some scenic routes have unpaved or challenging roads that may not be suitable for all vehicles.\n",
            "\n",
            "**To help me give you a more tailored recommendation, tell me:**\n",
            "\n",
            "*   **How much time do you have for the trip? (e.g., a weekend, a week, etc.)**\n",
            "*   **What is your destination?**\n",
            "*   **Who are you traveling with?**\n",
            "*   **What are your main interests on this trip? (e.g., relaxation, adventure, photography, history, etc.)**\n",
            "*   **What is your approximate budget for the trip?**\n",
            "\n",
            "Once I have this information, I can give you a much more specific and helpful recommendation!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# TOT (Tree of Thought) prompt\n",
        "tot_prompt = \"\"\"\n",
        "Let's plan a road trip. I have two options for routes:\n",
        "1. Route A: A scenic route with lots of sightseeing opportunities.\n",
        "2. Route B: A faster route with fewer stops.\n",
        "\n",
        "Let's think through the advantages of each option.\n",
        "Option 1: What are the benefits of taking the scenic route?\n",
        "Option 2: What are the benefits of taking the faster route?\n",
        "Which route should I take?\n",
        "\"\"\"\n",
        "\n",
        "# Generate the response with advanced parameters\n",
        "response = model.generate_content(\n",
        "    tot_prompt,  # Pass the prompt for decision-making as the first argument\n",
        "    generation_config=genai.GenerationConfig(  # Configure the model's text generation parameters\n",
        "        max_output_tokens=2000,  # Allow longer, detailed response\n",
        "        temperature=0.8,         # Balanced creativity to make responses varied but sensible\n",
        "        top_k=50,               # Limit the choices to the top 50 most probable words\n",
        "        top_p=0.9               # Nucleus sampling for diversity while ensuring relevance\n",
        "    )\n",
        ")\n",
        "\n",
        "# Display the response from the model\n",
        "print(\"Road Trip Planning (Tree of Thought) Response:\")\n",
        "print(response.text)  # Output the reasoned conclusion about the best route\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}