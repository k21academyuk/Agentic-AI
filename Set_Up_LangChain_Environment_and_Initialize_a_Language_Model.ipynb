{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Set Up LangChain Environment and Initialize a Language Models"
      ],
      "metadata": {
        "id": "Fz0N3dvYAzh9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In this lab we will cover:**\n",
        "\n",
        "1. What is LangChain (quick intro)\n",
        "\n",
        "2. Initializing language model (OpenAI)\n",
        "3. Basic model usage: generating completions\n",
        "4. Working with chat models\n",
        "5. Using Prompt Templates\n",
        "6. Putting it all together in an LLMChain"
      ],
      "metadata": {
        "id": "UKxxEatTCa0W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. What is LangChain**\n",
        "\n",
        "**LangChain** is a framework that makes it easier to build applications using large language models (LLMs) like GPT. It helps developers create complex AI systems that can interact with data, tools, and APIs in a structured way.\n",
        "\n",
        "**Here are the key features in simpler terms:**\n",
        "\n",
        "1. **Chains:** These are steps that work together in a sequence. For example, one step could ask an LLM a question, and the next step could use the answer to search for information online.\n",
        "\n",
        "2. **Memory:** LangChain can remember information from previous steps or interactions, so your AI can have ongoing, meaningful conversations or keep track of context across multiple actions.\n",
        "\n",
        "3. **Tooling:** LangChain connects LLMs to external services, like databases or APIs, so your AI can get information or perform actions beyond just text generation.\n",
        "\n",
        "4. **Prompts** It helps you easily create and manage questions or instructions (called prompts) for the AI to respond to, with the ability to adjust based on the context.\n",
        "\n",
        "5. **Agents:** These are AI systems that decide what actions to take based on input from the user or other systems. They use the LLM to guide decisions, like a smart assistant.\n",
        "\n",
        "In short, LangChain helps you build more advanced AI applications that can talk, understand, and take actions based on real-world data."
      ],
      "metadata": {
        "id": "XE0IvSkkRaJz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lab Execution"
      ],
      "metadata": {
        "id": "8B1KH_EVyZ6t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Pre-requisites**\n",
        "\n",
        "You will need an OpenAI API key to complete this lab.\n",
        "If you donot have one yet, please create it by following the guide **[Create API Keys for Various LLMs](https://https://www.skool.com/k21academy/classroom/0a7dc03d?md=6910d36b55a1496d95e5fa4cf3fb1460)** — specifically, the section **Creating OpenAI API Keys.**"
      ],
      "metadata": {
        "id": "PJZbheaYyeTr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Intializing Language Model**"
      ],
      "metadata": {
        "id": "aO37zUw9DAjy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Installing Langchain and OpenAI packages\n",
        "\n",
        "- ```langchain==0.3.27```\n",
        "It installs LangChain, a tool that helps you build applications using language models, like GPT.\n",
        "\n",
        "- ```openai==1.98.0```\n",
        "It installs the OpenAI package, which allows you to interact with OpenAI’s models (like GPT-3).\n",
        "\n",
        "- ```langchain_openai==0.3.8```\n",
        "It installs the integration package that connects LangChain with OpenAI’s models, enabling seamless use of both together."
      ],
      "metadata": {
        "id": "om76LPUwDKT8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-H_ixCDMNZJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23bdce96-231d-4d79-d3cd-bfe6b05af7ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain==0.3.27 in /usr/local/lib/python3.11/dist-packages (0.3.27)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.27) (0.3.72)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.27) (0.3.9)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.27) (0.4.9)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.27) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.27) (2.0.42)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.27) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.27) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain==0.3.27) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain==0.3.27) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain==0.3.27) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain==0.3.27) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain==0.3.27) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain==0.3.27) (3.11.1)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain==0.3.27) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain==0.3.27) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.27) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.27) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.27) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.3.27) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.3.27) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.3.27) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.3.27) (2025.7.14)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.3.27) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain==0.3.27) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain==0.3.27) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain==0.3.27) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain==0.3.27) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain==0.3.27) (1.3.1)\n",
            "Requirement already satisfied: openai==1.98.0 in /usr/local/lib/python3.11/dist-packages (1.98.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai==1.98.0) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai==1.98.0) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai==1.98.0) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai==1.98.0) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai==1.98.0) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai==1.98.0) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai==1.98.0) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai==1.98.0) (4.14.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai==1.98.0) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai==1.98.0) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai==1.98.0) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.98.0) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai==1.98.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai==1.98.0) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai==1.98.0) (0.4.1)\n",
            "Collecting langchain_openai==0.3.8\n",
            "  Downloading langchain_openai-0.3.8-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.42 in /usr/local/lib/python3.11/dist-packages (from langchain_openai==0.3.8) (0.3.72)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.58.1 in /usr/local/lib/python3.11/dist-packages (from langchain_openai==0.3.8) (1.98.0)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain_openai==0.3.8) (0.9.0)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.42->langchain_openai==0.3.8) (0.4.9)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.42->langchain_openai==0.3.8) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.42->langchain_openai==0.3.8) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.42->langchain_openai==0.3.8) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.42->langchain_openai==0.3.8) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.42->langchain_openai==0.3.8) (25.0)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.42->langchain_openai==0.3.8) (2.11.7)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai==0.3.8) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai==0.3.8) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai==0.3.8) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai==0.3.8) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai==0.3.8) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai==0.3.8) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai==0.3.8) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai==0.3.8) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.58.1->langchain_openai==0.3.8) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain_openai==0.3.8) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain_openai==0.3.8) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain_openai==0.3.8) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.42->langchain_openai==0.3.8) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.42->langchain_openai==0.3.8) (3.11.1)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.42->langchain_openai==0.3.8) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.42->langchain_openai==0.3.8) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.42->langchain_openai==0.3.8) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.42->langchain_openai==0.3.8) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.42->langchain_openai==0.3.8) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai==0.3.8) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai==0.3.8) (2.5.0)\n",
            "Downloading langchain_openai-0.3.8-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain_openai\n",
            "Successfully installed langchain_openai-0.3.8\n"
          ]
        }
      ],
      "source": [
        "# Installing LangChain version 0.3.27, a framework for building language model applications\n",
        "!pip install langchain==0.3.27\n",
        "\n",
        "# Installing OpenAI Python package version 1.98.0, which allows interaction with OpenAI's API (for example, GPT models)\n",
        "!pip install openai==1.98.0\n",
        "\n",
        "# Installing LangChain OpenAI integration package version 0.3.8, which connects LangChain with OpenAI's models\n",
        "!pip install langchain_openai==0.3.8"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Setting Up OpenAI API Key for Authentication\n",
        "\n",
        "\n",
        "**Syntax:**\n",
        "```\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\"\n",
        "```\n",
        "\n",
        "**Note:** Replace \"your-api-key-here\" with your actual OpenAI API key in the below code. Ensure that you keep your API key secure and do not expose it in public code repositories."
      ],
      "metadata": {
        "id": "eKXc3moPDWlm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the os module, which provides a way to interact with the operating system\n",
        "import os\n",
        "\n",
        "# Set an environment variable named 'OPENAI_API_KEY' with the provided API key value.\n",
        "# This allows the code to access the OpenAI API securely without hardcoding the key elsewhere in the program.\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-kkk9RQvMEYlIY4aYD2tehZxZVgFDlrf9Que1aRjnH*********************************************ILi4CQXa9nT3BlbkFJggG-tIdPHPA2eg58ZNlMMpqAmWPKTRB3Z58RYEA\""
      ],
      "metadata": {
        "id": "MmQLIuNIMumE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Code Explanation*\n",
        "\n",
        "```import os```\n",
        "- This command imports the os module, which provides functions to interact with the operating system. It’s essential for managing environment variables.\n",
        "\n",
        "```os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\"```\n",
        " - This command sets an environment variable named OPENAI_API_KEY with your OpenAI API key. By storing the key this way, the code can access it securely without directly including it in the program, keeping your API key private and safe.\n",
        "\n"
      ],
      "metadata": {
        "id": "sIPmn5tyMRrj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Initializing the LLMs"
      ],
      "metadata": {
        "id": "Px7X4KL6DiI5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the OpenAI class from langchain_openai to interact with OpenAI models\n",
        "from langchain_openai import OpenAI\n",
        "\n",
        "# Create an instance of the OpenAI model with specific parameters\n",
        "# - temperature=0.7: Controls the randomness of the output. Higher values make the output more random, while lower values make it more deterministic.\n",
        "# - model_name=\"gpt-3.5-turbo-instruct\": Specifies the specific OpenAI model to use (in this case, the \"gpt-3.5-turbo-instruct\" model).\n",
        "llm_openai = OpenAI(temperature=0.7, model_name=\"gpt-3.5-turbo-instruct\")"
      ],
      "metadata": {
        "id": "v3X41JdNNMh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Code Explanation*\n",
        "\n",
        "```from langchain_openai import OpenAI```\n",
        "- This command imports the OpenAI class from the langchain_openai package, which allows you to interact with OpenAI's models, such as GPT-3.5, for generating text.\n",
        "\n",
        "```llm_openai = OpenAI(temperature=0.7, model_name=\"gpt-3.5-turbo-instruct\")```\n",
        "- This line creates an instance of the OpenAI model with the following parameters:\n",
        "\n",
        "- ```temperature=0.7:``` Controls the randomness of the output. A value closer to 1 makes the model's responses more creative and random, while a value closer to 0 makes the output more deterministic and focused.\n",
        "\n",
        "- ```model_name=\"gpt-3.5-turbo-instruct\":``` Specifies the exact OpenAI model to use. In this case, it selects the \"gpt-3.5-turbo-instruct\" model, which is designed for handling instruction-based tasks and generating structured responses."
      ],
      "metadata": {
        "id": "leyxSEFBNSIl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Basic LLM Usage – Generating Completions**\n",
        "\n",
        "Generate text completions using OpenAI's language model. The task is to prompt the model to generate a short story based on a given prompt and print the generated response"
      ],
      "metadata": {
        "id": "CexESZPyD1Wf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ask the OpenAI model to write a short story about a robot learning to paint\n",
        "response = llm_openai.invoke(\"Write a short story about a robot learning to paint.\")\n",
        "\n",
        "# Print the story that the model generates\n",
        "print(response)"
      ],
      "metadata": {
        "id": "rAuKhImEOQ8A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82674e68-3c62-4c6f-a685-189e5338b16e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "It was a shiny, silver robot named Robby. He was designed with the latest technology and was programmed to perform various tasks. His creators had given him abilities to clean, cook, and even play music. But there was one thing that Robby couldn't do - paint.\n",
            "\n",
            "Robby was fascinated by the beautiful paintings he saw in the art gallery near his home. He would stand outside the gallery and watch the artists at work, trying to understand their techniques. He was determined to learn how to paint, so he asked his creators to add this skill to his programming.\n",
            "\n",
            "At first, Robby struggled to hold the paintbrush and mix the colors. But he was determined to learn, and he practiced every day. His first few attempts were far from perfect, but he didn't give up. He studied the works of various artists and tried to replicate their style.\n",
            "\n",
            "As time passed, Robby's paintings started to improve. His lines became more precise, and his colors more vibrant. His creators were amazed by his progress and encouraged him to keep painting. Robby was overjoyed. He felt a sense of fulfillment and purpose in this new skill that he had acquired.\n",
            "\n",
            "One day, Robby's creators entered him into an art competition. They were confident in his\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Code Explanation*\n",
        "\n",
        "```response = llm_openai.invoke(\"Write a short story about a robot learning to paint.\")```\n",
        "- This command uses the **invoke** method of the **llm_openai** object (which is an instance of the OpenAI model). The invoke method sends a request to the OpenAI model with a prompt, and the model generates a response based on that input.\n",
        "\n",
        "- The **\"Write a short story about a robot learning to paint.\"** is the prompt that instructs the model to generate the story.\n",
        "\n",
        "- The **response** variable stores the output, which is the text generated by the model based on the prompt.\n",
        "\n",
        "```print(response)```\n",
        "- This command outputs the content stored in the response variable to the screen or console. After the model generates a story (or any other type of text), this command displays that generated text."
      ],
      "metadata": {
        "id": "_2g-R50lOUDX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Working with OpenAI Chat Model**\n",
        "\n",
        "In this section, we are working with the OpenAI Chat model using LangChain. We interact with the model by sending a list of user questions, and the model responds based on those inputs. This demonstrates how to use LangChain's ChatOpenAI class for multi-turn interactions."
      ],
      "metadata": {
        "id": "DiqNIbzRD-FA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary classes from langchain\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "# Initialize ChatOpenAI with GPT-3.5-turbo model and temperature setting for randomness\n",
        "chat = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\n",
        "\n",
        "# Create a list of messages where the user is asking questions\n",
        "messages = [\n",
        "    HumanMessage(content=\"What's the capital of France?\"),\n",
        "    HumanMessage(content=\"And who is the current president?\")\n",
        "]\n",
        "\n",
        "# Send the list of messages to the model and get the response\n",
        "response = chat.invoke(messages)\n",
        "\n",
        "# Print the model's answer to the questions\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "wXX4t7YzPfE3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef60c2cf-5ee5-415f-99e9-5b9cb4e53ec9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The capital of France is Paris, and the current president is Emmanuel Macron.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Code Explanation:*\n",
        "\n",
        "```from langchain_openai import ChatOpenAI```\n",
        "\n",
        "```from langchain_core.messages import HumanMessage```\n",
        "- These imports bring in the ChatOpenAI class to interact with the OpenAI chat model and the HumanMessage class to create messages that represent user inputs.\n",
        "\n",
        "\n",
        "```chat = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)```\n",
        "- This creates an instance of the ChatOpenAI model using the GPT-3.5-turbo version. The temperature=0.7 setting makes the responses more creative and varied, but not too random.\n",
        "\n",
        "```\n",
        "messages = [\n",
        "\n",
        "    HumanMessage(content=\"What's the capital of France?\"),\n",
        "    HumanMessage(content=\"And who is the current president?\")\n",
        "]\n",
        "```\n",
        "\n",
        "- This creates a list of HumanMessage objects, each representing a user question. The user is asking about the capital of France and the current president.\n",
        "\n",
        "```response = chat.invoke(messages)```\n",
        "\n",
        "- The invoke method sends the list of user questions to the OpenAI model and retrieves the model's response.\n",
        "\n",
        "```print(response.content)```\n",
        " - This prints the model's response, which in this case answers both questions: \"The capital of France is Paris, and the current president is Emmanuel Macron.\""
      ],
      "metadata": {
        "id": "8oxMsUIfQLAD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Using Prompt Templates\n",
        "\n",
        "Prompt templates are a way to dynamically construct prompts using placeholders that can be filled with different values at runtime.\n",
        "\n",
        "They are useful for creating flexible prompts that adapt to different inputs, making them ideal for production applications where you need to generate various prompts based on user data or other variables.\n",
        "\n",
        "In this section, we are working with Prompt Templates to dynamically create prompts using variables. This allows you to generate custom prompts for OpenAI models, which is especially useful for production environments where prompts need to change based on user input."
      ],
      "metadata": {
        "id": "5zUwdb38EfUF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import PromptTemplate from langchain_core to create a template with placeholders\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "# Create a template where the placeholder {country} will be replaced by the input variable\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"country\"],  # Define that 'country' will be the input variable\n",
        "    template=\"What is the currency in {country}?\"  # The template with a placeholder\n",
        ")\n",
        "\n",
        "# Replace the placeholder {country} with \"USA\" to create the final prompt\n",
        "final_prompt = prompt.format(country=\"USA\")\n",
        "\n",
        "# Print the final prompt\n",
        "print(final_prompt)\n",
        "# Output: What is the currency in USA?"
      ],
      "metadata": {
        "id": "NBK8lyf-PvkK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "211057c4-86b6-4a63-d099-f1323d64aa7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is the currency in USA?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Code Explanation:*\n",
        "\n",
        "```from langchain_core.prompts import PromptTemplate```\n",
        "- This imports the PromptTemplate class from the langchain_core.prompts module, which enables you to create and manage templates with placeholders.\n",
        "\n",
        "```\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"country\"],  # Define that 'country' will be the input variable\n",
        "    template=\"What is the currency in {country}?\"  # The template with a placeholder\n",
        ")\n",
        "```\n",
        "- This creates a PromptTemplate object where {country} is a placeholder that will be replaced with actual input when the prompt is used. The input_variables list indicates that \"country\" will be the dynamic input.\n",
        "\n",
        "\n",
        "```final_prompt = prompt.format(country=\"USA\")```\n",
        " - Here, the {country} placeholder is replaced with the string \"USA\", creating the final prompt: \"What is the currency in USA?\"\n",
        "\n",
        "\n",
        "```print(final_prompt)```\n",
        "- This command prints the final prompt after replacing the placeholder with the value \"USA\". The output will be: \"What is the currency in USA?\""
      ],
      "metadata": {
        "id": "1tT5CzAeTHif"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Composing an LLMChain\n",
        "\n",
        "**What is an LLMChain?**\n",
        "An LLMChain in LangChain is a way to combine a prompt, a language model (LLM), and output parsing in a sequence. This lets you create a smooth workflow where the prompt gets processed by the model, and the model's output can be further handled, making it useful for more complex tasks.\n",
        "\n",
        "\n",
        "In this section, we are creating an LLMChain by combining a prompt and the OpenAI model. We then use this chain to get a response from the model."
      ],
      "metadata": {
        "id": "z7-LUyZPGWlr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary modules\n",
        "from langchain_core.runnables import RunnableSequence\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "# Create a PromptTemplate (same as previous example)\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"country\"],  # Input variable is 'country'\n",
        "    template=\"What is the currency in {country}?\"  # Template with placeholder\n",
        ")\n",
        "\n",
        "# Create a ChatOpenAI instance to interact with the OpenAI model\n",
        "llm_openai = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\n",
        "\n",
        "# Create a sequence of actions (prompt and model) using the '|' operator\n",
        "# This combines the prompt with the model (llm_openai) into a chain\n",
        "chain = prompt | llm_openai\n",
        "\n",
        "# Use the 'invoke' method to pass in a dictionary with a country name\n",
        "output = chain.invoke({\"country\": \"USA\"})\n",
        "\n",
        "# Print the output from the model's response\n",
        "print(output)"
      ],
      "metadata": {
        "id": "WY2ZUxL5P5zI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19a649e2-be01-4fef-c00d-e875b7cea62e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "The currency used in the United States is the United States dollar (USD).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Code Explanation:*\n",
        "\n",
        "```\n",
        "from langchain_core.runnables import RunnableSequence\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "```\n",
        "These lines import the needed tools:\n",
        "\n",
        "- **RunnableSequence** to create a sequence of actions,\n",
        "\n",
        "- **ChatOpenAI** to interact with OpenAI’s chat models,\n",
        "\n",
        "- **PromptTemplate** to create templates for prompts.\n",
        "\n",
        "```\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"country\"],  # Define 'country' as the variable\n",
        "    template=\"What is the currency in {country}?\"  # Template with a placeholder\n",
        ")\n",
        "```\n",
        "- Here, we create a template where {country} is a placeholder. When we use this template, the placeholder will be replaced with the actual country name.\n",
        "\n",
        "\n",
        "```llm_openai = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)```\n",
        "\n",
        "- This line initializes the OpenAI model (gpt-3.5-turbo) with a temperature of 0.7, which controls the creativity of the responses.\n",
        "\n",
        "```chain = prompt | llm_openai```\n",
        "\n",
        "- The **|** operator combines the prompt with the OpenAI model into a sequence. This means the prompt will first be processed and then passed to the model.\n",
        "\n",
        "\n",
        "```output = chain.invoke({\"country\": \"USA\"})```\n",
        "- This line runs the chain by passing a dictionary where \"country\": \"USA\". It triggers the model to generate a response based on the prompt and input.\n",
        "\n",
        "\n",
        "```print(output)```\n",
        "\n",
        "- This will display the model's response, which in this case might be: \"The currency in USA is the Dollar.\""
      ],
      "metadata": {
        "id": "2ZO0OjCz1_mi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## **7. Full Example: Build a Joke Generator**\n",
        "\n",
        " We are building a joke generator using LangChain. We will create a prompt template, chain it with an OpenAI model, and run it to generate a funny joke based on a given topic."
      ],
      "metadata": {
        "id": "ytSs39YAGCmG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary modules\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_openai import OpenAI\n",
        "from langchain_core.runnables import RunnableSequence\n",
        "\n",
        "# 1. Define the prompt\n",
        "# This creates a template where '{topic}' will be replaced with the input topic.\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\"],  # Define 'topic' as the input variable\n",
        "    template=\"Tell me a funny joke about {topic}.\"  # The template string with a placeholder\n",
        ")\n",
        "\n",
        "# 2. Initialize the model\n",
        "# Set up the OpenAI model (GPT-3.5 turbo) with a creativity setting (temperature)\n",
        "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0.8)\n",
        "\n",
        "# 3. Build the chain\n",
        "# Combine the prompt and the model into a sequence using the '|' operator\n",
        "joke_chain = prompt | llm\n",
        "\n",
        "# 4. Run it\n",
        "# Pass a dictionary with the 'topic' as input to the chain and print the output\n",
        "print(joke_chain.invoke({\"topic\": \"programmers\"}))"
      ],
      "metadata": {
        "id": "-yhaFELXRxoa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "636cced2-82fe-4748-feeb-160e3a163d75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Why did the programmer quit his job?\n",
            "\n",
            "Because he didn't get arrays!\n"
          ]
        }
      ]
    }
  ]
}