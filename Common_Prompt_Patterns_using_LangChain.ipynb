{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Common Prompt Patterns using LangChain** (Persona Pattern, Question Refinement Pattern, Cognitive Verifier Pattern, Audience Pattern)"
      ],
      "metadata": {
        "id": "hdnVKKpr12f7"
      },
      "id": "hdnVKKpr12f7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **What is a Prompt?**\n",
        "\n",
        "A **prompt** in the context of AI and Natural Language Processing (NLP) is an input query or instruction given to a language model or AI agent. This prompt guides the model in generating a response based on the context, details, or instructions provided. Prompts are essential for interacting with AI models and determining the output quality and relevance. In the case of language models like GPT, a prompt can be a question, statement, or even a series of instructions that lead to a response that fits the user's needs."
      ],
      "metadata": {
        "id": "AdvqM2fq1qyd"
      },
      "id": "AdvqM2fq1qyd"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **What are Common Prompt Patterns?**\n",
        "\n",
        "Common prompt patterns refer to structured templates or formats used to interact with language models. These patterns help refine the way prompts are given, ensuring that the model produces the most relevant and accurate responses. By designing effective prompt patterns, users can control the type, style, and detail of the responses generated by the AI model.\n",
        "\n",
        "Below are some commonly used prompt patterns:\n",
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "9sxDGNBD150n"
      },
      "id": "9sxDGNBD150n"
    },
    {
      "cell_type": "markdown",
      "id": "4461c8e8-9b11-41b8-bbab-3f7138477c6e",
      "metadata": {
        "id": "4461c8e8-9b11-41b8-bbab-3f7138477c6e"
      },
      "source": [
        "\n",
        "-----\n",
        "\n",
        "### 1\\. Persona Pattern 🎭\n",
        "\n",
        "The **Persona Pattern** involves instructing the LLM to adopt a specific identity, role, or character. This primes the model to answer from a particular point of view, with a certain tone, and with specialized knowledge, leading to more focused and higher-quality responses.\n",
        "\n",
        "#### Concept\n",
        "\n",
        "You give the model a role, like \"You are an expert astrophysicist\" or \"You are a witty, sarcastic travel guide.\" All subsequent answers will be filtered through this persona.\n",
        "\n",
        "#### LangChain Implementation\n",
        "\n",
        "We use the `system` message in a `ChatPromptTemplate` to set the persona. This message tells the model how it should behave throughout the conversation."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "a. **Installing LangChain and Required Packages**\n",
        "\n",
        "In order to work with LangChain, we need to install the core LangChain package, LangChain OpenAI integration, and LangChain Community version. Below is the installation command along with an explanation for each package."
      ],
      "metadata": {
        "id": "Rm7-E3en5ahK"
      },
      "id": "Rm7-E3en5ahK"
    },
    {
      "cell_type": "code",
      "source": [
        "# Install LangChain package (core functionality)\n",
        "!pip install langchain==0.3.27  # This installs the core LangChain library that provides essential tools for building language model-based applications.\n",
        "\n",
        "# Install LangChain OpenAI integration (for OpenAI API usage)\n",
        "!pip install langchain-openai==0.3.8  # This package allows the use of OpenAI models (e.g., GPT-3, GPT-4) within LangChain, enabling smooth integration with OpenAI's API.\n",
        "\n",
        "# Install LangChain Community edition (for additional features)\n",
        "!pip install langchain-community==0.3.27  # This version includes community-driven contributions and additional features that expand the capabilities of LangChain."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSphoOLkRr6u",
        "outputId": "6048dcad-5fc2-43a0-b128-67c36a2d9a7f"
      },
      "id": "FSphoOLkRr6u",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain==0.3.27 in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain==0.3.27) (0.3.74)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain==0.3.27) (0.3.9)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain==0.3.27) (0.4.16)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain==0.3.27) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain==0.3.27) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain==0.3.27) (2.32.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain==0.3.27) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain==0.3.27) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain==0.3.27) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain==0.3.27) (4.15.0)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain==0.3.27) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain==0.3.27) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain==0.3.27) (3.11.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain==0.3.27) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain==0.3.27) (0.24.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.27) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.27) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.27) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain==0.3.27) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain==0.3.27) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain==0.3.27) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain==0.3.27) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.3.27) (3.2.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain==0.3.27) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain==0.3.27) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain==0.3.27) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain==0.3.27) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain==0.3.27) (1.3.1)\n",
            "Collecting langchain-openai==0.3.8\n",
            "  Downloading langchain_openai-0.3.8-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.42 in /usr/local/lib/python3.12/dist-packages (from langchain-openai==0.3.8) (0.3.74)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.58.1 in /usr/local/lib/python3.12/dist-packages (from langchain-openai==0.3.8) (1.101.0)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.12/dist-packages (from langchain-openai==0.3.8) (0.11.0)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.42->langchain-openai==0.3.8) (0.4.16)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.42->langchain-openai==0.3.8) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.42->langchain-openai==0.3.8) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.42->langchain-openai==0.3.8) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.42->langchain-openai==0.3.8) (4.15.0)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.42->langchain-openai==0.3.8) (25.0)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.42->langchain-openai==0.3.8) (2.11.7)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai==0.3.8) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai==0.3.8) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai==0.3.8) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai==0.3.8) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai==0.3.8) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai==0.3.8) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1,>=0.7->langchain-openai==0.3.8) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1,>=0.7->langchain-openai==0.3.8) (2.32.4)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.58.1->langchain-openai==0.3.8) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain-openai==0.3.8) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain-openai==0.3.8) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain-openai==0.3.8) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.42->langchain-openai==0.3.8) (3.0.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.42->langchain-openai==0.3.8) (3.11.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.42->langchain-openai==0.3.8) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.42->langchain-openai==0.3.8) (0.24.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.42->langchain-openai==0.3.8) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.42->langchain-openai==0.3.8) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.42->langchain-openai==0.3.8) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai==0.3.8) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai==0.3.8) (2.5.0)\n",
            "Downloading langchain_openai-0.3.8-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain-openai\n",
            "Successfully installed langchain-openai-0.3.8\n",
            "Collecting langchain-community==0.3.27\n",
            "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.12/dist-packages (from langchain-community==0.3.27) (0.3.74)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /usr/local/lib/python3.12/dist-packages (from langchain-community==0.3.27) (0.3.27)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain-community==0.3.27) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain-community==0.3.27) (2.32.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community==0.3.27) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community==0.3.27) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community==0.3.27) (8.5.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community==0.3.27)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community==0.3.27) (2.10.1)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community==0.3.27) (0.4.16)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community==0.3.27) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community==0.3.27) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.27) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.27) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.27) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.27) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.27) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.27) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.27) (1.20.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.27)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.27)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community==0.3.27) (0.3.9)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community==0.3.27) (2.11.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community==0.3.27) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community==0.3.27) (4.15.0)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community==0.3.27) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community==0.3.27) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community==0.3.27) (3.11.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community==0.3.27) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community==0.3.27) (0.24.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community==0.3.27) (1.1.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community==0.3.27) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain-community==0.3.27) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain-community==0.3.27) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain-community==0.3.27) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain-community==0.3.27) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community==0.3.27) (3.2.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community==0.3.27) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community==0.3.27) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community==0.3.27) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain-community==0.3.27) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community==0.3.27) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community==0.3.27) (2.33.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.27)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community==0.3.27) (1.3.1)\n",
            "Downloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 langchain-community-0.3.27 marshmallow-3.26.1 mypy-extensions-1.1.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "b. **Setting Up the OpenAI API Key Securely**\n",
        "\n",
        "The code below demonstrates how to securely input and set up the OpenAI API key for use within your environment. This ensures that your API key is kept private and not hard-coded into the script.\n",
        "\n",
        "***Note:*** Please paste your OpenAI API key in the dialog box below and press the Enter button. If you haven't created an API key yet, refer to the \"Prerequisites\" section for instructions on how to generate one. Remember to keep your API key secure and avoid exposing it in public code repositories."
      ],
      "metadata": {
        "id": "JAFDW3DX5sqJ"
      },
      "id": "JAFDW3DX5sqJ"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "624bdba3-e95b-4ddf-b0c4-167de528807b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "624bdba3-e95b-4ddf-b0c4-167de528807b",
        "outputId": "a0dec497-1745-4c7f-a587-315de822d3ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter Open AI API Key: ··········\n"
          ]
        }
      ],
      "source": [
        "# Import the getpass module for securely entering the OpenAI API key\n",
        "from getpass import getpass\n",
        "\n",
        "# Prompt the user to enter the OpenAI API key without displaying it on the screen\n",
        "OPENAI_KEY = getpass('Enter Open AI API Key: ')  # User input will be masked for privacy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# After entering the key, we store it as an environment variable for use later in the script\n",
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = OPENAI_KEY  # Set the environment variable to the entered API key"
      ],
      "metadata": {
        "id": "ov5UnVYzRjZH"
      },
      "id": "ov5UnVYzRjZH",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "c. **Creating a Chat-Based AI System Using LangChain**\n",
        "\n",
        "This code demonstrates how to set up a language model, define a prompt with a persona, create a chain of operations, and invoke the chain to answer a question. Let's break it down:"
      ],
      "metadata": {
        "id": "wq31A-MG6H8v"
      },
      "id": "wq31A-MG6H8v"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "1799cedf-6767-46f7-9b1f-f011b409301d",
      "metadata": {
        "id": "1799cedf-6767-46f7-9b1f-f011b409301d"
      },
      "outputs": [],
      "source": [
        "# 1. Set up the model\n",
        "# Initialize the ChatOpenAI model with the specified settings.\n",
        "# We are using gpt-3.5-turbo for this task with a temperature setting of 0.3 for controlled randomness in responses.\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.3)\n",
        "\n",
        "# 2. Define the prompt with a Persona\n",
        "# The 'system' message sets a persona for the AI model. This persona defines the behavior and style of the model’s responses.\n",
        "# Here, we are instructing the model to act as a world-class cybersecurity expert who explains complex topics simply, using analogies.\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a world-class cybersecurity expert with a knack for explaining complex topics in simple terms. You are direct and use analogies.\"),\n",
        "    (\"human\", \"{question}\")\n",
        "])\n",
        "\n",
        "# 3. Create the chain\n",
        "# The chain connects the prompt, the model, and the output parser into a flow that can process and generate responses.\n",
        "chain = prompt | llm | StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "fa66bc23-3a13-42df-8e69-d36d0c638840",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fa66bc23-3a13-42df-8e69-d36d0c638840",
        "outputId": "346e48bd-306f-48af-f092-1d0c41bd5b28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Think of a zero-day exploit as a secret backdoor in a house that only the burglar knows about. In the world of cybersecurity, a zero-day exploit is a vulnerability in software that is unknown to the software developer and therefore has no fix or patch available. This makes it extremely dangerous because hackers can exploit this vulnerability to launch attacks without the software developer being aware of it.\n",
            "\n",
            "Imagine if a thief knew about a hidden entrance in your house that you didn't even know existed. They could break in and steal your valuables without you ever realizing how they got in. Similarly, hackers can use zero-day exploits to infiltrate systems, steal sensitive information, or cause damage without the software developer or security teams being able to defend against it.\n",
            "\n",
            "Because zero-day exploits are not yet known to the software developer, there is no immediate fix available, leaving systems vulnerable until a patch is developed. This makes zero-day exploits highly sought after by cybercriminals and state-sponsored hackers because they provide a window of opportunity to carry out attacks undetected.\n"
          ]
        }
      ],
      "source": [
        "# 4. Invoke the chain with a question\n",
        "# Here, we provide a question to the system, asking about a 'zero-day exploit'.\n",
        "# The system processes the input using the defined persona and generates a relevant response.\n",
        "question = \"What is a 'zero-day exploit' and why is it so dangerous?\"\n",
        "response = chain.invoke({\"question\": question})\n",
        "\n",
        "# Print the response\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Explanation\n",
        "\n",
        "In this code, the `(\"system\", ...)` message assigns the **persona** of a cybersecurity expert. When the chain is invoked, the model doesn't just give a generic definition; it adopts the specified character. The response will likely use an analogy (e.g., \"Imagine a security company builds an indestructible lock, but a thief finds a secret, un-patched flaw on the very day it's released... that's a zero-day exploit\") because the persona instructed it to.\n",
        "\n",
        "----"
      ],
      "metadata": {
        "id": "jVRWI6uT2u6D"
      },
      "id": "jVRWI6uT2u6D"
    },
    {
      "cell_type": "markdown",
      "id": "6d1ef887-6324-48ec-9701-d682ea91b8a0",
      "metadata": {
        "id": "6d1ef887-6324-48ec-9701-d682ea91b8a0"
      },
      "source": [
        "### 2\\. Question Refinement Pattern 🧐\n",
        "\n",
        "The **Question Refinement Pattern** is used when a user's initial query is too vague or broad. Instead of trying to answer the ambiguous question directly, the model first helps the user formulate a better, more specific question.\n",
        "\n",
        "#### Concept\n",
        "\n",
        "The LLM acts as an expert assistant, analyzing the user's question and suggesting clearer, more effective versions of it before providing a final answer. This is a multi-step process.\n",
        "\n",
        "#### LangChain Implementation\n",
        "\n",
        "This is a perfect use case for chaining multiple components together using LCEL. We'll create one chain to refine the question and a second chain to answer the *refined* question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "c273cdbd-4c00-4303-b0f9-309f17018b5e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c273cdbd-4c00-4303-b0f9-309f17018b5e",
        "outputId": "e6e8a42d-f338-4632-cb4e-29b730bc5c27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As of 2021, there are several electric car models on the market that offer a range of features and specifications. Some of the key features and specifications of the latest electric car models include:\n",
            "\n",
            "1. Range: One of the most important factors to consider when purchasing an electric car is the range it can travel on a single charge. Many of the latest electric car models offer ranges of over 200 miles, with some models even exceeding 300 miles on a single charge.\n",
            "\n",
            "2. Charging time: Another important factor to consider is the charging time of the electric car. Many of the latest models come equipped with fast-charging capabilities, allowing them to charge up to 80% in as little as 30 minutes at a DC fast-charging station.\n",
            "\n",
            "3. Performance: Electric cars are known for their quick acceleration and smooth driving experience. Many of the latest models offer impressive performance specs, with 0-60 mph times rivaling those of traditional gas-powered vehicles.\n",
            "\n",
            "4. Technology: Electric cars are often equipped with the latest technology features, such as touchscreen infotainment systems, smartphone integration, and advanced driver-assist systems. Some models even offer over-the-air software updates to keep the car's systems up to date.\n",
            "\n",
            "5. Design: The latest electric car models come in a variety of designs, from sleek and futuristic to more traditional styles. Many models offer spacious interiors and cargo space, making them practical for everyday use.\n",
            "\n",
            "6. Sustainability: Electric cars are known for their environmentally friendly nature, emitting zero tailpipe emissions and reducing overall carbon footprint. Many of the latest models are made with sustainable materials and are designed with energy efficiency in mind.\n",
            "\n",
            "Overall, the latest electric car models on the market offer a combination of impressive range, fast charging capabilities, high performance, advanced technology features, stylish design, and sustainability. These features make electric cars a compelling choice for consumers looking to make the switch to a more environmentally friendly and efficient mode of transportation.\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# 1. Set up the model\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "# 2. Chain 1: Refine the user's question\n",
        "# The first chain helps refine the vague or broad question into something more specific.\n",
        "refine_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "You are an expert question creator. Given the user's question, refine it to be more specific and answerable.\n",
        "Only output the refined question.\n",
        "\n",
        "Original Question: {question}\n",
        "Refined Question:\"\"\")\n",
        "refiner_chain = refine_prompt | llm | StrOutputParser()\n",
        "\n",
        "# 3. Chain 2: Answer the refined question\n",
        "# The second chain answers the refined question generated by the first chain.\n",
        "answer_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "You are an expert on all topics. Answer the following question thoroughly.\n",
        "\n",
        "Question: {refined_question}\n",
        "Answer:\"\"\")\n",
        "answer_chain = answer_prompt | llm | StrOutputParser()\n",
        "\n",
        "# 4. Combine the chains\n",
        "# The output of the refine_chain is passed as 'refined_question' to the answer_chain.\n",
        "full_chain = {\"refined_question\": refiner_chain} | answer_chain\n",
        "\n",
        "# 5. Invoke the full chain with a vague question\n",
        "# Here, we invoke the full chain with a vague question (\"Tell me about cars\").\n",
        "vague_question = \"Tell me about cars.\"\n",
        "response = full_chain.invoke({\"question\": vague_question})\n",
        "\n",
        "# Print the response\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Explanation\n",
        "\n",
        "1.  The user's vague query (\"Tell me about cars.\") is first sent to the `refiner_chain`. This chain's LLM, guided by its prompt, might output a better question like, \"What are the key differences in engine technology between modern electric vehicles and traditional internal combustion engine cars?\"\n",
        "2.  LCEL then pipes this refined question as input to the `answer_chain`.\n",
        "3.  The final output is a detailed answer to the *specific, refined question*, not the original vague one. This results in a much more useful response.\n",
        "\n",
        "-----\n"
      ],
      "metadata": {
        "id": "QB4fJmOz28bN"
      },
      "id": "QB4fJmOz28bN"
    },
    {
      "cell_type": "markdown",
      "id": "ba1d137f-e1d9-4148-8284-94a2a27358fd",
      "metadata": {
        "id": "ba1d137f-e1d9-4148-8284-94a2a27358fd"
      },
      "source": [
        "### 3\\. Cognitive Verifier Pattern 🎯\n",
        "\n",
        "The **Cognitive Verifier Pattern** forces the model to \"show its work.\" Instead of just outputting an answer, it first generates the underlying reasoning, facts, or steps it used to arrive at the answer. This reduces the chances of hallucination and allows for verification.\n",
        "\n",
        "#### Concept\n",
        "\n",
        "The process is split into two parts:\n",
        "\n",
        "1.  **Generation:** The model answers the question but also provides its step-by-step reasoning.\n",
        "2.  **Verification/Extraction:** The model (or a second, simpler model/prompt) extracts the final answer from the detailed output.\n",
        "\n",
        "#### LangChain Implementation\n",
        "\n",
        "We create two prompts and chain them. The first generates a detailed thought process, and the second extracts the clean, final answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "21db936d-5516-4c08-8de3-554c4d0c09ba",
      "metadata": {
        "id": "21db936d-5516-4c08-8de3-554c4d0c09ba"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# 1. Set up the model\n",
        "llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
        "\n",
        "# 2. Prompt 1: Generate reasoning and the answer\n",
        "# This prompt generates both reasoning and a final answer.\n",
        "reasoning_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "You are a meticulous problem solver. Given a question, provide a step-by-step reasoning process and then state the final answer.\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Reasoning:\n",
        "[Provide your step-by-step thinking here]\n",
        "\n",
        "Final Answer:\n",
        "[State the final, concise answer here]\n",
        "\"\"\")\n",
        "\n",
        "# 3. Prompt 2: Extract only the final answer\n",
        "# This prompt extracts the final answer from the reasoning output.\n",
        "extractor_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "Extract the 'Final Answer' from the following text.\n",
        "\n",
        "{reasoning_output}\n",
        "\"\"\")\n",
        "\n",
        "# 4. Create the full chain\n",
        "# The first part generates the detailed output, the second part extracts the answer.\n",
        "cognitive_verifier_chain = (\n",
        "    reasoning_prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "full_chain = (\n",
        "    {\"reasoning_output\": cognitive_verifier_chain}\n",
        "    | extractor_prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "4d0b3e20-79d5-40f1-80a5-9a2e46cfecac",
      "metadata": {
        "id": "4d0b3e20-79d5-40f1-80a5-9a2e46cfecac",
        "outputId": "7d54ac27-b630-4c09-95e7-1137d7b375e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Final Answer ---\n",
            "The trains will meet at 11:48 AM.\n",
            "\n",
            "--- Full Reasoning Output ---\n",
            "Reasoning:\n",
            "\n",
            "Step 1: First, we need to determine how far the first train travels before the second train starts. Since the first train leaves an hour earlier and travels at 60 km/h, it will have traveled 60 km by the time the second train starts.\n",
            "\n",
            "Step 2: This means that when the second train starts, the two trains are 480 km - 60 km = 420 km apart.\n",
            "\n",
            "Step 3: Now, we need to determine how long it will take for the two trains to meet. Since they are traveling towards each other, we add their speeds together. So, they are closing the distance between them at a rate of 60 km/h + 90 km/h = 150 km/h.\n",
            "\n",
            "Step 4: To find out how long it will take for them to meet, we divide the distance they need to cover by their combined speed. So, 420 km ÷ 150 km/h = 2.8 hours.\n",
            "\n",
            "Step 5: Since the second train starts at 9:00 AM, they will meet at 9:00 AM + 2.8 hours = 11:48 AM.\n",
            "\n",
            "Final Answer:\n",
            "\n",
            "The two trains will meet at 11:48 AM.\n"
          ]
        }
      ],
      "source": [
        "# 5. Invoke with a multi-step question\n",
        "question = \"If a train leaves Station A at 8:00 AM traveling at 60 km/h, and a second train leaves Station B at 9:00 AM traveling at 90 km/h on the same track towards Station A, when will they meet if the stations are 480 km apart?\"\n",
        "final_answer = full_chain.invoke({\"question\": question})\n",
        "\n",
        "# Print the final answer\n",
        "print(\"--- Final Answer ---\")\n",
        "print(final_answer)\n",
        "\n",
        "# You can also run just the first part to see the reasoning\n",
        "reasoning = cognitive_verifier_chain.invoke({\"question\": question})\n",
        "print(\"\\n--- Full Reasoning Output ---\")\n",
        "print(reasoning)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5fd4430-45e6-4d7c-9b12-252e9431cb31",
      "metadata": {
        "id": "d5fd4430-45e6-4d7c-9b12-252e9431cb31"
      },
      "source": [
        "#### Explanation\n",
        "\n",
        "The `cognitive_verifier_chain` forces the model to first outline its entire logical process (calculating the head start, relative speed, etc.). The `full_chain` then uses an `extractor_prompt` to parse this detailed output and present only the clean, final answer (e.g., \"The trains will meet at 11:48 AM.\") to the user, while the full reasoning remains available for verification.\n",
        "\n",
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e4937ff-c5cf-49e9-b472-e5ff0a1b72a2",
      "metadata": {
        "id": "7e4937ff-c5cf-49e9-b472-e5ff0a1b72a2"
      },
      "source": [
        "### 4\\. Audience Pattern 👨‍👩‍👧‍👦\n",
        "\n",
        "The **Audience Pattern** involves explicitly telling the model who the intended audience is. This allows the model to tailor the complexity, tone, and vocabulary of its response to be appropriate for that specific audience.\n",
        "\n",
        "#### Concept\n",
        "\n",
        "You provide context about the end-user, such as \"explain this to a 5-year-old,\" \"write this for a team of software engineers,\" or \"compose this for a skeptical board of directors.\"\n",
        "\n",
        "#### LangChain Implementation\n",
        "\n",
        "This is easily done by adding an `{audience}` variable to the prompt template."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "c2bf8f9f-1e86-4748-a14a-0d10f88d405e",
      "metadata": {
        "id": "c2bf8f9f-1e86-4748-a14a-0d10f88d405e",
        "outputId": "3840cf9a-3b3f-4b95-aa73-5e4d90f763c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- For a a 5-year-old child ---\n",
            "Photosynthesis is how plants eat! But instead of using their mouths, plants use sunlight, water from the ground and air to make their own food. They use something green in their leaves called chlorophyll to capture sunlight. Once they catch the sunlight, they mix it with water and air to create food and give out fresh air for us to breathe. It's kind of like cooking but in a plant way!\n"
          ]
        }
      ],
      "source": [
        "# Import necessary modules from LangChain\n",
        "from langchain_openai import ChatOpenAI  # This imports the ChatOpenAI model, which we will use to interact with OpenAI's GPT model.\n",
        "from langchain_core.prompts import ChatPromptTemplate  # This imports the ChatPromptTemplate class to define custom prompts.\n",
        "from langchain_core.output_parsers import StrOutputParser  # This allows us to parse the output of the model.\n",
        "\n",
        "# Set up the language model\n",
        "llm = ChatOpenAI(model=\"gpt-4\")  # This initializes the GPT-4 model, setting up the language model for usage.\n",
        "\n",
        "# Define the prompt with an Audience placeholder\n",
        "# The {audience} is a placeholder that will allow us to dynamically change the target audience for the explanation.\n",
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"Explain the concept of '{concept}' to {audience}.\"  # The prompt instructs the model to explain a concept tailored to a specific audience.\n",
        ")\n",
        "\n",
        "# Create the chain\n",
        "chain = prompt | llm | StrOutputParser()  # This chain combines the prompt, the language model, and the output parser.\n",
        "\n",
        "# Invoke the chain for different audiences\n",
        "concept = \"photosynthesis\"  # Here, we're using the concept of \"photosynthesis\" to explain to different audiences.\n",
        "\n",
        "# Audience 1: A 5-year-old child\n",
        "audience_child = \"a 5-year-old child\"  # Defining the first audience as a child (for simplified explanation).\n",
        "response_child = chain.invoke({\"concept\": concept, \"audience\": audience_child})  # The model will generate a response suitable for a child.\n",
        "\n",
        "# Print the response for the 5-year-old audience\n",
        "print(f\"--- For a {audience_child} ---\")  # Print the heading to indicate the audience.\n",
        "print(response_child)  # Display the model's explanation tailored for a 5-year-old child."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "85ed3024-d447-4fb8-a360-0349ecf4e71c",
      "metadata": {
        "id": "85ed3024-d447-4fb8-a360-0349ecf4e71c",
        "outputId": "3b38d574-5045-47e9-ef34-d76f69630af1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- For a a biology undergraduate student ---\n",
            "Photosynthesis is a process used by plants, algae, and certain types of bacteria to harness energy from sunlight to convert water and carbon dioxide into glucose, which is a form of sugar that serves as a source of chemical energy. These organisms also produce a by-product of oxygen during photosynthesis.\n",
            "\n",
            "The process involves two stages. The initial phase, known as the light-dependent reactions or light reactions, occurs in the thylakoid membrane of the chloroplasts, where sunlight is converted into chemical energy, specifically in the form of ATP (Adenosine triphosphate) and NADPH (Nicotinamide adenine dinucleotide phosphate).\n",
            "\n",
            "The second phase, known as the light-independent reactions or the Calvin cycle, happens in the stroma of the chloroplasts. Here, the ATP and NADPH produced in the light-dependent reactions are used to make glucose from molecules of CO2 in a cycle of chemical reactions.\n",
            "\n",
            "Photosynthesis is fundamental to life on Earth, as it is the primary source of oxygen and it forms the basis of food chains. Moreover, it impacts the global carbon cycle and through that, the Earth's climate. Consequently, studying photosynthesis provides crucial insights into basic plant biology, global ecological patterns, and potential strategies for mitigating climate change.\n"
          ]
        }
      ],
      "source": [
        "# Audience 2: A biology undergraduate student\n",
        "audience_student = \"a biology undergraduate student\"  # Defining the second audience as a biology undergraduate student.\n",
        "response_student = chain.invoke({\"concept\": concept, \"audience\": audience_student})  # Invoking the chain with a more technical audience (biology student).\n",
        "\n",
        "# Print the response for the biology undergraduate audience\n",
        "print(f\"\\n--- For a {audience_student} ---\")  # Print the heading for the audience type.\n",
        "print(response_student)  # Display the explanation of photosynthesis tailored for a biology undergraduate student."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5917b35-b971-4171-8b10-3edc425ec0db",
      "metadata": {
        "id": "f5917b35-b971-4171-8b10-3edc425ec0db"
      },
      "source": [
        "#### Explanation\n",
        "\n",
        "By simply changing the `audience` variable, the chain produces dramatically different outputs for the same concept. For the child, it will use simple analogies like \"plants eating sunlight to make their food.\" For the student, it will use precise scientific terms like chlorophyll, chloroplasts, ATP, and NADPH. This makes the pattern incredibly powerful for creating context-aware content.\n",
        "\n",
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60486c89-05cf-43e9-8e48-db75933c00bb",
      "metadata": {
        "id": "60486c89-05cf-43e9-8e48-db75933c00bb"
      },
      "source": [
        "### Can this be done without LangChain? What's the benefit?\n",
        "\n",
        "Yes, absolutely. **All these patterns are fundamentally prompt engineering techniques.** You could implement them by manually formatting strings and making direct calls to the OpenAI API (or any other model provider)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "9fcbc0bb-3496-48eb-9939-c2c61c21d757",
      "metadata": {
        "id": "9fcbc0bb-3496-48eb-9939-c2c61c21d757",
        "outputId": "9e0a4024-b2b4-4e8b-a498-a40b880f8164",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A zero-day exploit is a type of cyberattack that takes advantage of a security vulnerability in software or hardware that is unknown to the party responsible for fixing the flaw, typically the software vendor or hardware manufacturer. The term \"zero-day\" refers to the fact that the developers have had zero days to fix the issue since they were unaware of it. It is considered particularly dangerous because the vulnerability can be exploited by attackers before a patch or mitigation becomes available, leaving systems defenseless against this specific type of attack.\n",
            "\n",
            "Zero-day exploits are often used in targeted attacks, such as corporate espionage or government surveillance, and can be sold on the black market for significant sums of money due to their potency and the advantage they provide to attackers. Once the vulnerability becomes known to the software vendor, they will typically rush to develop and distribute a patch to protect users, but until then, systems remain vulnerable.\n"
          ]
        }
      ],
      "source": [
        "# NON-LANGCHAIN EXAMPLE (Persona Pattern)\n",
        "import openai\n",
        "client = openai.OpenAI()\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model=\"gpt-4o\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a world-class cybersecurity expert...\"},\n",
        "    {\"role\": \"user\", \"content\": \"What is a 'zero-day exploit'?\"}\n",
        "  ]\n",
        ")\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd43d2af-b16b-4848-96c0-434b3e6f7419",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "cd43d2af-b16b-4848-96c0-434b3e6f7419"
      },
      "source": [
        "So, why use LangChain? The benefits become enormous as your application grows in complexity.\n",
        "\n",
        "1.  **Standardization and Reusability:** LangChain's `PromptTemplate` objects turn your prompts into reusable, standardized components. You can save them, share them, and version them easily.\n",
        "2.  **Composability (LCEL):** This is LangChain's superpower. As shown in the *Question Refinement* and *Cognitive Verifier* patterns, you can pipe (`|`) components together to create complex logic with very little code. Doing this with raw API calls would require messy, nested functions and manual state management.\n",
        "3.  **Model Agnosticism:** Your prompt templates and chains are not tied to OpenAI. You can easily swap `ChatOpenAI` for `ChatGoogleGenerativeAI` or `ChatAnthropic` with minimal code changes, allowing you to experiment with different models.\n",
        "4.  **Integration Ecosystem:** LangChain provides built-in tools for everything that comes *before* and *after* the prompt: document loaders to get data, text splitters, embedding models, vector stores for RAG, and output parsers to structure the final response.\n",
        "5.  **Observability and Debugging:** With a tool like **LangSmith**, you can trace the execution of every step in your chain, see the inputs and outputs, and debug complex interactions. This is nearly impossible to do with raw API calls.\n",
        "6.  **Productionization:** Tools like **LangServe** allow you to take any chain you build and instantly deploy it as a production-ready REST API.\n",
        "\n",
        "In short, while you *can* implement these patterns without LangChain, **LangChain provides the essential framework, structure, and tools to build, debug, and deploy them as robust, scalable applications.**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:langchain]",
      "language": "python",
      "name": "conda-env-langchain-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}