{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/k21academyuk/Agentic-AI/blob/main/Building_Simple_RAG_Using_Langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab: Building Simple RAG Using Langchain\n",
        "\n",
        "In this lab, we will build a **Retrieval-Augmented Generation (RAG)** system that combines information retrieval and text generation to process large documents, such as legal contracts. The RAG system leverages **FAISS** for efficient similarity search and **Hugging Face's generative models** (like GPT-2) for synthesizing human-like answers. By embedding the document content and indexing it with FAISS, we can quickly retrieve relevant sections of text and generate contextually accurate responses to specific queries.\n",
        "\n",
        "*The main objective of this system is to enable automated document analysis, allowing users to query large legal documents and get detailed, relevant, and concise answers without manually reading through the entire document.*\n",
        "\n",
        "This approach significantly improves efficiency for industries like law, where processing vast amounts of textual data is crucial."
      ],
      "metadata": {
        "id": "g9AxA1a9HXSs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Install Required Libraries\n",
        "\n",
        "In this step, we install the necessary libraries that will be used throughout the lab. **LangChain** is essential for orchestrating the RAG system, integrating document processing and retrieval with language models.\n",
        "\n",
        "**FAISS** is used to index and search the document embeddings efficiently, which allows us to quickly retrieve relevant content. **Transformers** provides access to pre-trained language models, and **Sentence-Transformers** allows us to generate meaningful sentence embeddings.\n",
        "\n",
        " These libraries are fundamental to building a scalable and efficient RAG system."
      ],
      "metadata": {
        "id": "FQB2XxQFejjG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85tgWgUCKOOR"
      },
      "outputs": [],
      "source": [
        "!pip install langchain\n",
        "!pip install faiss-cpu\n",
        "!pip install transformers\n",
        "!pip install sentence-transformers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Import Required Libraries\n",
        "\n",
        "1. The commands update **LangChain** to its latest version and install the **LangChain Community edition**, which provides additional features and integrations for building AI applications like document retrieval and text generation."
      ],
      "metadata": {
        "id": "lPL0Sqgmbyv8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade langchain\n",
        "!pip install langchain-community\n"
      ],
      "metadata": {
        "id": "rhd86jNwLGik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. This code imports libraries for building a **RAG** system, including **LangChain** for document retrieval and generation, FAISS for similarity search, and **SentenceTransformer** for generating embeddings. It also uses **TextLoader** to load documents and **RecursiveCharacterTextSplitter** to split large texts into chunks for efficient processing."
      ],
      "metadata": {
        "id": "5uwv8Jtfb0Jq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from sentence_transformers import SentenceTransformer\n"
      ],
      "metadata": {
        "id": "JRboMxIoLXxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. To verify that **LangChain** and its dependencies are installed correctly, you can run the below code. This will **print the version** of LangChain you have installed. Ensure it is up to date *(e.g., version 0.3.x or higher).*"
      ],
      "metadata": {
        "id": "IPGFcVNMf6oG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import langchain\n",
        "print(langchain.__version__)\n"
      ],
      "metadata": {
        "id": "oAz7w5g9LjNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Load and Preprocess Data (Documents)\n",
        "\n",
        "\n",
        "1. In this step, the **document** (such as a legal contract) is uploaded to Google Colab using the files.upload() function, which opens a dialog for selecting the file. This is essential to bring the document into the environment for further processing, and it ensures that the file is **accessible** for loading and manipulation.\n",
        "\n",
        "Here, we will assume you have a text file with your legal documents. If you Not then **Download the Document form Here:**\n",
        "\n",
        "[legal_documents.txt](https://github.com/k21academyuk/Agentic-AI/blob/main/legal_documents.txt)"
      ],
      "metadata": {
        "id": "HrVNVO-4gKBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()  # This will open the file upload dialog\n"
      ],
      "metadata": {
        "id": "LbVUmM0sMLyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. After uploading the file, you can **verify the file's existence** using the os module:\n"
      ],
      "metadata": {
        "id": "jpBCUUbbhR4A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.listdir())  # This will show the files in the current directory\n"
      ],
      "metadata": {
        "id": "Rm-X9JbbMVFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. The code uses ```TextLoader``` from LangChain to load the **legal document** ```(legal_documents.txt)```. The ```load()``` method reads the document and prepares it for further processing, such as splitting into chunks or generating embeddings. It is important to ensure the **file name** matches the uploaded document to avoid errors."
      ],
      "metadata": {
        "id": "Yu-_zIU5hVX8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loader = TextLoader(\"legal_documents.txt\")  # Make sure the file name matches the uploaded file\n",
        "documents = loader.load()\n"
      ],
      "metadata": {
        "id": "V44g43BVMW_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. After loading the document, we use the ```RecursiveCharacterTextSplitter``` to break the document into smaller, manageable chunks of text. Each chunk is typically sized at 1000 characters with an overlap of 200 characters. This step is important for **handling large documents effectively**, ensuring that chunks maintain context while being small enough for efficient processing, such as embedding generation and retrieval tasks."
      ],
      "metadata": {
        "id": "lsvBUJ_lhe43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load documents (text file in this case)\n",
        "loader = TextLoader(\"legal_documents.txt\")  # Replace with your actual file path\n",
        "documents = loader.load()\n",
        "\n",
        "# Split the documents into chunks to manage large documents\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "texts = text_splitter.split_documents(documents)\n"
      ],
      "metadata": {
        "id": "YzwvR3UxMfUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Generate Embeddings with Hugging Face\n",
        "\n",
        "In this step, we initialize a **Sentence Transformer model** from Hugging Face, specifically the ```all-MiniLM-L6-v2``` model, which is pre-trained to generate high-quality **sentence embeddings**. These embeddings are vector representations of text that capture the semantic meaning of each document chunk. The model is applied to each chunk of text from the previously split document, and the ```encode()``` method generates an embedding for each chunk. By creating embeddings, we can efficiently compare the semantic similarity between text chunks, which is crucial for **document retrieval** and generating contextually relevant answers to user queries."
      ],
      "metadata": {
        "id": "eTCe9YCGgPaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Hugging Face model (using a sentence transformer model)\n",
        "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Generate embeddings for each document chunk\n",
        "embeddings = [embedding_model.encode(text.page_content) for text in texts]  # Access page_content for text in documents\n",
        "\n",
        "# Now 'embeddings' will contain the embeddings for each chunk of text in the document\n"
      ],
      "metadata": {
        "id": "PSc1-OiRM-ge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Create FAISS Vector Store\n",
        "\n",
        "1. This step creates a **FAISS vector store** to store the embeddings generated from document chunks. FAISS enables efficient similarity search, allowing fast retrieval of relevant text chunks based on a query. The **HuggingFaceEmbeddings** class from LangChain generates these embeddings. However, there's a deprecation warning suggesting the use of the updated ```langchain_huggingface``` package for future compatibility."
      ],
      "metadata": {
        "id": "1CGk07rrgXbG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "# Create a FAISS vector store using the embeddings\n",
        "vector_store = FAISS.from_documents(texts, HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\"))\n"
      ],
      "metadata": {
        "id": "FLlEKS25NBlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. This step creates a **FAISS vector store** using the generated embeddings for efficient similarity search. It stores the embeddings and allows quick retrieval of relevant document chunks. The **FAISS index** is then saved locally with ```save_local()``` for future use, avoiding the need to regenerate the index every time."
      ],
      "metadata": {
        "id": "DppngXYhHK-0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a FAISS vector store using the generated embeddings\n",
        "vector_store = FAISS.from_documents(texts, HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\"))\n",
        "\n",
        "# Save the FAISS index (optional, for future use)\n",
        "vector_store.save_local(\"faiss_index\")\n"
      ],
      "metadata": {
        "id": "SWJsotl-NEog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: Set Up Retriever and Generative Model\n",
        "\n",
        "In this step, we set up the **retriever** to fetch the most relevant chunks of text from the **FAISS vector store** based on the user’s query. We also initialize a **text generation model** from **Hugging Face** (e.g., GPT-2) to synthesize answers based on the retrieved text. The **retriever** fetches the top **k** relevant document chunks, while the **generative model** generates a concise response based on those chunks. Combining these two components, the **RetrieverQA** chain enables the RAG system to retrieve context and generate a comprehensive answer, automating document analysis."
      ],
      "metadata": {
        "id": "c9m-xYA_gfL1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary dependencies\n",
        "!pip install transformers langchain faiss-cpu sentence-transformers\n",
        "\n",
        "# Import libraries\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from transformers import pipeline\n",
        "\n",
        "# Set up the retriever to fetch the top 2 most relevant chunks for each query\n",
        "retriever = vector_store.as_retriever(search_kwargs={\"k\": 2})\n",
        "\n",
        "# Use Hugging Face's GPT model for text generation\n",
        "generator = pipeline('text-generation', model='gpt2')  # You can change the model (e.g., GPT-Neo)\n",
        "\n",
        "# Wrap Hugging Face pipeline in LangChain's HuggingFacePipeline\n",
        "llm = HuggingFacePipeline(pipeline=generator)\n",
        "\n",
        "# Combine the retriever and LLM into a single chain\n",
        "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
        "\n",
        "# Query the system\n",
        "query = \"What are the termination clauses in this contract?\"\n",
        "result = qa.run(query)\n",
        "\n",
        "# Print the result (the answer generated by the system)\n",
        "print(result)\n"
      ],
      "metadata": {
        "id": "ECcI5zPLNi5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 7: Query the System\n",
        "\n",
        "In this step, we query the system (e.g., asking about termination clauses in a contract). The system retrieves relevant chunks of text and generates an answer using the pre-configured **retriever** and **generative model** (GPT-2). This step shows how the system can automatically generate responses based on specific queries.\n",
        "\n"
      ],
      "metadata": {
        "id": "3596YvuUg3vB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Query the system\n",
        "query = \"What are the termination clauses in this contract?\"\n",
        "result = qa.run(query)\n",
        "\n",
        "# Print the result (the answer generated by the system)\n",
        "print(result)\n"
      ],
      "metadata": {
        "id": "j8DlYURmN400"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 8: Evaluating RAG Performance Manually (Optional)\n",
        "\n",
        "Since **LangChain** has undergone several updates, we can manually evaluate the RAG system based on relevance, completeness, and conciseness.\n",
        "\n",
        "#### **1. Evaluate Relevance:**\n",
        "\n",
        "Relevance refers to how well the retrieved documents match the query. You can evaluate relevance by checking how closely the retrieved documents align with the context of the query."
      ],
      "metadata": {
        "id": "AsqHYvkZg_ej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the retrieved context for a query\n",
        "query = \"What are the termination clauses in this contract?\"\n",
        "context = qa.run(query)  # Get the generated answer\n",
        "\n",
        "# Evaluate relevance manually by checking if the context closely matches the expected answer\n",
        "# Example: You can look at the generated answer and verify if it contains relevant clauses from the document.\n",
        "print(\"Generated Answer: \", context)\n"
      ],
      "metadata": {
        "id": "cipDgiJeOaOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2. Evaluate Completeness**\n",
        "\n",
        "Completeness refers to whether the answer generated provides a full and detailed response to the question. You can manually assess whether the generated answer covers all aspects of the query."
      ],
      "metadata": {
        "id": "rhPLfuB7i19-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming the answer is stored in 'context', you can evaluate its completeness by checking:\n",
        "expected_clauses = [\"termination\", \"contract\", \"30 days' notice\", \"breach\"]\n",
        "completeness_score = 0\n",
        "\n",
        "# Check if the generated answer contains expected information (e.g., \"termination clauses\")\n",
        "for clause in expected_clauses:\n",
        "    if clause in context.lower():\n",
        "        completeness_score += 1\n",
        "\n",
        "print(f\"Completeness score: {completeness_score}/4\")\n"
      ],
      "metadata": {
        "id": "KgGxSqpBOwZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3. Evaluate Conciseness**\n",
        "\n",
        "Conciseness refers to whether the answer is clear and to the point, without unnecessary information. You can measure conciseness by reviewing the length of the generated answer and checking if the content is direct and relevant."
      ],
      "metadata": {
        "id": "JiPEsW4Vi5sR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate conciseness by checking if the answer is concise and to the point\n",
        "word_count = len(context.split())\n",
        "max_words = 150  # Define a reasonable word limit\n",
        "\n",
        "if word_count <= max_words:\n",
        "    conciseness_score = 1\n",
        "else:\n",
        "    conciseness_score = 0\n",
        "\n",
        "print(f\"Conciseness score: {conciseness_score}/1\")\n"
      ],
      "metadata": {
        "id": "uBl9_IRnO0Y_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Use Case:**\n",
        "\n",
        "A **law firm** needs an efficient solution to help lawyers quickly find **specific clauses** in lengthy **legal contracts**, such as **termination clauses**, **payment terms**, and **confidentiality agreements**. Manually reviewing contracts can be time-consuming and prone to human error, especially when the firm handles **hundreds of contracts** at once.\n",
        "\n",
        "In this scenario, the **RAG system** is implemented to assist lawyers by automatically retrieving the most relevant sections of text based on a query (e.g., **“What are the termination clauses in this contract?”**) and generating a concise, accurate response. This allows lawyers to save time and focus on the **legal interpretation** of the documents, rather than spending hours reading and searching through them manually.\n",
        "\n",
        "*The RAG system is designed to:*\n",
        "\n",
        "* Retrieve relevant document chunks based on a user's query.\n",
        "\n",
        "* Generate precise answers using context from the retrieved chunks.\n",
        "\n",
        "* Provide scalable and quick responses, even for large sets of documents.\n",
        "\n",
        "\n",
        "This system also helps the firm in terms of **cost efficiency** by automating manual document review and improving the overall **response time** to client queries, resulting in faster **turnaround times** and **better client service**."
      ],
      "metadata": {
        "id": "-q1S5VqFyFmT"
      }
    }
  ]
}