{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zASr5dp7u-V7"
   },
   "source": [
    "#Zero-shot Question Answering (QA) with HuggingFace Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gyVoeyXxvU3p"
   },
   "source": [
    "In this **comprehensive tutorial on running zero-shot Question Answering (QA)** with Hugging Face models, covering:\n",
    "\n",
    "* What is zero-shot QA?\n",
    "* How it differs from classic extractive QA\n",
    "* Pipelines and direct model usage\n",
    "* Best model options (including `facebook/bart-large-mnli`, NLI models, and zero-shot QG/QA)\n",
    "* End-to-end code with explanations\n",
    "* Bonus: Using Multilingual Zero-Shot QA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "InOCUvAnvXRI"
   },
   "source": [
    "## 1. **What is Zero-Shot QA?**\n",
    "\n",
    "* **Classic (Extractive) QA:** Given a **context** and a **question**, the model extracts the answer from the context. Needs the answer to be *explicitly present* in the text.\n",
    "* **Zero-Shot QA:** The model answers a question based on *world knowledge* or *generalization*, even if it hasn’t seen a similar Q\\&A pair during training, and often *without being fine-tuned for QA*.\n",
    "\n",
    "  * **Use Cases:** Open-domain QA, knowledge base QA, document search where context may not directly include the answer.\n",
    "  * **How?** Common method is to reframe QA as NLI (Natural Language Inference): treat question as hypothesis and context as premise, then check if the context entails the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OLVk1DPcva6N"
   },
   "source": [
    "## 2. **Approaches for Zero-Shot QA**\n",
    "\n",
    "## - **A. NLI-Based (Entailment) Approach**\n",
    "\n",
    "* **NLI (Natural Language Inference)** is a task in NLP that determines whether a hypothesis is true (entailed), false (contradicted), or uncertain (neutral) based on a given context.\n",
    "* Use a model like `facebook/bart-large-mnli` or `roberta-large-mnli`.\n",
    "* Check whether a candidate answer is supported (“entailed”) by the context.\n",
    "* **Workflow:**\n",
    "\n",
    "  1. Generate answer candidates (retrieval, text spans, or generation).\n",
    "  2. For each candidate, use NLI model to see if context entails the Q\\&A pair.#\n",
    "\n",
    "### - **B. Generative Zero-Shot QA**\n",
    "\n",
    "* Use models like `google/flan-t5-xxl`, `tiiuae/falcon-7b-instruct`, or `mistralai/Mistral-7B-Instruct` for direct question answering without fine-tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "co8LNIkJvkGT"
   },
   "source": [
    "## 3. **A. Zero-Shot QA using NLI Models (Pipeline)**\n",
    "\n",
    "#### **Example 1: QA with NLI as Zero-Shot Classifier**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 385,
     "referenced_widgets": [
      "c9d98440c2c342a6bc7278b7100f4c48",
      "6af1a56d3b854398ab8c4eb1df5fae5d",
      "e15c8db7422c49f8aaff6054ef467d54",
      "98daea11bb5b4d7a8d27da329b2369d4",
      "808a1fea072343458640958f91a3ce9a",
      "9e8c4b6884074168ac943cba1d2a5ed3",
      "e1958ec949a34afcb3e963ce06ba74b5",
      "629a7a6515ac4b3781c23102226f10e5",
      "d4e5afc75c754bdc9bc9a48a670c329d",
      "302165da3dde41cd92555baf84b33601",
      "9fe8f8018ba040b4989dc5f02d2dc4d4",
      "da1454dbbcd64f0b8d83f0163b549223",
      "758b862814b64e4d9b59ace49c20c5c3",
      "c39dc46d311d497b8e7a3a48493856ad",
      "e497befdca5642eeb894d4872234735b",
      "70895589b13b478caa3d00187cab21c4",
      "6e01f8aff4af41609ee6eac82ef6c36c",
      "e8938a48e3e34fbab463eb5c27ca0519",
      "09ee6a09e6584cdeb7849a7fd6d9b179",
      "b0dcc8678c0a4d6f8b584fab79d2296f",
      "3838fd806d31482db92ef114839ca97b",
      "c7e4ae637e0d4439acd74c8697da2353",
      "9c832a486e294f05a05b9244cf2c9539",
      "85c3215f49ca4954bee419d338fb173d",
      "8d188015b1c74845b807b6a4157b94f8",
      "3456239e9d0549ada62c1f679aedb1a0",
      "f1ff372e9d0f491dbbfc78e7ba71373f",
      "b0acda94381a43d8b53cadc6ea00eeec",
      "f1506302484a462aa01048daeffc4c95",
      "c9717383074b4b658fd822e1d3147ffa",
      "734a0bb64fe8407c86350040bbcc1c7a",
      "dec33cbe279340108d237af9b5adcecc",
      "e2cf201d93ae45bc99ba1e7bef4c5f9f",
      "61c0e1407fc94ae2987007c16eefc934",
      "49936e9d42aa4cc885ad7e37e4a1e507",
      "9c3825b64fb941fa90e84627eafe11ec",
      "1253090e651e4fbeaec5fab4c4ac9583",
      "c7e6702be9f041a2940bcb7c51879129",
      "33eda7aeb9114b82bddd8447a36f9987",
      "2eeeaabab6f84112b41ae3a744145e16",
      "541008defba5451b86c75b498b0d181b",
      "6be2c83aa5d84bf48ab15aad6f4b87da",
      "cc695f44bd714763bb8b7a952ea15964",
      "5aa558e4b757465e9b4ea5efc716c1e1",
      "ba5c277fa6e94b9fbb023fdffc6d8730",
      "d33464d9328d45ff8f40fb32294423ed",
      "b3036569244048598644d020b72a3ab9",
      "e6ebb93035644d83a2adcdebbe7e95e1",
      "99423d2670624ff28071de42db828100",
      "99cb452d68d541a58bfc53cbc2a47e90",
      "6b53cf0077c648d196b8d6d256701207",
      "3bcbded350894ae293a8c8d92e34e0d9",
      "054140819cc94ab9840e482263ea0b0f",
      "6235830f21334b7dacaf8a96ea5d177d",
      "c9bde03ab8d447b9b1459a123c1bbd44",
      "9c58765d32114123ac8acd62db8f6acb",
      "3a67f6379ca749288b9a7d7a53563a8b",
      "63dd997e0e5c4408aa0741ed5a7bb9f5",
      "a9d53186d0cd4a6d8e02884fb653344c",
      "684b302fd5d64b32ba21cc57eac78997",
      "46638d0775df4e58a05d970f38d9e9ae",
      "692bd612e9de4a73a1b4cf96312215f8",
      "61c8a36b7ca347869f9733b673915201",
      "746a6192bbf3458c9dc73d593cd8f72f",
      "9a350fc25f3e4a529c8428844f7a4c09",
      "0debd74402f54c5d982724916b455e31"
     ]
    },
    "id": "A9A9QD2buyme",
    "outputId": "8534e1aa-1529-4484-9234-37f21298acb2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9d98440c2c342a6bc7278b7100f4c48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da1454dbbcd64f0b8d83f0163b549223",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c832a486e294f05a05b9244cf2c9539",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61c0e1407fc94ae2987007c16eefc934",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba5c277fa6e94b9fbb023fdffc6d8730",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c58765d32114123ac8acd62db8f6acb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The Ganges flows through India.', 'The Ganges flows through Bangladesh.', 'The Ganges flows through Nepal.', 'The Ganges flows through China.']\n",
      "[0.964613676071167, 0.9313793778419495, 0.0002740573254413903, 0.00010426752851344645]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# BART-large MNLI is a strong zero-shot model for natural language inference tasks\n",
    "nli_pipeline = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "# Define the context, which provides information about the Ganges river\n",
    "context = \"The Ganges is a trans-boundary river of Asia which flows through India and Bangladesh.\"\n",
    "# Define the question for which we need to generate the answer\n",
    "question = \"Which countries does the Ganges flow through?\"\n",
    "\n",
    "# Candidate answers (these can be improved via retrieval/heuristics based on the context)\n",
    "candidate_answers = [\"India\", \"Bangladesh\", \"Nepal\", \"China\"]\n",
    "\n",
    "# We turn the question and each candidate answer into \"hypotheses\" for classification\n",
    "hypotheses = [f\"The Ganges flows through {ans}.\" for ans in candidate_answers]  # Creating hypotheses\n",
    "\n",
    "# Use the zero-shot classification pipeline to evaluate the hypotheses against the context\n",
    "result = nli_pipeline(\n",
    "    sequences=context,  # The context provided to the model\n",
    "    candidate_labels=hypotheses,  # The hypotheses created from candidate answers\n",
    "    multi_label=True  # Allowing for multiple valid labels (answers)\n",
    ")\n",
    "\n",
    "# Print the predicted labels (answers) that the model found most likely\n",
    "print(result[\"labels\"])\n",
    "\n",
    "# Print the confidence scores for each hypothesis (how strongly the model supports each answer)\n",
    "print(result[\"scores\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jU-42HGavvBH"
   },
   "source": [
    "This code uses a **Natural Language Inference (NLI)** approach to find answers to a question based on a given context. Specifically, it uses a zero-shot classification model `(facebook/bart-large-mnli)` to check which candidate answers are most likely correct according to the context.\n",
    "\n",
    "**Think of it as:**\n",
    "\n",
    "* Context: The information you already have.\n",
    "\n",
    "* Question: What you want to know.\n",
    "\n",
    "* Candidate answers: Possible answers you want the model to check.\n",
    "\n",
    "* NLI model: Decides if each candidate answer is supported by the context.\n",
    "\n",
    "### **Code Explanation:**\n",
    "\n",
    "* `pipeline` is a helper function from the transformers library.It simplifies using pre-trained models for tasks like question answering, text generation, or classification.\n",
    "\n",
    "* `\"zero-shot-classification\"`: Allows the model to classify text without training on specific labels.\n",
    "\n",
    "* `facebook/bart-large-mnli`: A strong model trained for Natural Language Inference, meaning it can check if a statement is true, false, or neutral based on a context.\n",
    "\n",
    "* NLI models need statements (hypotheses) to check, not just single words.\n",
    "\n",
    "* `sequences=context`: The text the model uses as reference.\n",
    "\n",
    "* `candidate_labels=hypotheses`: The statements (hypotheses) the model evaluates.\n",
    "\n",
    "* `multi_label=True`: Allows more than one answer to be correct at the same time.\n",
    "\n",
    "* The model gives a score for each hypothesis, showing how strongly the context supports it.\n",
    "\n",
    "### **Expected Output**:\n",
    "\n",
    "* The model predicts India and Bangladesh as the correct answers because the context explicitly mentions them.\n",
    "\n",
    "* Nepal and China get very low scores because the context does not support them.\n",
    "\n",
    "* The labels are ranked from most likely true to least likely true, and the scores show the confidence (0 to 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S6B4E9lpvyj-"
   },
   "source": [
    "## 4. **B. Zero-Shot QA with Generative LLMs**\n",
    "\n",
    "#### **Example 2: Using FLAN-T5 for Open QA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258,
     "referenced_widgets": [
      "c124f90cd08d45c6a71fd03fff28fba4",
      "27f3b0bc3aac477ebb1f1007c704a8d1",
      "9bfc9b5581104eeb833f07a8e99ff549",
      "72dd2de6d6fc4e008dbea513f9b6fa90",
      "2b52e7006906400e9e483a26a8aef501",
      "10a52d6e49654f4fa52920f7ba775789",
      "dc6f944fb41145d98e3648f9e81b95d8",
      "bf6e861be49e46878903b2260fa55a8b",
      "d43e15e0709248aa825d3c55bc8abfe3",
      "11b66831da264cc887cf0b99b2e33ff5",
      "250c64ec6593456aa2e1a09faeb0c64a",
      "5266c90ee9dd4f5bb03853313b6c03be",
      "08c52bf9f8ac4924b10b1209a20ab1f3",
      "7c72f021adc74f8b83fbe938c4430c86",
      "03851d0dd4f545d0ad54f9818051f48a",
      "e9f80389599641fb946d1f5066ba1823",
      "3ce8b5d9e39d41468c296270cf18d747",
      "743e75258971478da568f34b7a2cbbca",
      "69014c8603944e97a1436e4244dc0c92",
      "c330e5c750f04c8bb0c63b4d2119aa83",
      "15a52df9067648aa8fc70827a23e7a82",
      "474c053f6c1548b5b1532f221ad77832",
      "63adf5db834f425692cf857d2d26b351",
      "a449b1495833446fa5de02f9aee23c75",
      "bb243427184546fbaeeb74c6f49f2964",
      "7209b1abe7d84c28b8feae99a129229c",
      "4f0f079c9dcd4525b6d4bdd61c4415db",
      "196dd6af3631491392189785d5e9df47",
      "0b583fe36ad046e49a9f17de9ccfa90d",
      "31e69a3ce0f64114ba92d4ab571d184d",
      "81c4e63ba0594666aa94547895a1fa0f",
      "ce92d5a5bf8041eeab9306e27520954a",
      "297b2bd437a04530bde60c1e172d813a",
      "dd60b6b462314e95b70f58e456e529c9",
      "639e4935e9f545f6abfc1354859739e4",
      "20b4e220381b45138d53fa1a4af4ad1e",
      "72be9a01681d4fb1954038a8ac56a2f3",
      "f1cdec43cc7741ec9130a75bb053eb0f",
      "d84f2422fee544d48665a50a7a7362ec",
      "742ffe9618c84fc8844ac637059dda2c",
      "489f87053e7a48e292dc764707676f96",
      "946ac5dc85c045019f05c861c276ff7a",
      "cb69062d158543c9b327ecb371eb39c7",
      "bc9a44f0a54c4c8faa6a783ee069c77b",
      "16a2fbf14bfc4a7e8dee04684455af12",
      "cde168c0615a41c687d3a553f07b5592",
      "01d46c6cdec84fcb8162f734ba524dc4",
      "40296681f1e247fdb6805d4ee4177a35",
      "e3a5ddaf9e3a430a925d1e3d171d1632",
      "0dc8c6055e51412b960498213822c083",
      "456f79ca5618450dbed1bccd2e7bac75",
      "42313dadca414027b003f6f32b314342",
      "8aae27e7c1074fcca343f825b6627502",
      "b9b43f4399204177a56def48bab759ba",
      "9fb0b99a94194112a217c0aed0e9f604",
      "7749ce73082849798445a809d2c27c47",
      "9b5fa1d9059f42f0a0216f87d06f9708",
      "d8049c68610a40d4b61e6b437c84eaaa",
      "a4acbde1dfae46478c25a5457a601d06",
      "8b7d2df19e094c09a46cc72f08c16c06",
      "600d76c024354b30ba1c554c5753beb5",
      "45e22a88e48c477fb2734a7cddaebb7b",
      "f8005f0fe86543fda6754075da3c772f",
      "e9e55e65bf58481a8522c210d40b7bc4",
      "5b9b1ee2604943ec9f060e0609549f06",
      "6f48d10e69a64bb299b6198fa95898e5",
      "987708e47df74c8f8f26acd6a6006e8d",
      "8ad2e129142c47a381b8f6a18dd9b228",
      "805b8f42c98d4218ab3a003e43c1fbe9",
      "99fa1da33435472a978471997463b5f5",
      "4489fbf1145f42f9b28d4c13fbe5f914",
      "77d12731c2db462c891afe685e7f8b75",
      "be48b8088f58428c9b9d8e8c6e44b67b",
      "b612ef68c0e64952b8275d6e7eb189b3",
      "3952ba8357d74cf98b2f66c1de2a9458",
      "30b56b754de2407ea6546a968eb0c5c0",
      "719f35faa00c45abb07258e82bb67f1b"
     ]
    },
    "id": "zNcLuGZwv1sk",
    "outputId": "148112bc-3483-43af-c0c6-9c51d1014aea"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c124f90cd08d45c6a71fd03fff28fba4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5266c90ee9dd4f5bb03853313b6c03be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63adf5db834f425692cf857d2d26b351",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd60b6b462314e95b70f58e456e529c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16a2fbf14bfc4a7e8dee04684455af12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7749ce73082849798445a809d2c27c47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "987708e47df74c8f8f26acd6a6006e8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "India and Bangladesh\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Load the tokenizer and model for FLAN-T5 (can be changed to other models like flan-t5-xl, Mistral-7B, etc.)\n",
    "model_name = \"google/flan-t5-large\"  # Or \"flan-t5-xl\", \"mistralai/Mistral-7B-Instruct\", etc.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)  # Initialize the tokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)  # Initialize the pre-trained model\n",
    "\n",
    "# Define the question and context\n",
    "question = \"Which countries does the Ganges flow through?\"  # The question to be answered\n",
    "context = \"The Ganges is a trans-boundary river of Asia which flows through India and Bangladesh.\"  # The context providing information about the Ganges\n",
    "\n",
    "# Format the input prompt by combining the question and context\n",
    "prompt = f\"Question: {question}\\nContext: {context}\\nAnswer:\"  # The formatted prompt for the model\n",
    "\n",
    "# Tokenize the input prompt to prepare it for the model\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")  # Convert the prompt into token format suitable for the model\n",
    "\n",
    "# Generate an answer from the model based on the tokenized input\n",
    "outputs = model.generate(**inputs, max_new_tokens=32)  # Generate an answer with a limit of 32 new tokens\n",
    "\n",
    "# Decode and print the generated answer (removes special tokens used by the model)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))  # Convert token IDs back to human-readable text and print the answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YTlmqwwFv5D-"
   },
   "source": [
    "* **For true open-domain QA,** just use the question (no context) and see how the model generalizes.\n",
    "\n",
    "**Flan-T5** is a variant of the T5 (Text-to-Text Transfer Transformer) model, developed by Google. The \"Flan\" part refers to the fine-tuning methodology used on T5, where the model is trained on a broad set of instruction-based datasets. This fine-tuning process makes Flan-T5 more effective at following instructions, such as answering questions, completing tasks, or summarizing information.\n",
    "\n",
    "### **Code Explanation**:\n",
    "\n",
    "* `AutoTokenizer`: Converts text into tokens (numbers) that the model understands.\n",
    "\n",
    "* `AutoModelForSeq2SeqLM`: Loads a sequence-to-sequence model, which can generate text as output.\n",
    "\n",
    "* `model_name`: Specifies the pre-trained model to use (FLAN-T5-large).\n",
    "\n",
    "* `tokenizer`: Prepares text for the model (splits it into tokens).\n",
    "\n",
    "* `model`: Loads the pre-trained FLAN-T5 model for text generation.\n",
    "\n",
    "* `return_tensors=\"pt\"*: Prepares tokens as PyTorch tensors, which the model uses internally. Converts text into numbers (tokens) that the model can process.\n",
    "\n",
    "* `max_new_tokens=32`: Limits the length of the answer to 32 tokens. The model predicts the answer based on the tokens.\n",
    "\n",
    "* `skip_special_tokens=True`: Removes extra symbols used internally by the model. Converts the model's token output back into human-readable text.\n",
    "\n",
    "### **Expected Output:**\n",
    "\n",
    "* This code automatically answers questions based on given text.\n",
    "\n",
    "* It uses FLAN-T5, a pre-trained model that understands context and generates answers.\n",
    "\n",
    " * Useful when you want AI to provide answers without manually programming the logic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dpev7vY0v7OV"
   },
   "source": [
    "## 5. **C. End-to-End Zero-Shot QA Pipeline (Custom Function)**\n",
    "\n",
    "Suppose you want to get an answer **and** check with NLI if it’s supported by the context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BOwePwHyv-OA",
    "outputId": "0560ba48-a413-4675-c7ee-69ed8b003dc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated answer: India and Bangladesh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entailment score: 0.9881207942962646\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Step 1: Generate an answer using FLAN-T5 (or similar models like Mistral)\n",
    "qa_model = \"google/flan-t5-large\"  # The model name for FLAN-T5 large version\n",
    "tokenizer = AutoTokenizer.from_pretrained(qa_model)  # Load the tokenizer for the model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(qa_model)  # Load the pre-trained FLAN-T5 model\n",
    "\n",
    "# Define the question and context for the model\n",
    "question = \"Which countries does the Ganges flow through?\"  # The question to be answered\n",
    "context = \"The Ganges is a trans-boundary river of Asia which flows through India and Bangladesh.\"  # The context providing information\n",
    "\n",
    "# Create the input prompt by combining the question and context\n",
    "prompt = f\"Question: {question}\\nContext: {context}\\nAnswer:\"  # Formatting the input for the model\n",
    "\n",
    "# Tokenize the input prompt to prepare it for the model\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")  # Convert the prompt into tensor format suitable for the model\n",
    "\n",
    "# Generate the answer from the model based on the tokenized input\n",
    "outputs = model.generate(**inputs, max_new_tokens=32)  # Limit the output to 32 new tokens for the answer\n",
    "\n",
    "# Decode the generated output (token IDs) into human-readable text\n",
    "answer = tokenizer.decode(outputs[0], skip_special_tokens=True)  # Remove special tokens used by the model\n",
    "\n",
    "# Print the generated answer\n",
    "print(\"Generated answer:\", answer)  # Output the generated answer\n",
    "\n",
    "# Step 2: Check if the generated answer is entailed by the context using NLI (Natural Language Inference)\n",
    "nli = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")  # Load the NLI pipeline with BART-large-MNLI\n",
    "hypothesis = f\"{answer}\"  # The hypothesis is the generated answer (e.g., \"India and Bangladesh.\")\n",
    "\n",
    "# Run zero-shot classification to check if the answer is supported by the context\n",
    "result = nli(sequences=context, candidate_labels=[hypothesis])  # Evaluate the hypothesis against the context\n",
    "\n",
    "# Print the entailment score, which indicates how much the hypothesis aligns with the context\n",
    "print(\"Entailment score:\", result[\"scores\"][0])  # Output the entailment score (confidence level)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k_EuupVeQ0ze"
   },
   "source": [
    "This code performs two tasks:\n",
    "\n",
    " 1. **Generate an Answer (using FLAN-T5):**\n",
    "\n",
    "* It combines a question and context, feeds them into the FLAN-T5 model to generate an answer.\n",
    "\n",
    "* Example: For the question \"Which countries does the Ganges flow through?\" and the context \"The Ganges flows through India and Bangladesh\", it generates: \"India and Bangladesh\".\n",
    "\n",
    "2. **Check Entailment (using BART-large-MNLI):**\n",
    "\n",
    "* It checks if the generated answer is supported by the context using a zero-shot classification model.\n",
    "\n",
    "* The entailment score indicates how confident the model is that the answer is true based on the context.\n",
    "\n",
    "| **Function / Method**                          | **Purpose**                                                                                 |\n",
    "|:---------------------------------------------|:-------------------------------------------------------------------------------------------|\n",
    "| `AutoTokenizer.from_pretrained()`            | Loads the tokenizer for the model to convert text into tokens (numerical representations). |\n",
    "| `AutoModelForSeq2SeqLM.from_pretrained()`    | Loads the pre-trained sequence-to-sequence language model used to generate answers.        |\n",
    "| `tokenizer()`                                | Processes and converts input text into tokens that the model can understand.                |\n",
    "| `model.generate()`                           | Generates a sequence of tokens (the answer) based on the processed input tokens.           |\n",
    "| `tokenizer.decode()`                         | Converts the generated tokens from the model back into human-readable text.                |\n",
    "| `pipeline()`                                 | A high-level utility from Hugging Face to easily load models for specific tasks.           |\n",
    "| `nli()` (as part of a pipeline)              | Uses Natural Language Inference to check if an answer is supported by context.             |\n",
    "\n",
    "### **Expected Output:**\n",
    "\n",
    "* `Generated answer`: The model generates the answer based on the given context.\n",
    "\n",
    "* `Entailment score`: The NLI model provides a high score (e.g., 0.99) indicating that the hypothesis (\"India and Bangladesh\") is strongly supported by the context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3qd5Tv9UwBuZ"
   },
   "source": [
    "## 6. **D. Multilingual Zero-Shot QA**\n",
    "\n",
    "Use models like `joeddav/xlm-roberta-large-xnli` for cross-lingual entailment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284,
     "referenced_widgets": [
      "97ab3d00253649daab6b306299e337b1",
      "17ebf806a02c4571948bd2ddc386aed6",
      "751de3cd49aa44e8988049f9abe4d15e",
      "8e5ea32833b949d591bef54d07464487",
      "3a0a61dbd13548ceb33731613a359d77",
      "bdc1163fdbe242af9b80a08ef992300c",
      "62e50cf5ec7f47d899354741bb5c75c0",
      "be95459918834e95aebc2500c8c730fa",
      "4675aa051c71480db9ab51f8d2e5fce0",
      "7c209e91d40a46a7bd54514bc3c6f58b",
      "68e9bd715eb8481bbbc972cf22e5938b",
      "5c78c315c24e4059bec6879ee7a2a721",
      "e67aec8f7e79498d8b8b61a293d2e8d2",
      "6a72b5bb559a4addb7a328980f65e1b3",
      "1b5afe25f2d34b24ababc6dd857d2539",
      "9cc4424351c44bc48fbe2ee4ae9cd7a3",
      "c428703435e941ce811b6323f585b87b",
      "7b1e6093f0bc421e958f1d00751a7056",
      "98bd3ae39c7441c285c3653d88f0bb1d",
      "8444b2da7d834db4b63b5ae52626ffae",
      "94a4d37a55d54b1894dc277e8f9e35f1",
      "5ea3a6d2f5ff42718b33cdc30e3133b5",
      "11d7606fc4954db2ad90e61e57a23e6c",
      "367ed19ae1594932acf806cb7e17e61e",
      "4d9142cdc2b6479c8a68803a2d15038f",
      "85f8548cb2a647a8be18cc67e3a3a87c",
      "c56ddda0638c49a48ae1729b378f1bc2",
      "657b9e1860f541a38dfbba42dcd0429e",
      "bddac92aa80541ed8fdb477f15a3a24a",
      "4f989aaeadd54dda9426f23dc132df2e",
      "1a48a51d52604d04bf07f28d94f4725f",
      "e7ce6c6ff12a48418522a54f1cdc3b6c",
      "dfd84449702043b187bb678359eb57e2",
      "9d7b4b22bd3742a58747845da612b5e4",
      "eb2a80c036a54b938815a3c7273e8f6d",
      "0729d78a553a4f238bc916dd786c47a6",
      "79cfdb904779494da1b33fa4e2abb4e5",
      "4f173cc9fd8142bea24c232c5c7b4598",
      "723e204d80aa4360b3d83c7ff321ebc5",
      "6f9016934ee34069a3f02afe37fa0103",
      "0b5c93ba9018488fadd97d59f0b8f024",
      "0961fa8fb10a4825935fbe9854aa6f6c",
      "b74072d926fb40b3b11688323503d3dd",
      "0a89efe207d54b5abb695f87662ec6bc",
      "1edb94d56cdf40488215772210856f74",
      "e9aa37cb865d4ec7a65b40bfe0909805",
      "7e918d8d1e2742a1bacf12d4de74c416",
      "f0eca67b5ee945b0a1dff023d33ce6c2",
      "a80dba67d58d4fc0acf9fdbd3b51e035",
      "00cd0182e59f4d64b807087bd27ecdb7",
      "93e07676255a4ecbb6d7f90b59fa46ef",
      "cbcc1b7537d445c5ba8e0be038a078f0",
      "574096199fb54fc5a5ac926de20adedc",
      "5b9d2d85cb3740ec9728a39b5b258480",
      "b4301e0ecc614e04b329736b1cfd062b"
     ]
    },
    "id": "2cu9ySkHwEu3",
    "outputId": "ab4dc603-0daa-4f59-8f0e-a834b82ad7d8"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97ab3d00253649daab6b306299e337b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/734 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c78c315c24e4059bec6879ee7a2a721",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at joeddav/xlm-roberta-large-xnli were not used when initializing XLMRobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11d7606fc4954db2ad90e61e57a23e6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d7b4b22bd3742a58747845da612b5e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1edb94d56cdf40488215772210856f74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sequence': 'La Seine est un fleuve français, long de 777 kilomètres.', 'labels': ['La Seine traverse Paris.'], 'scores': [0.70330810546875]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the zero-shot classification pipeline with the XLM-RoBERTa-large-XNLI model for multilingual tasks\n",
    "nli = pipeline(\"zero-shot-classification\", model=\"joeddav/xlm-roberta-large-xnli\")\n",
    "\n",
    "# Define the context in French, describing the Seine river\n",
    "context = \"La Seine est un fleuve français, long de 777 kilomètres.\"  # The context information about the Seine river\n",
    "\n",
    "# Define the question related to the context\n",
    "question = \"Quel est le fleuve qui traverse Paris ?\"  # The question asking about the river that crosses Paris\n",
    "\n",
    "# Define the hypothesis which we want to check against the context\n",
    "hypothesis = \"La Seine traverse Paris.\"  # Hypothesis statement asserting that the Seine crosses Paris\n",
    "\n",
    "# Use the NLI model to check if the hypothesis is entailed by the context\n",
    "result = nli(sequences=context, candidate_labels=[hypothesis])  # Run the zero-shot classification\n",
    "\n",
    "# Print the result, which includes the labels (hypothesis) and the corresponding entailment scores\n",
    "print(result)  # Output the result showing the labels and their respective scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z6_kyQgIRVFI"
   },
   "source": [
    "This code uses the **XLM-RoBERTa-large-XNLI model** for zero-shot classification to check if a hypothesis is supported by a given context.\n",
    "\n",
    "**XLM-RoBERTa-large-XNLI** is a cross-lingual model fine-tuned for zero-shot classification tasks, capable of understanding and classifying text in multiple languages.\n",
    "\n",
    "### **Code Explanation**\n",
    "\n",
    "* **zero-shot-classification**: Specifies the task the model will perform. In this case, it classifies text without any prior training on the specific labels.\n",
    "\n",
    "* `\"joeddav/xlm-roberta-large-xnli\"`: Specifies the XLM-RoBERTa-large-XNLI model, which is trained for multilingual tasks (XNLI stands for Cross-lingual Natural Language Inference). It can be used to check if a hypothesis holds true for a given context across various languages.\n",
    "\n",
    "* `hypothesis`: The statement being tested to see if it is true based on the provided context. Here, the hypothesis asserts that the Seine crosses Paris.\n",
    "\n",
    "* `sequences=context`: The context to evaluate the hypothesis against.\n",
    "\n",
    "* ``candidate_labels=[hypothesis]``: The hypothesis that needs to be checked.\n",
    "\n",
    "* `nli()`: The zero-shot classification model runs inference to see if the hypothesis is entailed (i.e., supported) by the context.\n",
    "\n",
    "### **Expected Output:**\n",
    "\n",
    "The expected output will show the hypothesis (\"La Seine traverse Paris\") along with a high confidence score, as the context directly mentions that the Seine is a French river, and it crosses Paris.\n",
    "\n",
    "* `labels`: The hypothesis that was evaluated. In this case, the hypothesis is that \"La Seine traverse Paris.\"\n",
    "\n",
    "* `scores`: The confidence score (between 0 and 1). A higher score (e.g., 0.99) means the model strongly supports the hypothesis based on the context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hCqmfEFZwHRB"
   },
   "source": [
    "## 7. **E. Model Options for Zero-Shot QA**\n",
    "\n",
    "| Approach         | Model Examples                                      | Hugging Face Model Card                                                         |\n",
    "| ---------------- | --------------------------------------------------- | ------------------------------------------------------------------------------- |\n",
    "| NLI (Entailment) | facebook/bart-large-mnli, roberta-large-mnli        | [bart-large-mnli](https://huggingface.co/facebook/bart-large-mnli)              |\n",
    "| Generative QA    | google/flan-t5-large, mistralai/Mistral-7B-Instruct | [flan-t5-large](https://huggingface.co/google/flan-t5-large)                    |\n",
    "| Multilingual NLI | joeddav/xlm-roberta-large-xnli                      | [xlm-roberta-large-xnli](https://huggingface.co/joeddav/xlm-roberta-large-xnli) |\n",
    "| Classic QA       | deepset/roberta-base-squad2 (not zero-shot)         | [squad2](https://huggingface.co/deepset/roberta-base-squad2)                    |\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
