{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Zero-shot Question Answering (QA) with HuggingFace Models"
      ],
      "metadata": {
        "id": "zASr5dp7u-V7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this **comprehensive tutorial on running zero-shot Question Answering (QA)** with Hugging Face models, covering:\n",
        "\n",
        "* What is zero-shot QA?\n",
        "* How it differs from classic extractive QA\n",
        "* Pipelines and direct model usage\n",
        "* Best model options (including `facebook/bart-large-mnli`, NLI models, and zero-shot QG/QA)\n",
        "* End-to-end code with explanations\n",
        "* Bonus: Using Multilingual Zero-Shot QA"
      ],
      "metadata": {
        "id": "gyVoeyXxvU3p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. **What is Zero-Shot QA?**\n",
        "\n",
        "* **Classic (Extractive) QA:** Given a **context** and a **question**, the model extracts the answer from the context. Needs the answer to be *explicitly present* in the text.\n",
        "* **Zero-Shot QA:** The model answers a question based on *world knowledge* or *generalization*, even if it hasn’t seen a similar Q\\&A pair during training, and often *without being fine-tuned for QA*.\n",
        "\n",
        "  * **Use Cases:** Open-domain QA, knowledge base QA, document search where context may not directly include the answer.\n",
        "  * **How?** Common method is to reframe QA as NLI (Natural Language Inference): treat question as hypothesis and context as premise, then check if the context entails the answer."
      ],
      "metadata": {
        "id": "InOCUvAnvXRI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. **Approaches for Zero-Shot QA**\n",
        "\n",
        "## - **A. NLI-Based (Entailment) Approach**\n",
        "\n",
        "* **NLI (Natural Language Inference)** is a task in NLP that determines whether a hypothesis is true (entailed), false (contradicted), or uncertain (neutral) based on a given context.\n",
        "* Use a model like `facebook/bart-large-mnli` or `roberta-large-mnli`.\n",
        "* Check whether a candidate answer is supported (“entailed”) by the context.\n",
        "* **Workflow:**\n",
        "\n",
        "  1. Generate answer candidates (retrieval, text spans, or generation).\n",
        "  2. For each candidate, use NLI model to see if context entails the Q\\&A pair.#\n",
        "\n",
        "### - **B. Generative Zero-Shot QA**\n",
        "\n",
        "* Use models like `google/flan-t5-xxl`, `tiiuae/falcon-7b-instruct`, or `mistralai/Mistral-7B-Instruct` for direct question answering without fine-tuning.\n"
      ],
      "metadata": {
        "id": "OLVk1DPcva6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. **A. Zero-Shot QA using NLI Models (Pipeline)**\n",
        "\n",
        "#### **Example 1: QA with NLI as Zero-Shot Classifier**\n"
      ],
      "metadata": {
        "id": "co8LNIkJvkGT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9A9QD2buyme"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# BART-large MNLI is a strong zero-shot model for natural language inference tasks\n",
        "nli_pipeline = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "\n",
        "# Define the context, which provides information about the Ganges river\n",
        "context = \"The Ganges is a trans-boundary river of Asia which flows through India and Bangladesh.\"\n",
        "# Define the question for which we need to generate the answer\n",
        "question = \"Which countries does the Ganges flow through?\"\n",
        "\n",
        "# Candidate answers (these can be improved via retrieval/heuristics based on the context)\n",
        "candidate_answers = [\"India\", \"Bangladesh\", \"Nepal\", \"China\"]\n",
        "\n",
        "# We turn the question and each candidate answer into \"hypotheses\" for classification\n",
        "hypotheses = [f\"The Ganges flows through {ans}.\" for ans in candidate_answers]  # Creating hypotheses\n",
        "\n",
        "# Use the zero-shot classification pipeline to evaluate the hypotheses against the context\n",
        "result = nli_pipeline(\n",
        "    sequences=context,  # The context provided to the model\n",
        "    candidate_labels=hypotheses,  # The hypotheses created from candidate answers\n",
        "    multi_label=True  # Allowing for multiple valid labels (answers)\n",
        ")\n",
        "\n",
        "# Print the predicted labels (answers) that the model found most likely\n",
        "print(result[\"labels\"])\n",
        "\n",
        "# Print the confidence scores for each hypothesis (how strongly the model supports each answer)\n",
        "print(result[\"scores\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code uses a **Natural Language Inference (NLI)** approach to find answers to a question based on a given context. Specifically, it uses a zero-shot classification model `(facebook/bart-large-mnli)` to check which candidate answers are most likely correct according to the context.\n",
        "\n",
        "**Think of it as:**\n",
        "\n",
        "* Context: The information you already have.\n",
        "\n",
        "* Question: What you want to know.\n",
        "\n",
        "* Candidate answers: Possible answers you want the model to check.\n",
        "\n",
        "* NLI model: Decides if each candidate answer is supported by the context.\n",
        "\n",
        "### **Code Explanation:**\n",
        "\n",
        "* `pipeline` is a helper function from the transformers library.It simplifies using pre-trained models for tasks like question answering, text generation, or classification.\n",
        "\n",
        "* `\"zero-shot-classification\"`: Allows the model to classify text without training on specific labels.\n",
        "\n",
        "* `facebook/bart-large-mnli`: A strong model trained for Natural Language Inference, meaning it can check if a statement is true, false, or neutral based on a context.\n",
        "\n",
        "* NLI models need statements (hypotheses) to check, not just single words.\n",
        "\n",
        "* `sequences=context`: The text the model uses as reference.\n",
        "\n",
        "* `candidate_labels=hypotheses`: The statements (hypotheses) the model evaluates.\n",
        "\n",
        "* `multi_label=True`: Allows more than one answer to be correct at the same time.\n",
        "\n",
        "* The model gives a score for each hypothesis, showing how strongly the context supports it.\n",
        "\n",
        "### **Expected Output**:\n",
        "\n",
        "* The model predicts India and Bangladesh as the correct answers because the context explicitly mentions them.\n",
        "\n",
        "* Nepal and China get very low scores because the context does not support them.\n",
        "\n",
        "* The labels are ranked from most likely true to least likely true, and the scores show the confidence (0 to 1)."
      ],
      "metadata": {
        "id": "jU-42HGavvBH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. **B. Zero-Shot QA with Generative LLMs**\n",
        "\n",
        "#### **Example 2: Using FLAN-T5 for Open QA**"
      ],
      "metadata": {
        "id": "S6B4E9lpvyj-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "# Load the tokenizer and model for FLAN-T5 (can be changed to other models like flan-t5-xl, Mistral-7B, etc.)\n",
        "model_name = \"google/flan-t5-large\"  # Or \"flan-t5-xl\", \"mistralai/Mistral-7B-Instruct\", etc.\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)  # Initialize the tokenizer\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)  # Initialize the pre-trained model\n",
        "\n",
        "# Define the question and context\n",
        "question = \"Which countries does the Ganges flow through?\"  # The question to be answered\n",
        "context = \"The Ganges is a trans-boundary river of Asia which flows through India and Bangladesh.\"  # The context providing information about the Ganges\n",
        "\n",
        "# Format the input prompt by combining the question and context\n",
        "prompt = f\"Question: {question}\\nContext: {context}\\nAnswer:\"  # The formatted prompt for the model\n",
        "\n",
        "# Tokenize the input prompt to prepare it for the model\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")  # Convert the prompt into token format suitable for the model\n",
        "\n",
        "# Generate an answer from the model based on the tokenized input\n",
        "outputs = model.generate(**inputs, max_new_tokens=32)  # Generate an answer with a limit of 32 new tokens\n",
        "\n",
        "# Decode and print the generated answer (removes special tokens used by the model)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))  # Convert token IDs back to human-readable text and print the answer\n"
      ],
      "metadata": {
        "id": "zNcLuGZwv1sk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **For true open-domain QA,** just use the question (no context) and see how the model generalizes.\n",
        "\n",
        "**Flan-T5** is a variant of the T5 (Text-to-Text Transfer Transformer) model, developed by Google. The \"Flan\" part refers to the fine-tuning methodology used on T5, where the model is trained on a broad set of instruction-based datasets. This fine-tuning process makes Flan-T5 more effective at following instructions, such as answering questions, completing tasks, or summarizing information.\n",
        "\n",
        "### **Code Explanation**:\n",
        "\n",
        "* `AutoTokenizer`: Converts text into tokens (numbers) that the model understands.\n",
        "\n",
        "* `AutoModelForSeq2SeqLM`: Loads a sequence-to-sequence model, which can generate text as output.\n",
        "\n",
        "* `model_name`: Specifies the pre-trained model to use (FLAN-T5-large).\n",
        "\n",
        "* `tokenizer`: Prepares text for the model (splits it into tokens).\n",
        "\n",
        "* `model`: Loads the pre-trained FLAN-T5 model for text generation.\n",
        "\n",
        "* `return_tensors=\"pt\"*: Prepares tokens as PyTorch tensors, which the model uses internally. Converts text into numbers (tokens) that the model can process.\n",
        "\n",
        "* `max_new_tokens=32`: Limits the length of the answer to 32 tokens. The model predicts the answer based on the tokens.\n",
        "\n",
        "* `skip_special_tokens=True`: Removes extra symbols used internally by the model. Converts the model's token output back into human-readable text.\n",
        "\n",
        "### **Expected Output:**\n",
        "\n",
        "* This code automatically answers questions based on given text.\n",
        "\n",
        "* It uses FLAN-T5, a pre-trained model that understands context and generates answers.\n",
        "\n",
        " * Useful when you want AI to provide answers without manually programming the logic."
      ],
      "metadata": {
        "id": "YTlmqwwFv5D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. **C. End-to-End Zero-Shot QA Pipeline (Custom Function)**\n",
        "\n",
        "Suppose you want to get an answer **and** check with NLI if it’s supported by the context:"
      ],
      "metadata": {
        "id": "dpev7vY0v7OV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "# Step 1: Generate an answer using FLAN-T5 (or similar models like Mistral)\n",
        "qa_model = \"google/flan-t5-large\"  # The model name for FLAN-T5 large version\n",
        "tokenizer = AutoTokenizer.from_pretrained(qa_model)  # Load the tokenizer for the model\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(qa_model)  # Load the pre-trained FLAN-T5 model\n",
        "\n",
        "# Define the question and context for the model\n",
        "question = \"Which countries does the Ganges flow through?\"  # The question to be answered\n",
        "context = \"The Ganges is a trans-boundary river of Asia which flows through India and Bangladesh.\"  # The context providing information\n",
        "\n",
        "# Create the input prompt by combining the question and context\n",
        "prompt = f\"Question: {question}\\nContext: {context}\\nAnswer:\"  # Formatting the input for the model\n",
        "\n",
        "# Tokenize the input prompt to prepare it for the model\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")  # Convert the prompt into tensor format suitable for the model\n",
        "\n",
        "# Generate the answer from the model based on the tokenized input\n",
        "outputs = model.generate(**inputs, max_new_tokens=32)  # Limit the output to 32 new tokens for the answer\n",
        "\n",
        "# Decode the generated output (token IDs) into human-readable text\n",
        "answer = tokenizer.decode(outputs[0], skip_special_tokens=True)  # Remove special tokens used by the model\n",
        "\n",
        "# Print the generated answer\n",
        "print(\"Generated answer:\", answer)  # Output the generated answer\n",
        "\n",
        "# Step 2: Check if the generated answer is entailed by the context using NLI (Natural Language Inference)\n",
        "nli = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")  # Load the NLI pipeline with BART-large-MNLI\n",
        "hypothesis = f\"{answer}\"  # The hypothesis is the generated answer (e.g., \"India and Bangladesh.\")\n",
        "\n",
        "# Run zero-shot classification to check if the answer is supported by the context\n",
        "result = nli(sequences=context, candidate_labels=[hypothesis])  # Evaluate the hypothesis against the context\n",
        "\n",
        "# Print the entailment score, which indicates how much the hypothesis aligns with the context\n",
        "print(\"Entailment score:\", result[\"scores\"][0])  # Output the entailment score (confidence level)\n"
      ],
      "metadata": {
        "id": "BOwePwHyv-OA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code performs two tasks:\n",
        "\n",
        " 1. **Generate an Answer (using FLAN-T5):**\n",
        "\n",
        "* It combines a question and context, feeds them into the FLAN-T5 model to generate an answer.\n",
        "\n",
        "* Example: For the question \"Which countries does the Ganges flow through?\" and the context \"The Ganges flows through India and Bangladesh\", it generates: \"India and Bangladesh\".\n",
        "\n",
        "2. **Check Entailment (using BART-large-MNLI):**\n",
        "\n",
        "* It checks if the generated answer is supported by the context using a zero-shot classification model.\n",
        "\n",
        "* The entailment score indicates how confident the model is that the answer is true based on the context.\n",
        "\n",
        "| **Function / Method**                          | **Purpose**                                                                                 |\n",
        "|:---------------------------------------------|:-------------------------------------------------------------------------------------------|\n",
        "| `AutoTokenizer.from_pretrained()`            | Loads the tokenizer for the model to convert text into tokens (numerical representations). |\n",
        "| `AutoModelForSeq2SeqLM.from_pretrained()`    | Loads the pre-trained sequence-to-sequence language model used to generate answers.        |\n",
        "| `tokenizer()`                                | Processes and converts input text into tokens that the model can understand.                |\n",
        "| `model.generate()`                           | Generates a sequence of tokens (the answer) based on the processed input tokens.           |\n",
        "| `tokenizer.decode()`                         | Converts the generated tokens from the model back into human-readable text.                |\n",
        "| `pipeline()`                                 | A high-level utility from Hugging Face to easily load models for specific tasks.           |\n",
        "| `nli()` (as part of a pipeline)              | Uses Natural Language Inference to check if an answer is supported by context.             |\n",
        "\n",
        "### **Expected Output:**\n",
        "\n",
        "* `Generated answer`: The model generates the answer based on the given context.\n",
        "\n",
        "* `Entailment score`: The NLI model provides a high score (e.g., 0.99) indicating that the hypothesis (\"India and Bangladesh\") is strongly supported by the context."
      ],
      "metadata": {
        "id": "k_EuupVeQ0ze"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. **D. Multilingual Zero-Shot QA**\n",
        "\n",
        "Use models like `joeddav/xlm-roberta-large-xnli` for cross-lingual entailment:"
      ],
      "metadata": {
        "id": "3qd5Tv9UwBuZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load the zero-shot classification pipeline with the XLM-RoBERTa-large-XNLI model for multilingual tasks\n",
        "nli = pipeline(\"zero-shot-classification\", model=\"joeddav/xlm-roberta-large-xnli\")\n",
        "\n",
        "# Define the context in French, describing the Seine river\n",
        "context = \"La Seine est un fleuve français, long de 777 kilomètres.\"  # The context information about the Seine river\n",
        "\n",
        "# Define the question related to the context\n",
        "question = \"Quel est le fleuve qui traverse Paris ?\"  # The question asking about the river that crosses Paris\n",
        "\n",
        "# Define the hypothesis which we want to check against the context\n",
        "hypothesis = \"La Seine traverse Paris.\"  # Hypothesis statement asserting that the Seine crosses Paris\n",
        "\n",
        "# Use the NLI model to check if the hypothesis is entailed by the context\n",
        "result = nli(sequences=context, candidate_labels=[hypothesis])  # Run the zero-shot classification\n",
        "\n",
        "# Print the result, which includes the labels (hypothesis) and the corresponding entailment scores\n",
        "print(result)  # Output the result showing the labels and their respective scores\n"
      ],
      "metadata": {
        "id": "2cu9ySkHwEu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code uses the **XLM-RoBERTa-large-XNLI model** for zero-shot classification to check if a hypothesis is supported by a given context.\n",
        "\n",
        "**XLM-RoBERTa-large-XNLI** is a cross-lingual model fine-tuned for zero-shot classification tasks, capable of understanding and classifying text in multiple languages.\n",
        "\n",
        "### **Code Explanation**\n",
        "\n",
        "* **zero-shot-classification**: Specifies the task the model will perform. In this case, it classifies text without any prior training on the specific labels.\n",
        "\n",
        "* `\"joeddav/xlm-roberta-large-xnli\"`: Specifies the XLM-RoBERTa-large-XNLI model, which is trained for multilingual tasks (XNLI stands for Cross-lingual Natural Language Inference). It can be used to check if a hypothesis holds true for a given context across various languages.\n",
        "\n",
        "* `hypothesis`: The statement being tested to see if it is true based on the provided context. Here, the hypothesis asserts that the Seine crosses Paris.\n",
        "\n",
        "* `sequences=context`: The context to evaluate the hypothesis against.\n",
        "\n",
        "* ``candidate_labels=[hypothesis]``: The hypothesis that needs to be checked.\n",
        "\n",
        "* `nli()`: The zero-shot classification model runs inference to see if the hypothesis is entailed (i.e., supported) by the context.\n",
        "\n",
        "### **Expected Output:**\n",
        "\n",
        "The expected output will show the hypothesis (\"La Seine traverse Paris\") along with a high confidence score, as the context directly mentions that the Seine is a French river, and it crosses Paris.\n",
        "\n",
        "* `labels`: The hypothesis that was evaluated. In this case, the hypothesis is that \"La Seine traverse Paris.\"\n",
        "\n",
        "* `scores`: The confidence score (between 0 and 1). A higher score (e.g., 0.99) means the model strongly supports the hypothesis based on the context."
      ],
      "metadata": {
        "id": "z6_kyQgIRVFI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. **E. Model Options for Zero-Shot QA**\n",
        "\n",
        "| Approach         | Model Examples                                      | Hugging Face Model Card                                                         |\n",
        "| ---------------- | --------------------------------------------------- | ------------------------------------------------------------------------------- |\n",
        "| NLI (Entailment) | facebook/bart-large-mnli, roberta-large-mnli        | [bart-large-mnli](https://huggingface.co/facebook/bart-large-mnli)              |\n",
        "| Generative QA    | google/flan-t5-large, mistralai/Mistral-7B-Instruct | [flan-t5-large](https://huggingface.co/google/flan-t5-large)                    |\n",
        "| Multilingual NLI | joeddav/xlm-roberta-large-xnli                      | [xlm-roberta-large-xnli](https://huggingface.co/joeddav/xlm-roberta-large-xnli) |\n",
        "| Classic QA       | deepset/roberta-base-squad2 (not zero-shot)         | [squad2](https://huggingface.co/deepset/roberta-base-squad2)                    |\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "hCqmfEFZwHRB"
      }
    }
  ]
}