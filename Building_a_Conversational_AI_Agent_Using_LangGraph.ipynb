{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "fdeace7c",
      "metadata": {
        "id": "fdeace7c"
      },
      "source": [
        "# Building a Conversational AI Agent Using LangGraph\n",
        "This guide is focused on using LangGraph to build a conversational AI agent. LangGraph is a powerful framework for constructing agentic systems—AI-driven systems that can handle tasks like decision-making, problem-solving, and interacting with users through natural language. LangGraph’s components are designed to be reusable and customizable, enabling developers to create intelligent agents with minimal effort."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9c905cd",
      "metadata": {
        "id": "e9c905cd"
      },
      "source": [
        "## 1. Install dependencies\n",
        "To install the necessary dependencies, run the following command:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93bd5e22",
      "metadata": {
        "id": "93bd5e22"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langgraph \"langchain[google-genai]\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "674271f5",
      "metadata": {
        "id": "674271f5"
      },
      "source": [
        "LangGraph and LangChain are required to build and interact with AI agents. `langchain` helps us interact with language models like Gemini."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Initialize Google Gemini Chat Model"
      ],
      "metadata": {
        "id": "LH00ts8MVka-"
      },
      "id": "LH00ts8MVka-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this step, we are initializing the Google Gemini chat model using LangChain’s init_chat_model function. The model is designed to interact with the user and generate responses."
      ],
      "metadata": {
        "id": "JKelmwjgYRna"
      },
      "id": "JKelmwjgYRna"
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "import getpass  # Used to securely prompt the user for sensitive input like an API key\n",
        "import os  # Used to interact with the operating system, like accessing environment variables\n",
        "\n",
        "# Checking if the GOOGLE_API_KEY environment variable is set\n",
        "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
        "    # If the API key is not set, prompt the user to enter it securely\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")\n",
        "\n",
        "# Importing the necessary function from LangChain to initialize the chat model\n",
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "# Initializing the Google Gemini chat model with specific settings\n",
        "model = init_chat_model(\"gemini-2.0-flash\", model_provider=\"google_genai\")\n"
      ],
      "metadata": {
        "id": "emsVyvZHVj6o"
      },
      "id": "emsVyvZHVj6o",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "\n",
        "**getpass:** This library is used to securely prompt the user to input sensitive information, such as an API key. It doesn't display the input as it's typed, ensuring security.\n",
        "\n",
        "**os:** The os library allows interaction with the operating system. Here, it's used to check if the API key is already set in the environment and to set the API key if not.\n",
        "\n",
        "**os.environ.get(\"GOOGLE_API_KEY\"):** This checks if the environment variable GOOGLE_API_KEY is already set. If it's not, the user is prompted to enter it.\n",
        "\n",
        "**init_chat_model:** This function from the langchain.chat_models module is used to initialize the Google Gemini chat model (version 2.0, with the \"flash\" configuration) for generating responses."
      ],
      "metadata": {
        "id": "csE3xJzZXMPE"
      },
      "id": "csE3xJzZXMPE"
    },
    {
      "cell_type": "markdown",
      "id": "bfc86077",
      "metadata": {
        "id": "bfc86077"
      },
      "source": [
        "## 3. Create an agent\n",
        "Now we will create an agent using `create_react_agent` and attach a tool to fetch the weather for a city."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b049c54",
      "metadata": {
        "id": "6b049c54"
      },
      "outputs": [],
      "source": [
        "from langgraph.prebuilt import create_react_agent\n",
        "\n",
        "def get_weather(city: str) -> str:\n",
        "    \"\"\"Get weather for a given city.\"\"\"\n",
        "    return f\"It's always sunny in {city}!\"\n",
        "\n",
        "agent = create_react_agent(\n",
        "    model=\"google_genai:gemini-2.0-flash\",\n",
        "    tools=[get_weather],\n",
        "    prompt=\"You are a helpful assistant\"\n",
        ")\n",
        "\n",
        "# Run the agent\n",
        "agent.invoke(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\n",
        ")\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d18bc21",
      "metadata": {
        "id": "6d18bc21"
      },
      "source": [
        "### Explanation\n",
        "- **create_react_agent**: A function from LangGraph that creates a conversational agent.\n",
        "- **Model**: We use 'gemini-2.0-flash' model from Google Gemini AI to power the agent.\n",
        "- **Tools**: The `get_weather` function acts as a tool that the agent can call.\n",
        "- **Invoke**: The agent responds to the user input, asking about the weather in San Francisco (sf)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fce717c2",
      "metadata": {
        "id": "fce717c2"
      },
      "source": [
        "## 4. Configure an LLM (Language Model)\n",
        "We will now configure the language model's parameters, such as temperature for controlling response randomness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "4994081f",
      "metadata": {
        "id": "4994081f"
      },
      "outputs": [],
      "source": [
        "\n",
        "from langchain.chat_models import init_chat_model\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "\n",
        "# Initialize the LLM with a specified temperature (0 for deterministic responses)\n",
        "model = init_chat_model(\n",
        "    \"google_genai:gemini-2.0-flash\",\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "# Create the agent with the configured model and weather tool\n",
        "agent = create_react_agent(\n",
        "    model=model,\n",
        "    tools=[get_weather],\n",
        ")\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fda78cbd",
      "metadata": {
        "id": "fda78cbd"
      },
      "source": [
        "### Explanation\n",
        "- **init_chat_model**: Initializes the model with parameters like `temperature`, which controls the randomness of the model's responses.\n",
        "- This setup makes the model's behavior more predictable."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f1acd86",
      "metadata": {
        "id": "4f1acd86"
      },
      "source": [
        "## 4. Add a Custom Prompt\n",
        "Now we will define a custom static prompt to guide the agent's responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bd68121",
      "metadata": {
        "id": "2bd68121"
      },
      "outputs": [],
      "source": [
        "\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "\n",
        "# Static prompt (fixed text that doesn't change)\n",
        "agent = create_react_agent(\n",
        "    model=\"google_genai:gemini-2.0-flash\",\n",
        "    tools=[get_weather],\n",
        "    prompt=\"Never answer questions about the weather.\"\n",
        ")\n",
        "\n",
        "# Try invoking the agent with a weather-related question\n",
        "response = agent.invoke(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\n",
        ")\n",
        "\n",
        "print(response)  # Expect an answer telling the agent not to answer weather questions\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "521d40c4",
      "metadata": {
        "id": "521d40c4"
      },
      "source": [
        "### Explanation\n",
        "- **Static Prompt**: We defined a fixed instruction telling the agent not to answer any weather-related questions. This guides the agent's behavior and response."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edc37571",
      "metadata": {
        "id": "edc37571"
      },
      "source": [
        "## 5. Add Memory (Multi-turn Conversations)\n",
        "We will now add memory to the agent so that it can remember past conversations for multi-turn interactions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8550b4c2",
      "metadata": {
        "id": "8550b4c2"
      },
      "outputs": [],
      "source": [
        "\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "from langgraph.checkpoint.memory import InMemorySaver\n",
        "\n",
        "# Set up the memory to store conversation state\n",
        "checkpointer = InMemorySaver()\n",
        "\n",
        "# Create the agent with memory\n",
        "agent = create_react_agent(\n",
        "    model=\"google_genai:gemini-2.0-flash\",\n",
        "    tools=[get_weather],\n",
        "    checkpointer=checkpointer\n",
        ")\n",
        "\n",
        "# Run agent for a conversation with a unique thread ID to remember the session\n",
        "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "\n",
        "# First conversation\n",
        "sf_response = agent.invoke(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]},\n",
        "    config\n",
        ")\n",
        "\n",
        "# Second conversation, using the same thread_id to keep the context\n",
        "ny_response = agent.invoke(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": \"what about new york?\"}]},\n",
        "    config\n",
        ")\n",
        "\n",
        "print(sf_response)\n",
        "print(ny_response)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d17aa51",
      "metadata": {
        "id": "5d17aa51"
      },
      "source": [
        "### Explanation\n",
        "- **Memory**: Using `InMemorySaver`, the agent is able to store the state of the conversation, making it capable of understanding and continuing multi-turn conversations.\n",
        "- **Thread ID**: This is used to uniquely identify a conversation session. By reusing the same `thread_id`, the agent remembers prior exchanges."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5847dfe7",
      "metadata": {
        "id": "5847dfe7"
      },
      "source": [
        "## 6. Configure Structured Output\n",
        "Finally, we will configure the agent to return structured responses using a Pydantic model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fab48fe1",
      "metadata": {
        "id": "fab48fe1"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "\n",
        "class WeatherResponse(BaseModel):\n",
        "    conditions: str\n",
        "\n",
        "agent = create_react_agent(\n",
        "    model=\"google_genai:gemini-2.0-flash\",\n",
        "    tools=[get_weather],\n",
        "    response_format=WeatherResponse\n",
        ")\n",
        "\n",
        "response = agent.invoke(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\n",
        ")\n",
        "\n",
        "response[\"structured_response\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b396ed0b",
      "metadata": {
        "id": "b396ed0b"
      },
      "source": [
        "### Explanation\n",
        "- **Pydantic Model**: We define a `WeatherResponse` model to specify that the response from the agent should contain a `conditions` field.\n",
        "- **Structured Response**: The agent now returns the weather conditions in a structured, predictable format."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
