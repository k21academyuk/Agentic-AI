{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "fdeace7c",
      "metadata": {
        "id": "fdeace7c"
      },
      "source": [
        "# Building a Conversational AI Agent Using LangGraph\n",
        "This guide is focused on using LangGraph to build a conversational AI agent. LangGraph is a powerful framework for constructing agentic systems—AI-driven systems that can handle tasks like decision-making, problem-solving, and interacting with users through natural language. LangGraph’s components are designed to be reusable and customizable, enabling developers to create intelligent agents with minimal effort."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9c905cd",
      "metadata": {
        "id": "e9c905cd"
      },
      "source": [
        "## 1. Install dependencies\n",
        "To install the necessary dependencies, run the following command:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "93bd5e22",
      "metadata": {
        "id": "93bd5e22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "becbc69a-f38a-436f-8bdc-e72eea51da6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langgraph\n",
            "  Downloading langgraph-0.5.3-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (0.6.15)\n",
            "Requirement already satisfied: langchain[google-genai] in /usr/local/lib/python3.11/dist-packages (0.3.26)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15) (2.25.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15) (5.29.5)\n",
            "Requirement already satisfied: langchain-core>=0.1 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.3.69)\n",
            "Collecting langgraph-checkpoint<3.0.0,>=2.1.0 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-2.1.1-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting langgraph-prebuilt<0.6.0,>=0.5.0 (from langgraph)\n",
            "  Downloading langgraph_prebuilt-0.5.2-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting langgraph-sdk<0.2.0,>=0.1.42 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.1.73-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langgraph) (2.11.7)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from langgraph) (3.5.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain[google-genai]) (0.3.8)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain[google-genai]) (0.4.6)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain[google-genai]) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain[google-genai]) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain[google-genai]) (6.0.2)\n",
            "Collecting langchain-google-genai (from langchain[google-genai])\n",
            "  Downloading langchain_google_genai-2.1.8-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15) (1.70.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15) (1.73.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15) (1.71.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-ai-generativelanguage==0.6.15) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-ai-generativelanguage==0.6.15) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-ai-generativelanguage==0.6.15) (4.9.1)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (25.0)\n",
            "Collecting ormsgpack>=1.10.0 (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph)\n",
            "  Downloading ormsgpack-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.11.0)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain[google-genai]) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain[google-genai]) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langgraph) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langgraph) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langgraph) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain[google-genai]) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain[google-genai]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain[google-genai]) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain[google-genai]) (2025.7.14)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain[google-genai]) (3.2.3)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain-google-genai->langchain[google-genai])\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "INFO: pip is looking at multiple versions of langchain-google-genai to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting langchain-google-genai (from langchain[google-genai])\n",
            "  Downloading langchain_google_genai-2.1.7-py3-none-any.whl.metadata (7.0 kB)\n",
            "  Downloading langchain_google_genai-2.1.6-py3-none-any.whl.metadata (7.0 kB)\n",
            "  Downloading langchain_google_genai-2.1.5-py3-none-any.whl.metadata (5.2 kB)\n",
            "  Downloading langchain_google_genai-2.1.4-py3-none-any.whl.metadata (5.2 kB)\n",
            "  Downloading langchain_google_genai-2.1.3-py3-none-any.whl.metadata (4.7 kB)\n",
            "  Downloading langchain_google_genai-2.1.2-py3-none-any.whl.metadata (4.7 kB)\n",
            "  Downloading langchain_google_genai-2.1.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "INFO: pip is still looking at multiple versions of langchain-google-genai to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading langchain_google_genai-2.1.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "  Downloading langchain_google_genai-2.0.11-py3-none-any.whl.metadata (3.6 kB)\n",
            "  Downloading langchain_google_genai-2.0.10-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: google-generativeai<0.9.0,>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai->langchain[google-genai]) (0.8.5)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai->langchain[google-genai]) (2.176.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai->langchain[google-genai]) (4.67.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core>=0.1->langgraph) (3.0.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-ai-generativelanguage==0.6.15) (0.6.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.3.1)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai->langchain[google-genai]) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai->langchain[google-genai]) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai->langchain[google-genai]) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai->langchain[google-genai]) (3.2.3)\n",
            "Downloading langgraph-0.5.3-py3-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint-2.1.1-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_prebuilt-0.5.2-py3-none-any.whl (23 kB)\n",
            "Downloading langgraph_sdk-0.1.73-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_google_genai-2.0.10-py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading ormsgpack-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (216 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.5/216.5 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: filetype, ormsgpack, langgraph-sdk, langgraph-checkpoint, langgraph-prebuilt, langgraph, langchain-google-genai\n",
            "Successfully installed filetype-1.2.0 langchain-google-genai-2.0.10 langgraph-0.5.3 langgraph-checkpoint-2.1.1 langgraph-prebuilt-0.5.2 langgraph-sdk-0.1.73 ormsgpack-1.10.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langgraph langchain[google-genai] google-ai-generativelanguage==0.6.15"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "674271f5",
      "metadata": {
        "id": "674271f5"
      },
      "source": [
        "LangGraph and LangChain are required to build and interact with AI agents. `langchain` helps us interact with language models like Gemini."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Initialize Google Gemini Chat Model"
      ],
      "metadata": {
        "id": "LH00ts8MVka-"
      },
      "id": "LH00ts8MVka-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this step, we are initializing the Google Gemini chat model using LangChain’s init_chat_model function. The model is designed to interact with the user and generate responses."
      ],
      "metadata": {
        "id": "JKelmwjgYRna"
      },
      "id": "JKelmwjgYRna"
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "import getpass  # Used to securely prompt the user for sensitive input like an API key\n",
        "import os  # Used to interact with the operating system, like accessing environment variables\n",
        "\n",
        "# Checking if the GOOGLE_API_KEY environment variable is set\n",
        "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
        "    # If the API key is not set, prompt the user to enter it securely\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")\n",
        "\n",
        "# Importing the necessary function from LangChain to initialize the chat model\n",
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "# Initializing the Google Gemini chat model with specific settings\n",
        "model = init_chat_model(\"gemini-2.0-flash\", model_provider=\"google_genai\")\n"
      ],
      "metadata": {
        "id": "emsVyvZHVj6o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16a94b88-d6c1-4e55-fb41-16ddd0ad412e"
      },
      "id": "emsVyvZHVj6o",
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter API key for Google Gemini: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "\n",
        "**getpass:** This library is used to securely prompt the user to input sensitive information, such as an API key. It doesn't display the input as it's typed, ensuring security.\n",
        "\n",
        "**os:** The os library allows interaction with the operating system. Here, it's used to check if the API key is already set in the environment and to set the API key if not.\n",
        "\n",
        "**os.environ.get(\"GOOGLE_API_KEY\"):** This checks if the environment variable GOOGLE_API_KEY is already set. If it's not, the user is prompted to enter it.\n",
        "\n",
        "**init_chat_model:** This function from the langchain.chat_models module is used to initialize the Google Gemini chat model (version 2.0, with the \"flash\" configuration) for generating responses."
      ],
      "metadata": {
        "id": "csE3xJzZXMPE"
      },
      "id": "csE3xJzZXMPE"
    },
    {
      "cell_type": "markdown",
      "id": "bfc86077",
      "metadata": {
        "id": "bfc86077"
      },
      "source": [
        "## 3. Create an agent\n",
        "Now we will create an agent using `create_react_agent` and attach a tool to fetch the weather for a city."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6b049c54",
      "metadata": {
        "id": "6b049c54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acc218b2-cfb2-479a-b71e-23d8385bca6a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='what is the weather in sf', additional_kwargs={}, response_metadata={}, id='a7f6e192-96a3-4335-b690-1023b146d68d'),\n",
              "  AIMessage(content='', additional_kwargs={'function_call': {'name': 'get_weather', 'arguments': '{\"city\": \"sf\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run--5c3ac1dc-84a6-4749-b4e0-763e2f6f2287-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'sf'}, 'id': '2d0d77a1-99f7-460d-a44e-a82da294eae4', 'type': 'tool_call'}], usage_metadata={'input_tokens': 25, 'output_tokens': 5, 'total_tokens': 30, 'input_token_details': {'cache_read': 0}}),\n",
              "  ToolMessage(content=\"It's always sunny in sf!\", name='get_weather', id='f1f68220-6887-4060-abce-4beffb3b94a8', tool_call_id='2d0d77a1-99f7-460d-a44e-a82da294eae4'),\n",
              "  AIMessage(content=\"It's always sunny in sf!\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run--79eda837-bd6a-4296-bd62-da7e5f7cb91b-0', usage_metadata={'input_tokens': 42, 'output_tokens': 9, 'total_tokens': 51, 'input_token_details': {'cache_read': 0}})]}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "from langgraph.prebuilt import create_react_agent\n",
        "\n",
        "def get_weather(city: str) -> str:\n",
        "    \"\"\"Get weather for a given city.\"\"\"\n",
        "    return f\"It's always sunny in {city}!\"\n",
        "\n",
        "agent = create_react_agent(\n",
        "    model=\"google_genai:gemini-2.0-flash\",\n",
        "    tools=[get_weather],\n",
        "    prompt=\"You are a helpful assistant\"\n",
        ")\n",
        "\n",
        "# Run the agent\n",
        "agent.invoke(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d18bc21",
      "metadata": {
        "id": "6d18bc21"
      },
      "source": [
        "### Explanation\n",
        "- **create_react_agent**: A function from LangGraph that creates a conversational agent.\n",
        "- **Model**: We use 'gemini-2.0-flash' model from Google Gemini AI to power the agent.\n",
        "- **Tools**: The `get_weather` function acts as a tool that the agent can call.\n",
        "- **Invoke**: The agent responds to the user input, asking about the weather in San Francisco (sf)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fce717c2",
      "metadata": {
        "id": "fce717c2"
      },
      "source": [
        "## 4. Configure an LLM (Language Model)\n",
        "We will now configure the language model's parameters, such as temperature for controlling response randomness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "4994081f",
      "metadata": {
        "id": "4994081f"
      },
      "outputs": [],
      "source": [
        "\n",
        "from langchain.chat_models import init_chat_model\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "\n",
        "# Initialize the LLM with a specified temperature (0 for deterministic responses)\n",
        "model = init_chat_model(\n",
        "    \"google_genai:gemini-2.0-flash\",\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "# Create the agent with the configured model and weather tool\n",
        "agent = create_react_agent(\n",
        "    model=model,\n",
        "    tools=[get_weather],\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fda78cbd",
      "metadata": {
        "id": "fda78cbd"
      },
      "source": [
        "### Explanation\n",
        "- **init_chat_model**: Initializes the model with parameters like `temperature`, which controls the randomness of the model's responses.\n",
        "- This setup makes the model's behavior more predictable."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f1acd86",
      "metadata": {
        "id": "4f1acd86"
      },
      "source": [
        "## 4. Add a Custom Prompt\n",
        "Now we will define a custom static prompt to guide the agent's responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "2bd68121",
      "metadata": {
        "id": "2bd68121",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dfaa135-e92b-4611-b82e-8c97b8f5e6fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'messages': [HumanMessage(content='what is the weather in sf', additional_kwargs={}, response_metadata={}, id='620410ff-d248-4466-9eb0-8bc612c283ab'), AIMessage(content='I am sorry, I cannot fulfill this request. The available tools lack the desired functionality.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run--8a4b8704-e14a-44a5-95ba-8c14b61e4675-0', usage_metadata={'input_tokens': 27, 'output_tokens': 19, 'total_tokens': 46, 'input_token_details': {'cache_read': 0}})]}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "\n",
        "# Static prompt (fixed text that doesn't change)\n",
        "agent = create_react_agent(\n",
        "    model=\"google_genai:gemini-2.0-flash\",\n",
        "    tools=[get_weather],\n",
        "    prompt=\"Never answer questions about the weather.\"\n",
        ")\n",
        "\n",
        "# Try invoking the agent with a weather-related question\n",
        "response = agent.invoke(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\n",
        ")\n",
        "\n",
        "print(response)  # Expect an answer telling the agent not to answer weather questions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "521d40c4",
      "metadata": {
        "id": "521d40c4"
      },
      "source": [
        "### Explanation\n",
        "- **Static Prompt**: We defined a fixed instruction telling the agent not to answer any weather-related questions. This guides the agent's behavior and response."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edc37571",
      "metadata": {
        "id": "edc37571"
      },
      "source": [
        "## 5. Add Memory (Multi-turn Conversations)\n",
        "We will now add memory to the agent so that it can remember past conversations for multi-turn interactions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "8550b4c2",
      "metadata": {
        "id": "8550b4c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f51d718b-140a-4404-f708-78b43848f01b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'messages': [HumanMessage(content='what is the weather in sf', additional_kwargs={}, response_metadata={}, id='9f5c6aec-3be7-48b9-b007-e4115e390644'), AIMessage(content='Could you please spell out the full city name?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run--fc13a7de-a1c9-4d8f-9b70-c34937e922b9-0', usage_metadata={'input_tokens': 20, 'output_tokens': 11, 'total_tokens': 31, 'input_token_details': {'cache_read': 0}})]}\n",
            "{'messages': [HumanMessage(content='what is the weather in sf', additional_kwargs={}, response_metadata={}, id='9f5c6aec-3be7-48b9-b007-e4115e390644'), AIMessage(content='Could you please spell out the full city name?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run--fc13a7de-a1c9-4d8f-9b70-c34937e922b9-0', usage_metadata={'input_tokens': 20, 'output_tokens': 11, 'total_tokens': 31, 'input_token_details': {'cache_read': 0}}), HumanMessage(content='what about new york?', additional_kwargs={}, response_metadata={}, id='7e34ac5c-b291-45b1-bfc6-9e6fba01a99c'), AIMessage(content='', additional_kwargs={'function_call': {'name': 'get_weather', 'arguments': '{\"city\": \"New York\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run--13874712-89b2-4b95-96da-e94e70ae7d46-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'New York'}, 'id': 'd6826649-40dc-455f-ad1e-d5da8e7ad8c8', 'type': 'tool_call'}], usage_metadata={'input_tokens': 35, 'output_tokens': 6, 'total_tokens': 41, 'input_token_details': {'cache_read': 0}}), ToolMessage(content=\"It's always sunny in New York!\", name='get_weather', id='873936f2-c704-4e7a-ba8b-95d7e6d40cf6', tool_call_id='d6826649-40dc-455f-ad1e-d5da8e7ad8c8'), AIMessage(content=\"It's always sunny in New York!\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run--2ee07f9d-fd25-4c66-ae99-66342a270c02-0', usage_metadata={'input_tokens': 54, 'output_tokens': 10, 'total_tokens': 64, 'input_token_details': {'cache_read': 0}})]}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "from langgraph.checkpoint.memory import InMemorySaver\n",
        "\n",
        "# Set up the memory to store conversation state\n",
        "checkpointer = InMemorySaver()\n",
        "\n",
        "# Create the agent with memory\n",
        "agent = create_react_agent(\n",
        "    model=\"google_genai:gemini-2.0-flash\",\n",
        "    tools=[get_weather],\n",
        "    checkpointer=checkpointer\n",
        ")\n",
        "\n",
        "# Run agent for a conversation with a unique thread ID to remember the session\n",
        "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "\n",
        "# First conversation\n",
        "sf_response = agent.invoke(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]},\n",
        "    config\n",
        ")\n",
        "\n",
        "# Second conversation, using the same thread_id to keep the context\n",
        "ny_response = agent.invoke(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": \"what about new york?\"}]},\n",
        "    config\n",
        ")\n",
        "\n",
        "print(sf_response)\n",
        "print(ny_response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d17aa51",
      "metadata": {
        "id": "5d17aa51"
      },
      "source": [
        "### Explanation\n",
        "- **Memory**: Using `InMemorySaver`, the agent is able to store the state of the conversation, making it capable of understanding and continuing multi-turn conversations.\n",
        "- **Thread ID**: This is used to uniquely identify a conversation session. By reusing the same `thread_id`, the agent remembers prior exchanges."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5847dfe7",
      "metadata": {
        "id": "5847dfe7"
      },
      "source": [
        "## 6. Configure Structured Output\n",
        "Finally, we will configure the agent to return structured responses using a Pydantic model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "fab48fe1",
      "metadata": {
        "id": "fab48fe1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "718c9210-3e8a-4f34-d1dd-9e4bb9c535a2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "WeatherResponse(conditions='San Francisco')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "from pydantic import BaseModel\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "\n",
        "class WeatherResponse(BaseModel):\n",
        "    conditions: str\n",
        "\n",
        "agent = create_react_agent(\n",
        "    model=\"google_genai:gemini-2.0-flash\",\n",
        "    tools=[get_weather],\n",
        "    response_format=WeatherResponse\n",
        ")\n",
        "\n",
        "response = agent.invoke(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\n",
        ")\n",
        "\n",
        "response[\"structured_response\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b396ed0b",
      "metadata": {
        "id": "b396ed0b"
      },
      "source": [
        "### Explanation\n",
        "- **Pydantic Model**: We define a `WeatherResponse` model to specify that the response from the agent should contain a `conditions` field.\n",
        "- **Structured Response**: The agent now returns the weather conditions in a structured, predictable format."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}