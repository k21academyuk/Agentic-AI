{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lab: Building Simple RAG Using Langchain\n",
        "\n",
        "In this lab, we will build a **Retrieval-Augmented Generation (RAG)** system that combines information retrieval and text generation to process large documents, such as legal contracts. The RAG system leverages **FAISS** for efficient similarity search and **Hugging Face's generative models** (like GPT-2) for synthesizing human-like answers. By embedding the document content and indexing it with FAISS, we can quickly retrieve relevant sections of text and generate contextually accurate responses to specific queries.\n",
        "\n",
        "*The main objective of this system is to enable automated document analysis, allowing users to query large legal documents and get detailed, relevant, and concise answers without manually reading through the entire document.*\n",
        "\n",
        "This approach significantly improves efficiency for industries like law, where processing vast amounts of textual data is crucial."
      ],
      "metadata": {
        "id": "g9AxA1a9HXSs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Install Required Libraries\n",
        "\n",
        "In this step, we install the necessary libraries that will be used throughout the lab. **LangChain** is essential for orchestrating the RAG system, integrating document processing and retrieval with language models.\n",
        "\n",
        "**FAISS** is used to index and search the document embeddings efficiently, which allows us to quickly retrieve relevant content. **Transformers** provides access to pre-trained language models, and **Sentence-Transformers** allows us to generate meaningful sentence embeddings.\n",
        "\n",
        " These libraries are fundamental to building a scalable and efficient RAG system."
      ],
      "metadata": {
        "id": "FQB2XxQFejjG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85tgWgUCKOOR"
      },
      "outputs": [],
      "source": [
        "!pip install langchain\n",
        "!pip install faiss-cpu\n",
        "!pip install transformers\n",
        "!pip install sentence-transformers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Explanation of the Code:**\n",
        "\n",
        "These commands install essential libraries needed to build a Retrieval-Augmented Generation (RAG) system using LangChain and related tools.\n",
        "\n",
        "* **!pip install langchain:** Installs LangChain, a framework for integrating large language models (LLMs) with tasks like document retrieval and generation.\n",
        "\n",
        "* **!pip install faiss-cpu:** Installs FAISS, a library for fast similarity search over high-dimensional data, used for efficient document retrieval.\n",
        "\n",
        "* **!pip install transformers:** Installs the Transformers library, providing access to pre-trained models for tasks like text generation and embeddings.\n",
        "\n",
        "* **!pip install sentence-transformers:** Installs Sentence-Transformers, which generates sentence embeddings to capture the semantic meaning of text for similarity-based tasks."
      ],
      "metadata": {
        "id": "tfIGmB0tUhlp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Import Required Libraries\n",
        "\n",
        "1. The commands update **LangChain** to its latest version and install the **LangChain Community edition**, which provides additional features and integrations for building AI applications like document retrieval and text generation."
      ],
      "metadata": {
        "id": "lPL0Sqgmbyv8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade langchain\n",
        "!pip install langchain-community\n"
      ],
      "metadata": {
        "id": "rhd86jNwLGik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Explanation of the Code:**\n",
        "\n",
        "* **!pip install --upgrade langchain:** Upgrades the LangChain library to the latest version, ensuring you have access to the newest features, improvements, and bug fixes for developing applications with large language models (LLMs).\n",
        "\n",
        "* **!pip install langchain-community:** Installs the LangChain Community edition, which includes additional tools, integrations, and community-driven enhancements that extend the core LangChain functionality for more advanced use cases."
      ],
      "metadata": {
        "id": "nJH0ykkHVMfW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. This code imports libraries for building a **RAG** system, including **LangChain** for document retrieval and generation, FAISS for similarity search, and **SentenceTransformer** for generating embeddings. It also uses **TextLoader** to load documents and **RecursiveCharacterTextSplitter** to split large texts into chunks for efficient processing."
      ],
      "metadata": {
        "id": "5uwv8Jtfb0Jq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from sentence_transformers import SentenceTransformer\n"
      ],
      "metadata": {
        "id": "JRboMxIoLXxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The commands themselves don't produce direct outputs, as they are imports. However, the expected behavior is:\n",
        "\n",
        "* The necessary modules and functions are loaded into the environment, making them available for use in the code.\n",
        "\n",
        "* No explicit output will be displayed unless errors occur (e.g., if a module is not installed or there is an issue with the import).\n",
        "\n",
        "#### **Explanation of the Code:**\n",
        "\n",
        "1. **import os**: Imports the os module for interacting with the operating system, such as file handling and directory management.\n",
        "\n",
        "2. **from langchain.chains import RetrievalQA**: Imports RetrievalQA from LangChain, enabling the creation of a chain that combines document retrieval and question answering.\n",
        "\n",
        "3. **from langchain.vectorstores import FAISS**: Imports FAISS from LangChain, which is used to create a vector store for efficient similarity search over high-dimensional data.\n",
        "\n",
        "4. **from langchain.embeddings import HuggingFaceEmbeddings**: Imports HuggingFaceEmbeddings from LangChain, enabling the generation of text embeddings using pre-trained models from Hugging Face.\n",
        "\n",
        "5. **from langchain.document_loaders import TextLoader**: Imports TextLoader from LangChain to load text documents (e.g., .txt files) for further processing.\n",
        "\n",
        "6. **from langchain.text_splitter import RecursiveCharacterTextSplitter** :Imports RecursiveCharacterTextSplitter from LangChain, used for splitting large documents into smaller, manageable chunks based on character count.\n",
        "\n",
        "7. **from sentence_transformers import SentenceTransformer**: Imports SentenceTransformer, a library for generating sentence embeddings to represent the semantic meaning of text."
      ],
      "metadata": {
        "id": "x60PQc2JYoZS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. To verify that **LangChain** and its dependencies are installed correctly, you can run the below code. This will **print the version** of LangChain you have installed. Ensure it is up to date *(e.g., version 0.3.x or higher).*"
      ],
      "metadata": {
        "id": "IPGFcVNMf6oG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import langchain\n",
        "print(langchain.__version__)\n"
      ],
      "metadata": {
        "id": "oAz7w5g9LjNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Expected Output:**\n",
        "\n",
        "The version number of the LangChain library installed in your environment will be printed\n",
        "\n",
        "```0.3.27```"
      ],
      "metadata": {
        "id": "BP6PEVqKbeeS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Load and Preprocess Data (Documents)\n",
        "\n",
        "\n",
        "1. In this step, the **document** (such as a legal contract) is uploaded to Google Colab using the files.upload() function, which opens a dialog for selecting the file. This is essential to bring the document into the environment for further processing, and it ensures that the file is **accessible** for loading and manipulation.\n",
        "\n",
        "Here, we will assume you have a text file with your legal documents. If you Not then **Download the Document form Here:**\n",
        "\n",
        "[legal_documents.txt](https://github.com/k21academyuk/Agentic-AI/blob/main/legal_documents.txt)"
      ],
      "metadata": {
        "id": "HrVNVO-4gKBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()  # This will open the file upload dialog\n"
      ],
      "metadata": {
        "id": "LbVUmM0sMLyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Code Explanation:**\n",
        "\n",
        "1. **from google.colab import files**: This imports the files module from Google Colab, which provides methods for file handling in a Colab environment, such as uploading or downloading files.\n",
        "\n",
        "2. **uploaded = files.upload():** This triggers the file upload dialog in Google Colab, allowing the user to select and upload files from their local system to the Colab environment. The uploaded files are stored in the uploaded variable, which is a dictionary containing the uploaded file names and their corresponding file objects."
      ],
      "metadata": {
        "id": "m7aLesDZclK7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. After uploading the file, you can **verify the file's existence** using the os module:\n"
      ],
      "metadata": {
        "id": "jpBCUUbbhR4A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.listdir())  # This will show the files in the current directory\n"
      ],
      "metadata": {
        "id": "Rm-X9JbbMVFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Expected Output:**\n",
        "\n",
        "The output will be a list of files and directories in the current directory. For example:\n",
        "\n",
        "```['.config', 'legal_documents.txt', 'sample_data']```\n",
        "\n",
        "This shows the files available in the current directory, including the uploaded file ```legal_documents.txt.```\n",
        "\n",
        "#### **Code Explanation:**\n",
        "\n",
        "* **print(os.listdir())**: This prints the list of files and directories present in the current working directory. It helps verify the available files in the environment."
      ],
      "metadata": {
        "id": "PrUL80ZJfJvn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. The code uses ```TextLoader``` from LangChain to load the **legal document** ```(legal_documents.txt)```. The ```load()``` method reads the document and prepares it for further processing, such as splitting into chunks or generating embeddings. It is important to ensure the **file name** matches the uploaded document to avoid errors."
      ],
      "metadata": {
        "id": "Yu-_zIU5hVX8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loader = TextLoader(\"legal_documents.txt\")  # Make sure the file name matches the uploaded file\n",
        "documents = loader.load()\n"
      ],
      "metadata": {
        "id": "V44g43BVMW_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Code Explanation:**\n",
        "\n",
        "* **loader = TextLoader(\"legal_documents.txt\"):** This initializes a TextLoader object that loads the legal_documents.txt file from the current directory. It's crucial that the file name matches exactly, including the extension, to avoid errors.\n",
        "\n",
        "* **documents = loader.load():** The load() method reads the content of the legal_documents.txt file and stores it in the documents variable, making the text available for further processing."
      ],
      "metadata": {
        "id": "iUJEezJKf2u9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. After loading the document, we use the ```RecursiveCharacterTextSplitter``` to break the document into smaller, manageable chunks of text. Each chunk is typically sized at 1000 characters with an overlap of 200 characters. This step is important for **handling large documents effectively**, ensuring that chunks maintain context while being small enough for efficient processing, such as embedding generation and retrieval tasks."
      ],
      "metadata": {
        "id": "lsvBUJ_lhe43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load documents (text file in this case)\n",
        "loader = TextLoader(\"legal_documents.txt\")  # Replace with your actual file path\n",
        "documents = loader.load()\n",
        "\n",
        "# Split the documents into chunks to manage large documents\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "texts = text_splitter.split_documents(documents)\n"
      ],
      "metadata": {
        "id": "YzwvR3UxMfUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Code Explanation:**\n",
        "\n",
        "* The **TextLoader** loads the legal_documents.txt file from the file system, ensuring the file path is correct for successful loading.\n",
        "\n",
        "* **loader.load()** reads the content of the file and loads it into memory, returning the document in a list format.\n",
        "\n",
        "* The **RecursiveCharacterTextSplitter** splits the document into smaller chunks of 1000 characters each, with a 200-character overlap to ensure context is maintained between adjacent chunks.\n",
        "\n",
        "* **split_documents(documents)** processes the loaded document and divides it into manageable text chunks for more efficient processing, particularly useful for tasks like embedding or retrieval."
      ],
      "metadata": {
        "id": "wespxxeOginw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Generate Embeddings with Hugging Face\n",
        "\n",
        "In this step, we initialize a **Sentence Transformer model** from Hugging Face, specifically the ```all-MiniLM-L6-v2``` model, which is pre-trained to generate high-quality **sentence embeddings**. These embeddings are vector representations of text that capture the semantic meaning of each document chunk. The model is applied to each chunk of text from the previously split document, and the ```encode()``` method generates an embedding for each chunk. By creating embeddings, we can efficiently compare the semantic similarity between text chunks, which is crucial for **document retrieval** and generating contextually relevant answers to user queries."
      ],
      "metadata": {
        "id": "eTCe9YCGgPaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Hugging Face model (using a sentence transformer model)\n",
        "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Generate embeddings for each document chunk\n",
        "embeddings = [embedding_model.encode(text.page_content) for text in texts]  # Access page_content for text in documents\n",
        "\n",
        "# Now 'embeddings' will contain the embeddings for each chunk of text in the document\n"
      ],
      "metadata": {
        "id": "PSc1-OiRM-ge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Code Explanation:**\n",
        "\n",
        "* **SentenceTransformer('all-MiniLM-L6-v2')** initializes the Hugging Face model all-MiniLM-L6-v2, which is a pre-trained model designed to generate embeddings that capture the semantic meaning of text.\n",
        "\n",
        "* **embedding_model.encode(text.page_content)** generates embeddings for each document chunk. The encode() function processes the text chunks and converts them into vector representations (numerical embeddings).\n",
        "\n",
        "* The result is a list of embeddings, where each entry corresponds to a chunk of text, allowing for efficient comparison and retrieval based on semantic similarity."
      ],
      "metadata": {
        "id": "-UGmRNBLg-my"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Create FAISS Vector Store\n",
        "\n",
        "1. This step creates a **FAISS vector store** to store the embeddings generated from document chunks. FAISS enables efficient similarity search, allowing fast retrieval of relevant text chunks based on a query. The **HuggingFaceEmbeddings** class from LangChain generates these embeddings. However, there's a deprecation warning suggesting the use of the updated ```langchain_huggingface``` package for future compatibility."
      ],
      "metadata": {
        "id": "1CGk07rrgXbG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "# Create a FAISS vector store using the embeddings\n",
        "vector_store = FAISS.from_documents(texts, HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\"))\n"
      ],
      "metadata": {
        "id": "FLlEKS25NBlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Code Explanation:**\n",
        "\n",
        "* ```from langchain.vectorstores import FAISS:``` This imports the FAISS class from LangChain, which is used to create an efficient similarity search index for document embeddings.\n",
        "\n",
        "* ```FAISS.from_documents(texts, HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\"))```: This line creates a **FAISS vector** store using the document chunks (stored in texts) and the **HuggingFaceEmbeddings** to generate embeddings from the sentence-transformers/all-MiniLM-L6-v2 model. The vector store allows for fast similarity searches on the document embeddings."
      ],
      "metadata": {
        "id": "6dc60C5IhQRi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. In this step the **FAISS index** is then saved locally with ```save_local()``` for future use, avoiding the need to regenerate the index every time."
      ],
      "metadata": {
        "id": "DppngXYhHK-0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the FAISS index (optional, for future use)\n",
        "vector_store.save_local(\"faiss_index\")\n"
      ],
      "metadata": {
        "id": "SWJsotl-NEog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This command **saves** the FAISS vector store locally to a directory named ```faiss_index```. Saving the index allows you to reuse it later without needing to regenerate the embeddings, saving computational resources and time. This is especially useful for large datasets or when you want to store the index for future retrieval operations."
      ],
      "metadata": {
        "id": "-s4YIgEsica8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: Set Up Retriever and Generative Model\n",
        "\n",
        "In this step, we set up the **retriever** to fetch the most relevant chunks of text from the **FAISS vector store** based on the user’s query. We also initialize a **text generation model** from **Hugging Face** (e.g., GPT-2) to synthesize answers based on the retrieved text. The **retriever** fetches the top **k** relevant document chunks, while the **generative model** generates a concise response based on those chunks. Combining these two components, the **RetrieverQA** chain enables the RAG system to retrieve context and generate a comprehensive answer, automating document analysis."
      ],
      "metadata": {
        "id": "c9m-xYA_gfL1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. This step sets up a **retriever** and a generative model using LangChain and Hugging Face, enabling you to build a system that retrieves relevant information from documents and generates human-like responses."
      ],
      "metadata": {
        "id": "NYbH-82VL3lq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "i9eBnxKji280"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Code Explanation:**\n",
        "\n",
        "* ```from langchain.llms import HuggingFacePipeline```: This imports the **HuggingFacePipeline** class from LangChain, which is used to integrate Hugging Face models into LangChain workflows. It allows you to use Hugging Face's pre-trained models (like GPT) for tasks such as text generation within LangChain chains.\n",
        "\n",
        "* ```from transformers import pipeline```:This imports the pipeline function from the Transformers library. The pipeline function is used to load and use pre-trained models for specific NLP tasks such as text generation, question answering, translation, etc."
      ],
      "metadata": {
        "id": "9qZi0TLlL9Aq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. This step sets up the retriever to fetch relevant document chunks based on a query and initializes a **text generation model** to generate responses from those chunks."
      ],
      "metadata": {
        "id": "BEIanKJXMb14"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the retriever to fetch the top 2 most relevant chunks for each query\n",
        "retriever = vector_store.as_retriever(search_kwargs={\"k\": 2})\n",
        "\n",
        "# Use Hugging Face's GPT model for text generation\n",
        "generator = pipeline('text-generation', model='gpt2')  # You can change the model (e.g., GPT-Neo)"
      ],
      "metadata": {
        "id": "ECcI5zPLNi5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Code Explanation:**\n",
        "\n",
        "* **Retriever** fetches the top 2 relevant document chunks based on the query.\n",
        "\n",
        "* GPT-2 is set up as a **generative model** to produce responses based on the retrieved chunks."
      ],
      "metadata": {
        "id": "41ZocGotMwQo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. This step integrates the retriever and **text generation model (LLM)** into a single **LangChain QA (Question-Answering) system.**"
      ],
      "metadata": {
        "id": "taPUMnSfM-bf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrap Hugging Face pipeline in LangChain's HuggingFacePipeline\n",
        "# Added max_length to limit the input sequence length\n",
        "llm = HuggingFacePipeline(pipeline=generator, model_kwargs={'max_length': 512})\n",
        "\n",
        "# Combine the retriever and LLM into a single chain\n",
        "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)"
      ],
      "metadata": {
        "id": "ptZJWv9Ajgu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Code Explanation:**\n",
        "\n",
        "* The **HuggingFacePipeline** wraps the Hugging Face model for easy use in LangChain.\n",
        "\n",
        "* ```llm=llm``` connects the Hugging Face model (GPT-2) as the generative model for the QA process.\n",
        "\n",
        "* ```retriever=retriever``` links the retriever (which fetches relevant document chunks) to the QA chain.\n",
        "\n",
        "* ```chain_type=\"stuff\"``` indicates that the system will use the full retrieved context (document chunks) for generating answers.\n",
        "\n",
        "* **RetrievalQA** combines the retriever and LLM into a single QA chain, enabling automated answers based on the context retrieved from the documents."
      ],
      "metadata": {
        "id": "bF1c_gGAOlF-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 7: Query the System\n",
        "\n",
        "In this step, we query the system (e.g., asking about termination clauses in a contract). The system retrieves relevant chunks of text and generates an answer using the pre-configured **retriever** and **generative model** (GPT-2). This step shows how the system can automatically generate responses based on specific queries.\n",
        "\n"
      ],
      "metadata": {
        "id": "3596YvuUg3vB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Query the system for termination clauses\n",
        "query = \"What are the termination clauses in this contract?\"\n",
        "result = qa.run(query)\n",
        "\n",
        "# Print the result (the answer generated by the system)\n",
        "print(result)\n"
      ],
      "metadata": {
        "id": "j8DlYURmN400"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Code Explanation:**\n",
        "\n",
        "* ```result = qa.run(query)```: This line runs the QA chain (qa), which has already been set up with a retriever and a generative model (likely GPT-based model).\n",
        "\n",
        "The system should ideally return a concise answer that directly addresses the termination clauses of the contract."
      ],
      "metadata": {
        "id": "ibkDAEXCPv3b"
      }
    }
  ]
}