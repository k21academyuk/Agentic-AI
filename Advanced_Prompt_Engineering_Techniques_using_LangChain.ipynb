{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Lab: Advanced Prompt Engineering Techniques using LangChain\n",
        "\n",
        "Prompt engineering is the process of designing the inputs or prompts given to an AI model to ensure it generates the most accurate, relevant, and useful responses.\n",
        "\n",
        "This lab explores four key prompt engineering techniques to improve your AI interactions using LangChain."
      ],
      "metadata": {
        "id": "61Tw9Hs8mVCh"
      },
      "id": "61Tw9Hs8mVCh"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **What is Prompt Engineering?**\n",
        "\n",
        "Prompt engineering involves creating well-crafted inputs (prompts) that guide the AI to produce the best possible outputs. It's about asking the right questions and setting the right context so that the AI model can understand and respond appropriately.\n",
        "\n",
        "In simple terms, it's like giving clear instructions to the AI to ensure that it performs the task accurately. The better the instructions (or prompt), the better the AI’s response."
      ],
      "metadata": {
        "id": "0eqIlB7Pmd9R"
      },
      "id": "0eqIlB7Pmd9R"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Prompt Engineering Techniques**\n",
        "- **Zero-shot Prompting:** The model performs a task without any examples or prior context, relying solely on the provided instructions. It uses its pre-trained knowledge to generate the answer.\n",
        "\n",
        "- **Few-shot Learning:** The model is provided with a small set of examples, allowing it to better grasp the task. The more examples it gets, the more accurately it can predict or generate outcomes.\n",
        "\n",
        "- **Chain-of-Thought Prompting:** The model is encouraged to think through the task step-by-step, breaking it down into smaller, logical parts. This process helps improve reasoning and results in more accurate answers.\n",
        "\n",
        "- **Tree-of-Thoughts:** An advanced form of chain-of-thought prompting, where the model explores multiple possible solutions or paths. It generates different answers and evaluates them in a branched structure, allowing for deeper exploration and decision-making."
      ],
      "metadata": {
        "id": "T6TtIscem4zA"
      },
      "id": "T6TtIscem4zA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Lab Execution**👇\n",
        "\n"
      ],
      "metadata": {
        "id": "2DqNVG0QnIgU"
      },
      "id": "2DqNVG0QnIgU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Prerequisites**\n",
        "You will need an OpenAI API key to complete this lab. If you donot have one yet, please create it by following the guide [Prerequisite Lab - Creating API Keys](https://www.skool.com/k21academy/classroom/0a7dc03d?md=35ab1858bac04486b66a32547d6da30f) — specifically, the section Creating OpenAI API Keys."
      ],
      "metadata": {
        "id": "aC_kwq0fncmB"
      },
      "id": "aC_kwq0fncmB"
    },
    {
      "cell_type": "markdown",
      "id": "f874ecbe-ed41-4dd6-87ae-ed844c6ac287",
      "metadata": {
        "id": "f874ecbe-ed41-4dd6-87ae-ed844c6ac287"
      },
      "source": [
        "### 1. **Setup the Environment**\n",
        "\n",
        "a. **Installing LangChain and Required Packages**\n",
        "In order to work with LangChain, we need to install the core LangChain package, LangChain OpenAI integration, and LangChain Community version. Below is the installation command along with an explanation for each package."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install LangChain package (core functionality)\n",
        "!pip install langchain==0.3.27  # This installs the core LangChain library that provides essential tools for building language model-based applications.\n",
        "\n",
        "# Install LangChain OpenAI integration (for OpenAI API usage)\n",
        "!pip install langchain-openai==0.3.8  # This package allows the use of OpenAI models (e.g., GPT-3, GPT-4) within LangChain, enabling smooth integration with OpenAI's API.\n",
        "\n",
        "# Install LangChain Community edition (for additional features)\n",
        "!pip install langchain-community==0.3.27  # This version includes community-driven contributions and additional features that expand the capabilities of LangChain."
      ],
      "metadata": {
        "id": "gipUbZ9bpCMf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0b029fb-4a4c-40bd-f538-8a70409fcd51"
      },
      "id": "gipUbZ9bpCMf",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain==0.3.27 in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain==0.3.27) (0.3.75)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain==0.3.27) (0.3.11)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain==0.3.27) (0.4.24)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain==0.3.27) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain==0.3.27) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain==0.3.27) (2.32.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain==0.3.27) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain==0.3.27) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain==0.3.27) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain==0.3.27) (4.15.0)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain==0.3.27) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain==0.3.27) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain==0.3.27) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain==0.3.27) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain==0.3.27) (0.24.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.27) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.27) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.27) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain==0.3.27) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain==0.3.27) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain==0.3.27) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain==0.3.27) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.3.27) (3.2.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain==0.3.27) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain==0.3.27) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain==0.3.27) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain==0.3.27) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain==0.3.27) (1.3.1)\n",
            "Collecting langchain-openai==0.3.8\n",
            "  Downloading langchain_openai-0.3.8-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.42 in /usr/local/lib/python3.12/dist-packages (from langchain-openai==0.3.8) (0.3.75)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.58.1 in /usr/local/lib/python3.12/dist-packages (from langchain-openai==0.3.8) (1.106.1)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.12/dist-packages (from langchain-openai==0.3.8) (0.11.0)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.42->langchain-openai==0.3.8) (0.4.24)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.42->langchain-openai==0.3.8) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.42->langchain-openai==0.3.8) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.42->langchain-openai==0.3.8) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.42->langchain-openai==0.3.8) (4.15.0)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.42->langchain-openai==0.3.8) (25.0)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.42->langchain-openai==0.3.8) (2.11.7)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai==0.3.8) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai==0.3.8) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai==0.3.8) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai==0.3.8) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai==0.3.8) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai==0.3.8) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1,>=0.7->langchain-openai==0.3.8) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1,>=0.7->langchain-openai==0.3.8) (2.32.4)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.58.1->langchain-openai==0.3.8) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain-openai==0.3.8) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain-openai==0.3.8) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain-openai==0.3.8) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.42->langchain-openai==0.3.8) (3.0.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.42->langchain-openai==0.3.8) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.42->langchain-openai==0.3.8) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.42->langchain-openai==0.3.8) (0.24.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.42->langchain-openai==0.3.8) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.42->langchain-openai==0.3.8) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.42->langchain-openai==0.3.8) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai==0.3.8) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai==0.3.8) (2.5.0)\n",
            "Downloading langchain_openai-0.3.8-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain-openai\n",
            "Successfully installed langchain-openai-0.3.8\n",
            "Collecting langchain-community==0.3.27\n",
            "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.12/dist-packages (from langchain-community==0.3.27) (0.3.75)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /usr/local/lib/python3.12/dist-packages (from langchain-community==0.3.27) (0.3.27)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain-community==0.3.27) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain-community==0.3.27) (2.32.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community==0.3.27) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community==0.3.27) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community==0.3.27) (8.5.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community==0.3.27)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community==0.3.27) (2.10.1)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community==0.3.27) (0.4.24)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community==0.3.27) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community==0.3.27) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.27) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.27) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.27) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.27) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.27) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.27) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.27) (1.20.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.27)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.27)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community==0.3.27) (0.3.11)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community==0.3.27) (2.11.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community==0.3.27) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community==0.3.27) (4.15.0)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community==0.3.27) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community==0.3.27) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community==0.3.27) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community==0.3.27) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community==0.3.27) (0.24.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community==0.3.27) (1.1.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community==0.3.27) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain-community==0.3.27) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain-community==0.3.27) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain-community==0.3.27) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain-community==0.3.27) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community==0.3.27) (3.2.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community==0.3.27) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community==0.3.27) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community==0.3.27) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain-community==0.3.27) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community==0.3.27) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community==0.3.27) (2.33.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.27)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community==0.3.27) (1.3.1)\n",
            "Downloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 langchain-community-0.3.27 marshmallow-3.26.1 mypy-extensions-1.1.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "b. **Setting Up the OpenAI API Key Securely**\n",
        "\n",
        "The code below demonstrates how to securely input and set up the OpenAI API key for use within your environment. This ensures that your API key is kept private and not hard-coded into the script.\n",
        "\n",
        "**Note:** Please paste your OpenAI API key in the dialog box below and press the Enter button. If you haven't created an API key yet, refer to the \"Prerequisites\" section for instructions on how to generate one. Remember to keep your API key secure and avoid exposing it in public code repositories."
      ],
      "metadata": {
        "id": "ET0SPfGBu3Bm"
      },
      "id": "ET0SPfGBu3Bm"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "1e04e453-8760-4a2a-a725-0b152cd484a9",
      "metadata": {
        "id": "1e04e453-8760-4a2a-a725-0b152cd484a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "217c8006-7938-42c5-e805-0972131ae916"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter Open AI API Key: ··········\n"
          ]
        }
      ],
      "source": [
        "# Import the getpass module for securely entering the OpenAI API key\n",
        "from getpass import getpass\n",
        "\n",
        "# Prompt the user to enter the OpenAI API key without displaying it on the screen\n",
        "OPENAI_KEY = getpass('Enter Open AI API Key: ')  # User input will be masked for privacy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# After entering the key, we store it as an environment variable for use later in the script\n",
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = OPENAI_KEY  # Set the environment variable to the entered API key"
      ],
      "metadata": {
        "id": "ldqpZ7Tjuk6j"
      },
      "id": "ldqpZ7Tjuk6j",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "c. **Initialize the Model**\n",
        "\n",
        "Once the OpenAI API key has been securely set up and stored as an environment variable, the next step is to **initialize the language model** that will be used for your application.\n",
        "\n",
        "In this case, we are using the ```ChatOpenAI``` class from the ```langchain_openai``` package, which allows us to interact with OpenAI models in a conversational format."
      ],
      "metadata": {
        "id": "1pj7iP88u-fa"
      },
      "id": "1pj7iP88u-fa"
    },
    {
      "cell_type": "markdown",
      "source": [
        "By specifying the model name ```(gpt-4o)``` and parameters such as ```temperature```, you control the behavior of the model:\n",
        "\n",
        "- model: Determines which version of OpenAI’s LLM you are using. Here, \"gpt-4o\" is chosen.\n",
        "\n",
        "- temperature: Controls randomness in responses.\n",
        "\n",
        "   - 0 means deterministic responses (same input → same output).\n",
        "\n",
        "   - Higher values (e.g., 0.7) make responses more creative and varied.\n",
        "\n",
        "This initialization step ensures that the model is ready to handle queries and generate outputs in subsequent steps."
      ],
      "metadata": {
        "id": "t2Nfe5eO01JC"
      },
      "id": "t2Nfe5eO01JC"
    },
    {
      "cell_type": "code",
      "source": [
        "# Import ChatOpenAI class from langchain_openai package\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Initialize the OpenAI Chat model\n",
        "# - model=\"gpt-4o\": specifies which OpenAI model to use\n",
        "# - temperature=0: makes responses deterministic (less random)\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)"
      ],
      "metadata": {
        "id": "e_332WdBpYnK"
      },
      "id": "e_332WdBpYnK",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "d. **Importing Display Functions for Jupyter/Colab**\n",
        "\n",
        "This code allows you to import display functions that enable you to format and display content (like Markdown) in Jupyter or Colab notebooks."
      ],
      "metadata": {
        "id": "nFO5pueL5HUI"
      },
      "id": "nFO5pueL5HUI"
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary display functions for Jupyter/Colab\n",
        "from IPython.display import display, Markdown"
      ],
      "metadata": {
        "id": "SN4l8eWKpza1"
      },
      "id": "SN4l8eWKpza1",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Zero-Shot Prompting\n",
        "\n",
        "In **Zero-Shot Prompting, you give the model a task without any examples.** The model is expected to generate answers or classify inputs based on the instructions it’s provided.\n",
        "\n",
        "**Zero-Shot Prompt Theory:**\n",
        "\n",
        "- The model doesn’t get any examples of inputs and outputs, and it must generalize based on the instructions alone.\n",
        "\n",
        "- This method is ideal when you want the model to handle a wide variety of inputs and need it to reason without needing specific examples.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "We want to categorize an incoming user support query into one of several categories: \"Login Issue,\" \"Billing Query,\" or \"Feature Request.\"\n",
        "\n",
        "#### LangChain Implementation (Zero-Shot)"
      ],
      "metadata": {
        "id": "VnYqMTViuPNO"
      },
      "id": "VnYqMTViuPNO"
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary modules from langchain\n",
        "from langchain.prompts import ChatPromptTemplate  # To create chat prompt templates\n",
        "from langchain.chains import LLMChain  # To create a chain that uses LLMs\n",
        "from langchain.llms import OpenAI  # To use OpenAI models for generating responses\n",
        "from langchain_core.output_parsers import StrOutputParser  # For parsing the output into a string\n",
        "\n",
        "# Zero-Shot prompt template\n",
        "final_prompt = ChatPromptTemplate.from_messages([  # Create a prompt template from predefined messages\n",
        "    (\"system\", \"Categorize the user's issue into one of the following categories: Login Issue, Billing Query, Feature Request.\"),  # System message defining the categorization task\n",
        "    (\"human\", \"{input}\")  # Human message template for the input\n",
        "])\n",
        "\n",
        "# Create chain\n",
        "# The chain is composed of three components:\n",
        "# 1. final_prompt - the defined prompt template that prepares the input message.\n",
        "# 2. llm - an instance of the LLM (e.g., OpenAI model) to process the input (Note: `llm` should be defined, but it's missing in the provided code).\n",
        "# 3. StrOutputParser - a parser that converts the LLM response to a string output\n",
        "chain = final_prompt | llm | StrOutputParser()\n",
        "\n",
        "# Input to be classified\n",
        "response = chain.invoke({\"input\": \"I'd like to upgrade my plan.\"})  # Pass the input string for classification\n",
        "print(response)  # Expected output: Billing Query  # Print the categorized output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXeuUOVPunEx",
        "outputId": "b12f1061-36f1-401a-b09a-b7a1ee22268a"
      },
      "id": "wXeuUOVPunEx",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Billing Query\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, the model uses just the prompt’s instruction to categorize the issue."
      ],
      "metadata": {
        "id": "y4oFFJC4u9Em"
      },
      "id": "y4oFFJC4u9Em"
    },
    {
      "cell_type": "markdown",
      "id": "296f0dcf-dbfb-4e52-a2b9-095b2e940e6f",
      "metadata": {
        "id": "296f0dcf-dbfb-4e52-a2b9-095b2e940e6f"
      },
      "source": [
        "### 3\\. Few-Shot Prompting 🎯\n",
        "\n",
        "In **Few-Shot Prompting, the model is provided with a few examples,** helping it understand the task’s context and style of reasoning before categorizing a new input.\n",
        "\n",
        "**Few-Shot Prompt Theory:**\n",
        "\n",
        "- The model receives a few examples to help it generalize, learning from the examples and improving its categorization skills.\n",
        "\n",
        "- This is particularly useful when a task requires more nuance or specific understanding based on previous patterns.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "In this case, you provide the model with a few labeled examples, showing how to classify issues like \"Login Issue,\" \"Billing Query,\" and \"Feature Request.\"\n",
        "\n",
        "#### LangChain Implementation (Few-Shot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "b6bd0f90-0425-42c7-937f-8da13a5b3fd7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6bd0f90-0425-42c7-937f-8da13a5b3fd7",
        "outputId": "fc5d893d-dbec-4d2a-960b-0b8e5b8c5380"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Billing Query\n"
          ]
        }
      ],
      "source": [
        "# Import necessary modules from langchain_core\n",
        "from langchain_core.prompts import FewShotChatMessagePromptTemplate, ChatPromptTemplate  # Used for creating prompt templates\n",
        "\n",
        "# Example input-output pairs for few-shot learning\n",
        "examples = [\n",
        "    {\"input\": \"I can't reset my password.\", \"output\": \"Login Issue\"},  # Example of a login issue\n",
        "    {\"input\": \"My latest invoice is incorrect.\", \"output\": \"Billing Query\"},  # Example of a billing query\n",
        "    {\"input\": \"Can you add support for dark mode?\", \"output\": \"Feature Request\"},  # Example of a feature request\n",
        "]\n",
        "\n",
        "# Define a basic prompt template with a human input and AI output\n",
        "example_prompt = ChatPromptTemplate.from_messages([  # Create the prompt template using examples\n",
        "    (\"human\", \"{input}\"),  # Placeholder for human input\n",
        "    (\"ai\", \"{output}\"),  # Placeholder for AI output\n",
        "])\n",
        "\n",
        "# Create a few-shot prompt that includes the example input-output pairs\n",
        "few_shot_prompt = FewShotChatMessagePromptTemplate(  # Define the few-shot prompt template\n",
        "    example_prompt=example_prompt,  # Use the example prompt for each few-shot example\n",
        "    examples=examples,  # Provide the list of example input-output pairs\n",
        ")\n",
        "\n",
        "# Final prompt template for classification\n",
        "final_prompt = ChatPromptTemplate.from_messages([  # Define the final prompt template\n",
        "    (\"system\", \"You are a support ticket classifier. Categorize the user's issue into one of the following categories: Login Issue, Billing Query, Feature Request.\"),  # System message defining the task\n",
        "    few_shot_prompt,  # Add the few-shot prompt to improve classification accuracy\n",
        "    (\"human\", \"{input}\"),  # Placeholder for the input provided by the user\n",
        "])\n",
        "\n",
        "# Create the chain of components\n",
        "# The chain consists of the final prompt, the LLM, and an output parser\n",
        "chain = final_prompt | llm | StrOutputParser()  # Here, `llm` should be defined as an LLM instance (e.g., OpenAI model)\n",
        "\n",
        "# Input to be classified\n",
        "response = chain.invoke({\"input\": \"I'd like to upgrade my plan.\"})  # Pass the input to the chain for classification\n",
        "print(response)  # Expected output: Billing Query  # Print the result"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34f82eef-3ec2-49b9-889f-5b44446c9a41",
      "metadata": {
        "id": "34f82eef-3ec2-49b9-889f-5b44446c9a41"
      },
      "source": [
        "  * **Why It's More Effective:** Few-shot prompting teaches the model the specific nuances and desired output format for your custom task. It dramatically improves accuracy for classification, extraction, and formatting tasks that are unique to your business.\n",
        "\n",
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3efc3477-e3fe-4f7a-8e5d-f619124029be",
      "metadata": {
        "id": "3efc3477-e3fe-4f7a-8e5d-f619124029be"
      },
      "source": [
        "### 4\\. Chain of Thought (CoT) Prompting 🧠\n",
        "\n",
        "**Chain of Thought** prompting encourages the model to break down a problem into intermediate steps before giving a final answer. This improves its reasoning on complex logical or mathematical problems.\n",
        "\n",
        "  * **Business Use Case:** An e-commerce system needs to automatically determine if a customer's order qualifies for a complex promotional discount.\n",
        "\n",
        "  * **Naive Prompt:** `\"Order total is $150, customer is a premium member, used coupon 'SAVE10'. Is the final price $120?\"`\n",
        "\n",
        "  * **Likely Naive Output:** A simple \"Yes\" or \"No,\" which could be wrong if the model misinterprets the sequence of applying discounts.\n",
        "\n",
        "#### LangChain Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "43497196-dcb1-4ddf-884d-784b9644462a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        },
        "id": "43497196-dcb1-4ddf-884d-784b9644462a",
        "outputId": "8517e259-ebdd-47d4-a534-467e1108b684"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Let's go through the calculation step-by-step:\n\n1. **Calculate the Premium Discount:**\n   - The customer is a Premium Member, which means they get a 20% discount on the order total.\n   - Order Total: $150\n   - Premium Discount: 20% of $150 = 0.20 * $150 = $30\n\n2. **Apply the Premium Discount:**\n   - Subtract the premium discount from the order total.\n   - Discounted Price = $150 - $30 = $120\n\n3. **Apply the 'SAVE10' Coupon:**\n   - The 'SAVE10' coupon takes $10 off the discounted price.\n   - Final Price after Coupon = $120 - $10 = $110\n\nTherefore, the final price of the order is $110."
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Create a ChatPromptTemplate with a detailed prompt that guides the model through a step-by-step process.\n",
        "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "Determine the final price of an order based on the rules below. Let's think step-by-step.\n",
        "\n",
        "Rules:\n",
        "1. Premium members get a 20% discount on the order total.\n",
        "2. Coupons are applied AFTER the premium discount.\n",
        "3. The 'SAVE10' coupon takes $10 off.\n",
        "\n",
        "Customer Order:\n",
        "- Order Total: $150\n",
        "- Customer Status: Premium Member\n",
        "- Coupon Used: 'SAVE10'\n",
        "\n",
        "First, calculate the premium discount.\n",
        "Then, apply the coupon to the discounted price.\n",
        "Finally, state the final price.\n",
        "\n",
        "Step-by-step calculation:\n",
        "\"\"\")\n",
        "\n",
        "# Create the chain which includes the prompt template, LLM model, and output parser\n",
        "chain = prompt | llm | StrOutputParser()  # `llm` is assumed to be an OpenAI model or any other LLM instance\n",
        "\n",
        "# Invoke the chain and process the prompt\n",
        "response = chain.invoke({})  # Empty dictionary as the input is fixed in the prompt\n",
        "\n",
        "# Display the output in a readable Markdown format\n",
        "display(Markdown(response))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1e7b12e-e98b-46f7-b7f3-555c258a8607",
      "metadata": {
        "id": "d1e7b12e-e98b-46f7-b7f3-555c258a8607"
      },
      "source": [
        "  * **Why It's More Effective:** By instructing the model to \"think step-by-step,\" you force it into a more rigorous and logical reasoning process, significantly reducing calculation errors and improving the accuracy of the final answer.\n",
        "\n",
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e88b1c41-4ffb-4ff3-aca0-e0aaa66042fd",
      "metadata": {
        "id": "e88b1c41-4ffb-4ff3-aca0-e0aaa66042fd"
      },
      "source": [
        "### 5\\. Tree of Thought (ToT) Prompting 🌳\n",
        "\n",
        "**Tree of Thoughts** is an advanced technique where the model generates multiple different reasoning paths (`branches`), evaluates them, and chooses the most promising one. This is too complex for a single prompt and is often simulated as a multi-step process.\n",
        "\n",
        "  * **Business Use Case:** A strategy team needs to decide on the best approach to enter a new market. They need to consider multiple options and their potential downsides.\n",
        "\n",
        "  * **Naive Prompt:** `\"What's a good strategy for entering the European market?\"`\n",
        "\n",
        "  * **Likely Naive Output:** One or two generic strategies (e.g., \"start with a digital-first approach\" or \"partner with a local distributor\") without deep evaluation.\n",
        "\n",
        "#### LangChain Implementation (Simulated ToT)\n",
        "\n",
        "We simulate ToT by chaining three distinct steps: Generate, Evaluate, and Select."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "3b387433-b530-47ad-ba47-dd54d68b4169",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3b387433-b530-47ad-ba47-dd54d68b4169",
        "outputId": "3eed8cb7-2464-4fb4-9a40-cd7c7ae72328"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the evaluation of pros and cons for each strategy, the most balanced and likely to succeed strategy appears to be the **Sustainability and Ethical Sourcing Campaign**. Here's the justification for this recommendation:\n",
            "\n",
            "1. **Alignment with Consumer Values:** European consumers increasingly prioritize sustainability and ethical practices. This strategy aligns well with these values, potentially enhancing brand loyalty and attracting a dedicated customer base.\n",
            "\n",
            "2. **Positive Brand Image and Differentiation:** By positioning the brand as socially responsible and environmentally conscious, the company can differentiate itself from competitors. This positive brand image can lead to increased consumer trust and long-term brand equity.\n",
            "\n",
            "3. **Partnership and Network Expansion:** Collaborating with NGOs and environmental organizations can strengthen the brand's credibility and expand its network. These partnerships can also provide valuable resources and insights, enhancing the effectiveness of the campaign.\n",
            "\n",
            "4. **Long-term Impact:** Initiatives like reforestation or clean water projects can have lasting positive effects on communities and the environment, contributing to a sustainable future and reinforcing the brand's commitment to social responsibility.\n",
            "\n",
            "5. **Mitigation of Cons:** While there are challenges such as verification and higher costs, these can be mitigated through rigorous oversight, transparent communication, and strategic partnerships. The risk of skepticism can be addressed by ensuring that sustainability claims are substantiated and credible.\n",
            "\n",
            "6. **Scalability and Adaptability:** The sustainability campaign can be scaled and adapted to different markets, allowing the brand to maintain a consistent global message while tailoring specific initiatives to local needs and conditions.\n",
            "\n",
            "While the other strategies offer unique advantages, they also present significant challenges that may limit their effectiveness or scalability. The Cultural Fusion Pop-Up Cafés, for example, face high costs and logistical challenges, while Localized Product Innovation involves complex supply chains and regulatory hurdles. In contrast, the Sustainability and Ethical Sourcing Campaign offers a more balanced approach with the potential for long-term success and positive brand impact.\n"
          ]
        }
      ],
      "source": [
        "# Import necessary modules\n",
        "from langchain_core.runnables import RunnablePassthrough  # Used to pass data through a chain without modification\n",
        "\n",
        "# 1. Generator: Propose several distinct options\n",
        "# Create a prompt that asks for three creative strategies for a coffee brand to enter the European market\n",
        "prompt_generate = ChatPromptTemplate.from_template(\"Generate three distinct and creative strategies for a US-based coffee brand to enter the European market.\")\n",
        "\n",
        "# Create a generator chain\n",
        "# This chain starts with a \"RunnablePassthrough\" (which simply passes data), runs the prompt to generate strategies, uses an LLM for processing, and parses the response into a string\n",
        "generator_chain = {\"original_prompt\": RunnablePassthrough()} | prompt_generate | llm | StrOutputParser()\n",
        "\n",
        "# 2. Evaluator: For each option, list pros and cons\n",
        "# Create a prompt to evaluate the strategies generated by the previous step\n",
        "prompt_evaluate = ChatPromptTemplate.from_template(\"\"\"\n",
        "For each of the following strategies, list the potential pros and cons.\n",
        "\n",
        "Strategies:\n",
        "{strategies}\n",
        "\n",
        "Evaluation:\n",
        "\"\"\")\n",
        "\n",
        "# Create the evaluator chain\n",
        "# This chain takes the output of the generator chain (the strategies) as input and uses the LLM to evaluate them\n",
        "evaluator_chain = {\"strategies\": generator_chain} | prompt_evaluate | llm | StrOutputParser()\n",
        "\n",
        "# 3. Selector: Based on the evaluation, choose the best path\n",
        "# Create a prompt to select the best strategy based on the evaluation of pros and cons\n",
        "prompt_select = ChatPromptTemplate.from_template(\"\"\"\n",
        "Based on the following evaluation of pros and cons, which strategy is the most balanced and likely to succeed? Justify your answer.\n",
        "\n",
        "Evaluation:\n",
        "{evaluation}\n",
        "\n",
        "Recommendation:\n",
        "\"\"\")\n",
        "\n",
        "# Create the final chain\n",
        "# This chain takes the output of the evaluator chain (the evaluation of strategies) and uses the LLM to choose the best strategy\n",
        "final_chain = {\"evaluation\": evaluator_chain} | prompt_select | llm | StrOutputParser()\n",
        "\n",
        "# Invoke the final chain to get the response\n",
        "response = final_chain.invoke({})  # The empty dictionary is passed as no specific input is needed for this example\n",
        "\n",
        "# Print the response\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the formatted Markdown\n",
        "display(Markdown(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 577
        },
        "id": "Wk_MDvqL9uTn",
        "outputId": "705dbef0-241e-4271-9646-75d17895ea4a"
      },
      "id": "Wk_MDvqL9uTn",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Based on the evaluation of the three strategies for entering the European market, **Cultural Collaboration and Local Partnerships** emerges as the most balanced and likely to succeed strategy. Here’s the justification for this recommendation:\n\n### Justification:\n\n1. **Local Expertise and Market Insights**: By collaborating with established local businesses, the brand can gain invaluable insights into consumer preferences, cultural nuances, and market dynamics. This local knowledge is crucial for tailoring products and marketing strategies effectively, which can significantly enhance the chances of success in a diverse and competitive market like Europe.\n\n2. **Brand Credibility and Acceptance**: Partnering with trusted local entities can enhance the brand's credibility and acceptance among European consumers. This is particularly important in a market where consumers may be wary of foreign brands. Local partnerships can help bridge the gap between the brand and the target audience, fostering trust and loyalty.\n\n3. **Cultural Relevance**: The ability to tailor offerings to local tastes through collaborations can create a more authentic brand experience. This cultural relevance can resonate with consumers, making them more likely to engage with and support the brand.\n\n4. **Community Engagement**: Engaging with local influencers and communities can foster loyalty and generate word-of-mouth marketing. This grassroots approach can be more effective than traditional marketing strategies, especially in a market where consumers value authenticity and local connections.\n\n5. **Mitigating Risks**: While there are cons associated with dependency on partners and potential brand dilution, these risks can be managed through careful selection of partners and clear communication of brand values. The collaborative approach allows for shared resources and expertise, which can mitigate some of the operational complexities and costs associated with other strategies.\n\n### Comparison with Other Strategies:\n\n- **Sustainability and Ethical Sourcing Focus**: While this strategy has strong consumer appeal, the higher costs and complexity of establishing an ethical supply chain may pose significant challenges. Additionally, consumer skepticism about sustainability claims could hinder the brand's ability to build trust.\n\n- **Experiential Coffee Culture Hub**: Although this strategy offers unique engagement opportunities, the high initial investment and operational complexity present substantial risks. The reliance on foot traffic and local interest can also limit the brand's reach and scalability.\n\n### Conclusion:\nCultural Collaboration and Local Partnerships provide a balanced approach that leverages local knowledge, enhances brand credibility, and fosters community engagement while managing risks effectively. This strategy aligns well with the brand's potential to succeed in the competitive European market, making it the most viable option among the three evaluated strategies."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80e4574e-2d14-43b6-9ffb-a7c0b7f193aa",
      "metadata": {
        "id": "80e4574e-2d14-43b6-9ffb-a7c0b7f193aa"
      },
      "source": [
        "  * **Why It's More Effective:** This structured, multi-step approach mimics an expert's decision-making process. It explores a wider solution space, critically evaluates each option, and provides a well-reasoned final recommendation, leading to much more robust strategic outputs than a single prompt ever could.\n",
        "\n",
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c9c65ac-ab40-45c8-ae66-dbdff21bb925",
      "metadata": {
        "id": "3c9c65ac-ab40-45c8-ae66-dbdff21bb925"
      },
      "source": [
        "### 6\\. ReAct Prompting (Reason + Act) 🤖\n",
        "\n",
        "**ReAct** is a pattern where the model can use external **tools** to find information it doesn't have. It **Re**asons about what it needs, **Act**s by calling a tool, observes the result, and repeats. This is the core concept behind **LangChain Agents**.\n",
        "\n",
        "  * **Business Use Case:** A financial analyst asks an AI assistant, \"How did NVIDIA's stock performance yesterday compare to the NASDAQ index?\" The model needs real-time data to answer this.\n",
        "\n",
        "  * **Naive Prompt:** `\"How did NVIDIA's stock perform yesterday vs. NASDAQ?\"`\n",
        "\n",
        "  * **Likely Naive Output:** \"I'm sorry, but I don't have access to real-time financial data. My knowledge is limited to my last training cut-off.\"\n",
        "\n",
        "#### LangChain Implementation (Agent)\n",
        "\n",
        "We give the model a `search` tool and use an agent to let it decide when to use it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaf9a5de"
      },
      "source": [
        "**Important:** Before running the following cell, you need to obtain a Tavily API key and add it to Colab's Secrets Manager.\n",
        "\n",
        "1.  **Get your API key:** Sign up or log in to [Tavily AI](https://tavily.com/) to get your API key.\n",
        "2.  **Setting Up the Tavily AI API Key Securely:** The code below demonstrates how to securely input and set up the Tavily AI API key for use within your environment. This ensures that your API key is kept private and not hard-coded into the script.\n",
        "\n",
        "**Note:** Please paste your Tavily AI API key in the dialog box below and press the Enter button. Remember to keep your API key secure and avoid exposing it in public code repositories."
      ],
      "id": "aaf9a5de"
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the getpass module for securely entering the Tavily API key\n",
        "from getpass import getpass\n",
        "\n",
        "# Prompt the user to enter the Tavily API key without displaying it on the screen\n",
        "TAVILY_API_KEY = getpass('Enter Tavily API Key: ')  # User input will be masked for privacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Eod6UZm3Q27",
        "outputId": "69b1ad7b-bb70-49dd-cbcb-2656d538b3ae"
      },
      "id": "3Eod6UZm3Q27",
      "execution_count": 24,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter Tavily API Key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# After entering the key, we store it as an environment variable for use later in the script\n",
        "import os\n",
        "os.environ['TAVILY_API_KEY'] = TAVILY_API_KEY  # Set the environment variable to the entered Tavily API key"
      ],
      "metadata": {
        "id": "BYXe9cK93i_1"
      },
      "id": "BYXe9cK93i_1",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ad13b3dd-0da5-4435-a301-d303accd2967",
        "outputId": "d25f2b91-07c8-4c5b-be84-bd55d7ab2038"
      },
      "source": [
        "# Import necessary modules\n",
        "from langchain.agents import create_openai_tools_agent, AgentExecutor\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults # Changed back to TavilySearchResults for consistency with the rest of the notebook\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from google.colab import userdata # Import userdata to access secrets\n",
        "\n",
        "# 1. Define the tools the agent can use\n",
        "# The agent will use the TavilySearchResults tool to fetch stock-related data (in this case, a max of 1 result).\n",
        "tools = [TavilySearchResults(max_results=1)]\n",
        "\n",
        "\n",
        "# 2. Create the prompt for the agent\n",
        "# The prompt provides instructions to the agent about its task.\n",
        "# The agent is a \"helpful financial assistant\" that answers questions regarding stock performance.\n",
        "prompt = ChatPromptTemplate.from_messages([  # Construct a template for the chat\n",
        "    (\"system\", \"You are a helpful financial assistant. Answer the user's questions about stock performance.\"),  # Define agent's role\n",
        "    (\"human\", \"{input}\"),  # Placeholder for the user's input\n",
        "    (\"placeholder\", \"{agent_scratchpad}\"),  # Placeholder for agent's thoughts and intermediate work\n",
        "])\n",
        "\n",
        "# 3. Create the agent\n",
        "# This creates an OpenAI-based agent capable of using the tools (here, TavilySearchResults) and the prompt.\n",
        "# Make sure 'llm' is defined in a previous cell\n",
        "agent = create_openai_tools_agent(llm, tools, prompt)  # The `llm` variable must be defined earlier as an OpenAI model, e.g., OpenAI(model=\"text-davinci-003\")\n",
        "\n",
        "# 4. Create the Agent Executor to run the agent\n",
        "# The `AgentExecutor` will run the agent, providing the agent with access to its tools and making the process verbose.\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
        "\n",
        "# 5. Invoke the agent to answer the user's question\n",
        "# This invokes the agent with a specific user input, asking how NVIDIA's stock performed compared to the NASDAQ index.\n",
        "response = agent_executor.invoke({\"input\": \"How did NVIDIA's (NVDA) stock performance yesterday compare to the NASDAQ index (IXIC)?\"})\n",
        "\n",
        "# 6. Print the agent's response\n",
        "# The agent processes the question and returns a response, which is printed.\n",
        "print(response[\"output\"])  # Output contains the agent's answer to the question"
      ],
      "id": "ad13b3dd-0da5-4435-a301-d303accd2967",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `tavily_search_results_json` with `{'query': 'NVIDIA NVDA stock performance October 19 2023'}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3m[{'title': 'NVIDIA Corporation (NVDA) Stock Historical Prices & Data', 'url': 'https://finance.yahoo.com/quote/NVDA/history?period1=1681633833&period2=1713256223', 'content': '| Nov 8, 2023 | 46.10 | 46.87 | 45.97 | 46.57 | 46.55 | 346,719,000 |\\n| Nov 7, 2023 | 45.72 | 46.22 | 45.16 | 45.96 | 45.93 | 343,165,000 |\\n| Nov 6, 2023 | 45.28 | 45.94 | 44.90 | 45.75 | 45.73 | 400,733,000 |\\n| Nov 3, 2023 | 44.02 | 45.31 | 43.72 | 45.01 | 44.98 | 424,610,000 |\\n| Nov 2, 2023 | 43.33 | 43.88 | 42.89 | 43.51 | 43.48 | 409,172,000 |\\n| Nov 1, 2023 | 40.88 | 42.38 | 40.87 | 42.33 | 42.30 | 437,593,000 |\\n| Oct 31, 2023 | 40.45 | 40.88 | 39.23 | 40.78 | 40.76 | 517,969,000 | [...] | Oct 30, 2023 | 41.09 | 41.77 | 40.48 | 41.16 | 41.14 | 388,028,000 |\\n| Oct 27, 2023 | 41.13 | 41.21 | 40.01 | 40.50 | 40.48 | 416,784,000 |\\n| Oct 26, 2023 | 41.85 | 42.26 | 39.88 | 40.33 | 40.30 | 541,001,000 |\\n| Oct 25, 2023 | 43.40 | 43.65 | 41.56 | 41.78 | 41.76 | 398,379,000 |\\n| Oct 24, 2023 | 43.08 | 43.70 | 42.69 | 43.66 | 43.64 | 401,463,000 |\\n| Oct 23, 2023 | 41.23 | 43.25 | 40.94 | 42.97 | 42.95 | 478,530,000 |\\n| Oct 20, 2023 | 41.89 | 42.47 | 41.08 | 41.39 | 41.36 | 477,266,000 | [...] | Nov 17, 2023 | 49.52 | 49.72 | 49.01 | 49.30 | 49.27 | 325,205,000 |\\n| Nov 16, 2023 | 48.68 | 49.53 | 48.33 | 49.48 | 49.45 | 339,756,000 |\\n| Nov 15, 2023 | 49.94 | 49.96 | 48.20 | 48.89 | 48.86 | 475,497,000 |\\n| Nov 14, 2023 | 49.68 | 49.83 | 49.04 | 49.66 | 49.63 | 416,954,000 |\\n| Nov 13, 2023 | 48.32 | 49.12 | 48.10 | 48.62 | 48.59 | 384,136,000 |\\n| Nov 10, 2023 | 47.50 | 48.47 | 47.28 | 48.33 | 48.31 | 421,245,000 |\\n| Nov 9, 2023 | 47.47 | 48.23 | 46.75 | 46.95 | 46.92 | 540,496,000 |', 'score': 0.8976328}]\u001b[0m\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `tavily_search_results_json` with `{'query': 'NASDAQ IXIC performance October 19 2023'}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3m[{'title': 'Stock Market News for Oct 19, 2023', 'url': 'https://www.nasdaq.com/articles/stock-market-news-for-oct-19-2023', 'content': 'fell 1% or 332.57 points to close at 33,665.08. Notably, 23 components of the 30-stock index ended in negative territory, while 3 ended in positive territory. The tech-heavy Nasdaq Composite finished at 13,314.03, plummeting 1.6% or 219.45 points due to weak performance of large-cap technology stocks. [...] Published Time: Thu, 10/19/2023 — 12:36\\n\\nStock Market News for Oct 19, 2023 | Nasdaq\\n\\nSkip to main content\\n\\n\\n\\n   Nasdaq+\\n   Weekly Macro+\\n   Scorecard\\n   Market Activity\\n\\n       U.S. Market Activity->\\n           Stocks\\n           Options\\n           ETFs\\n           Mutual Funds\\n           Indexes\\n           Cryptocurrency\\n           Currencies\\n           Fixed Income', 'score': 0.83225065}]\u001b[0m\u001b[32;1m\u001b[1;3mOn October 19, 2023, NVIDIA's (NVDA) stock performance and the NASDAQ index (IXIC) both experienced declines:\n",
            "\n",
            "- **NVIDIA (NVDA):** The specific performance details for NVIDIA on October 19, 2023, were not directly available in the search results. However, the general trend for large-cap technology stocks, including NVIDIA, contributed to the decline in the NASDAQ index.\n",
            "\n",
            "- **NASDAQ Index (IXIC):** The NASDAQ Composite fell by 1.6%, closing at 13,314.03. This decline was largely due to the weak performance of large-cap technology stocks, which likely included NVIDIA.\n",
            "\n",
            "If you need more detailed information about NVIDIA's specific stock price movement on that day, you might want to check a detailed financial news source or stock market data provider.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "On October 19, 2023, NVIDIA's (NVDA) stock performance and the NASDAQ index (IXIC) both experienced declines:\n",
            "\n",
            "- **NVIDIA (NVDA):** The specific performance details for NVIDIA on October 19, 2023, were not directly available in the search results. However, the general trend for large-cap technology stocks, including NVIDIA, contributed to the decline in the NASDAQ index.\n",
            "\n",
            "- **NASDAQ Index (IXIC):** The NASDAQ Composite fell by 1.6%, closing at 13,314.03. This decline was largely due to the weak performance of large-cap technology stocks, which likely included NVIDIA.\n",
            "\n",
            "If you need more detailed information about NVIDIA's specific stock price movement on that day, you might want to check a detailed financial news source or stock market data provider.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f952d085-97f3-4cc8-b439-6ce67cd19cd4",
      "metadata": {
        "id": "f952d085-97f3-4cc8-b439-6ce67cd19cd4"
      },
      "source": [
        "  * **Why It's More Effective:** ReAct (via agents) breaks the LLM out of its static knowledge box. It allows the model to interact with the real world by using APIs and tools, enabling it to answer questions about current events, query private databases, and perform actions, which is a massive leap in capability."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:langchain]",
      "language": "python",
      "name": "conda-env-langchain-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}